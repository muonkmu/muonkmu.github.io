<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>AI_HW_Design on MW Devlog</title>
        <link>https://muonkmu.github.io/categories/ai_hw_design/</link>
        <description>Recent content in AI_HW_Design on MW Devlog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 26 Feb 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://muonkmu.github.io/categories/ai_hw_design/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[AI HW Design] Chap08 Network Sparsity (2/2)</title>
        <link>https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-2/2/</link>
        <pubDate>Sun, 26 Feb 2023 00:00:00 +0000</pubDate>
        
        <guid>https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-2/2/</guid>
        <description>&lt;h2 id=&#34;scnn-accelerator&#34;&gt;SCNN Accelerator&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;SCNN은 sparse encoding scheme을 이용해서 activation / weight sparsity 지원&lt;/li&gt;
&lt;li&gt;Planar Tiled-Input Stationary-Cartesian Product-sparse (PT-IS-CP-sparse)라 부르는 새로운 Cartesian product flow를 제안 (activation / weight reuse)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;scnn-pt-is-cp-dense-dataflow&#34;&gt;SCNN PT-IS-CP-Dense Dataflow&lt;/h3&gt;
&lt;p&gt;PT-IS-CP-Dense dataflow는 convolution nested loop를 어떻게 분해할 것인가에 관한 것&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C X R X S 형태의 K개 filter, batch size N인 C X W X H 형태의 input activation 일 때&lt;/li&gt;
&lt;li&gt;Input Stationary (IS) 가 적용되면 loop order는 C→W→H→K→R→S 가 됨&lt;/li&gt;
&lt;li&gt;성능향상을 위해 blocking strategy 적용 (K output channel은 $K_c$ 사이즈의 K/$K_c$ output channel group으로 분리)&lt;/li&gt;
&lt;li&gt;K/$K_c$→C→W→H→$K_c$→R→S&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;intra-PE parallelism을 위해 PE 내부에서 spatial reuse 활용&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;filter weight(F)와 input activation(I)가 각 buffer에서 fetch되고 이는 F X I array 곱셈기로 전송&lt;/li&gt;
&lt;li&gt;filter weight와 input activation은 재활용 되며 partial sum은 향후 연산을 위해 메모리 접근 없이 저장됨&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;intra-PE parallelism을 위해 Spartial tiling 전략이 사용됨&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;W X H input activation는 $W_t$ X $H_t$ Planar Tiles(PT)로 나눠져서 PE로 분배됨&lt;/li&gt;
&lt;li&gt;또한 mutiple channel processing 지원 (C X $W_t$ X $H_t$이 PE에 할당됨)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;sliding window operation에서 edge에서 cross-tile dependency가 생기는데 data halo를 이용해 해결&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Input Halos : PE input buffer는 halo을 수용하기 위해 C x Wt x Ht보다 약간 큰 크기로 조정&lt;/li&gt;
&lt;li&gt;Output Halos : PE accumulation buffer도 halo을 수용하기 위해 Kc x Wt x Ht보다 약간 큰 크기로 조정. Halo에는 출력 채널 계산이 끝날 때 누적을 완료하기 위해 인접 PE와 통신하는 불완전한 부분 합계가 포함.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PT-IS-CP-Dense Dataflow의 최종 수식은 다음과 같다
&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-2/2/01_PT-IS-CP-dense_dataflow.png&#34;
	width=&#34;1060&#34;
	height=&#34;1140&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-2/2/01_PT-IS-CP-dense_dataflow_hu3c650d0832d33928f21fb381fc9933f6_155844_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-2/2/01_PT-IS-CP-dense_dataflow_hu3c650d0832d33928f21fb381fc9933f6_155844_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;PT-IS-CP- dense dataflow&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;92&#34;
		data-flex-basis=&#34;223px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>[AI HW Design] Chap08 Network Sparsity (1/2)</title>
        <link>https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/</link>
        <pubDate>Sat, 25 Feb 2023 00:00:00 +0000</pubDate>
        
        <guid>https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/</guid>
        <description>&lt;p&gt;8장은 시스템 throughput을 높이기 위해 ineffectual zoro operation을 스킵하는 다양한 구조에 대해 알아본다. (feature map encoding/indexing, weight sharing/pruning, quantized prediction)&lt;/p&gt;
&lt;h2 id=&#34;energy-efficient-inference-engine-eie&#34;&gt;Energy Efficient Inference Engine (EIE)&lt;/h2&gt;
&lt;p&gt;스탠포드 대학에서 나온 유명한 논문, 알고리즘(SW) + 가속기(SW) 양쪽에 대해 최적화한 논문으로 알고 있다. 중요도에 비해 설명이 부실한 듯&amp;hellip;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;EIE는 sparse matrix-vector mutiplication을 위한 compressed network model을 지원하고 weight sharing을 다룸&lt;/li&gt;
&lt;li&gt;Leading Non-Zero Detection Network, Central Conrol Unit, Processing Element 로 구성&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;leading-non-zero-detection-network-lnzd&#34;&gt;Leading Non-Zero Detection Network (LNZD)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;LNZD는 input activation으로 부터 nonzero element를 찾아내고 이를 LNZD node로 먹임&lt;/li&gt;
&lt;li&gt;node는 nonzero value와 index를 PE로 브로드캐스팅 함&lt;/li&gt;
&lt;li&gt;LNZD에 PE는 4개가 연결&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/01_EIE_PE_LNZD.png&#34;
	width=&#34;432&#34;
	height=&#34;300&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/01_EIE_PE_LNZD_huc96f5c73e9317af5fcbb4d0b7427ae06_21530_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/01_EIE_PE_LNZD_huc96f5c73e9317af5fcbb4d0b7427ae06_21530_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;EIE leading nonzero detection network&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;345px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;central-control-unit-ccu&#34;&gt;Central Control Unit (CCU)&lt;/h3&gt;
&lt;p&gt;CCU는 network segment computation을 제어&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;host와 커뮤니케이션 하고 PE state를 모니터링 함&lt;/li&gt;
&lt;li&gt;두가지 동작모드로 나뉨
&lt;ul&gt;
&lt;li&gt;computing mode : CCU는 LNZD로 부터 nonzero input activation을 받고 이를 PE로 브로드캐스팅, 모든 input channel이 스캔될때 까지 반복됨&lt;/li&gt;
&lt;li&gt;I/O mode : PE는 idle, activation과 weight가 DMA에 의해 접근 됨&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;processing-element-pe&#34;&gt;Processing Element (PE)&lt;/h3&gt;
&lt;p&gt;요약하면 ifmap 값을 읽어와서 여기 포인터를 이용하여 해당 weight 값과 효율적으로 곱한다는 이야기인듯..&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PE는 activation queue, pointer read unit, sparse matrix unit, arithmetic unit, activation R/W unit으로 구성&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/02_EIE_PE_archi.png&#34;
	width=&#34;1339&#34;
	height=&#34;371&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/02_EIE_PE_archi_hu4a8254844eadbc3e86f6c2cae8627339_197913_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/02_EIE_PE_archi_hu4a8254844eadbc3e86f6c2cae8627339_197913_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;EIE PE architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;360&#34;
		data-flex-basis=&#34;866px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;computation 동안 CCU는 nonzero input element와 index를 activation queue에 브로드캐스팅 함, PE는 큐가 다차면 input element를 처리(브로드캐스트는 중지)&lt;/li&gt;
&lt;li&gt;activation queue는 PE가 work backlog를 구축할 수 있도록 해줌(load balancing 문제 해결)&lt;/li&gt;
&lt;li&gt;pointer read unit은 activation queue의 인덱스를 이용하여 nonzero element의 시작/종료 포인터를 찾음&lt;/li&gt;
&lt;li&gt;싱글 사이클에 이를 처리하기 위해 포인터는 LSB와 함께 odd/even SRAM에 저장(의미를 잘 모르겠음)&lt;/li&gt;
&lt;li&gt;sparse matrix read unit은 sparse matrix memory로 부터 포인터를 이용하여 0이 아닌 값을 읽음(fmap?)&lt;/li&gt;
&lt;li&gt;arithmetic unit은 activation queue와 sparse matrix memory의 0이 아닌 값 MAC 연산&lt;/li&gt;
&lt;li&gt;연속적으로 가산기 사용되는 경우를 위한 bypass 경로 존재 (그림 보자)&lt;/li&gt;
&lt;li&gt;activation read/write unit의 경우 fully connected layrer를 위한 source/destination activation register를 가지고 있으며 이는 다음 레이어 연산시 교체됨&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deep-compression&#34;&gt;Deep Compression&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;EIE는 pruning과 weight sharing을 통해 네트워크 압축하기위해 Deep Commpression을 적용, 적용 예는 다음과 같음 (책에 그림 예시를 보자)
&lt;ul&gt;
&lt;li&gt;4X4 weight matrix라면 16개의 값을 4개의 index(code book)로 만듬&lt;/li&gt;
&lt;li&gt;index에 해당하는 weight는 Gradient를 가지고 fine-tunnig됨&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MAC 연산은 weight와 input activation vector가 0이 아닌 값에 대해서 수행&lt;/li&gt;
&lt;li&gt;EIE는 interleaved Commpressed Sparse Column(CSC) 적용 (Eyeriss와 약간 다르므로 책참조)
&lt;ul&gt;
&lt;li&gt;v는 0이 아닌 값, z는 0이 아닌 값 해당 v 값 앞에 0의 개수&lt;/li&gt;
&lt;li&gt;v,z는 하나의 large array pair에 $p_j$(벡터의 시작 포인터), $p_j+_1$(마지막 항목 다음 번 포인터)와 함께 저장됨&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sparse-matrix-computation&#34;&gt;Sparse Matrix Computation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;4개의 PE에서 input activation vector(a)는 weight matrix(w)와 곱해짐&lt;/li&gt;
&lt;li&gt;a를 스캔해서 0이 아닌 $a_j$는 인덱스 값과 함께 브로드캐스팅 됨&lt;/li&gt;
&lt;li&gt;PE는 index에 대응하는 0이 아닌 $w_j$를 곱합&lt;/li&gt;
&lt;li&gt;PE는 벡터 v를 $p_j$에서 $p_j+_1$까지만 스캔
(책에 예시 그림 있으니 참조)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;cambricon-x-accelerator&#34;&gt;Cambricon-X Accelerator&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;병렬연산에서 Nonzero neuron을 선택하기 위해 indexing scheme을 적용&lt;/li&gt;
&lt;li&gt;Control Processor (CP), Buffer Controller (BC), Input Neural Buffer (NBin), Output Neural Buffer (NBout), Direct Memory Access (DMA) Module, Computation Unit (CU)으로 구성&lt;/li&gt;
&lt;li&gt;중요 element는 BC Tn indexing unit(nonzero neuron을 인덱싱하는 유닛이며 PE와 수가 같다)&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/03_Cambricon-X_archi.png&#34;
	width=&#34;660&#34;
	height=&#34;588&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/03_Cambricon-X_archi_huabb91cf32cb2211099253a3648178d82_23226_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/03_Cambricon-X_archi_huabb91cf32cb2211099253a3648178d82_23226_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Cambricon-X architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;112&#34;
		data-flex-basis=&#34;269px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;computation-unit-cu&#34;&gt;Computation Unit (CU)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CU는 Tn개 PE로 구성되며 모든 PE는 fat tree 형태로 연결&lt;/li&gt;
&lt;li&gt;PE는 PE Functional Unit(PEFU)와 Synapse Buffer(SB)로 구성&lt;/li&gt;
&lt;li&gt;BC로 부터 neuron을, local BC로 부터 synaps를 읽어서 PEFU에 제공하며 output neuron은 BC에 다시 씀&lt;/li&gt;
&lt;li&gt;Tn개 PEFU는 Tm개 곱셈기와 가산기를 가져서 TnXTm 곱셈이 가능&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/04_Cambricon-X_PE_archi.png&#34;
	width=&#34;709&#34;
	height=&#34;666&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/04_Cambricon-X_PE_archi_hu87e93c84bfbce09b528c0e6429ef2f43_40459_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/04_Cambricon-X_PE_archi_hu87e93c84bfbce09b528c0e6429ef2f43_40459_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Cambricon-X PE architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;106&#34;
		data-flex-basis=&#34;255px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;SB는 synapse를 저장하고 메모리 access를 최소화 하기 위해 디자인, 책에서는 하기 예시를 듬(책그림 참조)
&lt;ul&gt;
&lt;li&gt;PE는 4개이고 output neuron 0이 input neruon 2개 연결, output neuron 1이 input neruon 5개 연결&lt;/li&gt;
&lt;li&gt;output neuron 0의 weight는 address 0에 output neuron 1의 weight는 address 1/2의 SB에 저장&lt;/li&gt;
&lt;li&gt;output neuron 0 계산 시 SB를 1번 읽고 output neuron 1은 두 번 읽음&lt;/li&gt;
&lt;li&gt;synapse의 수는 output neuron마다 다를 수 있기 때문에 SB가 비동기적으로 데이터를 로드하여 전체 성능을 향상&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;buffer-controller&#34;&gt;Buffer Controller&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;BC는 Indexing Module (IM)과 BC Functional Unit (BCFU) 로 구성
&lt;ul&gt;
&lt;li&gt;BCFU는 인덱싱을 위해 neuron을 저장&lt;/li&gt;
&lt;li&gt;IM은 BC의 nonzero nueron을 구분하고 nonzero indexed nueron만 전송&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;BC는 input neurons을 NBin에서 PE로 보내거나 BCFU로 제공, PE의 계산결과는 BCFU에 저장 또는 NBout에 쓰여짐&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/05_Cambricon-X_BC_archi.png&#34;
	width=&#34;982&#34;
	height=&#34;694&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/05_Cambricon-X_BC_archi_hu608d04ea44a57a3b368d80bb229fbc9d_35431_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/05_Cambricon-X_BC_archi_hu608d04ea44a57a3b368d80bb229fbc9d_35431_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Cambricon-X BC architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;141&#34;
		data-flex-basis=&#34;339px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;IM에는 두가지 하기 두가지 옵션이 있음 (책에 그림에 잘 나와 있음)
&lt;ul&gt;
&lt;li&gt;direct indexing : nonzero nueron의 여부를 0/1로 표현한 binary string 사용&lt;/li&gt;
&lt;li&gt;step indexing : nonzero nueron의 거리를 사용&lt;/li&gt;
&lt;li&gt;step indexing이 area와 power를 적게 소모함&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>[AI HW Design] Chap06 &amp; 07 In/Near Memory Computation</title>
        <link>https://muonkmu.github.io/p/ai-hw-design-chap06-07-in/near-memory-computation/</link>
        <pubDate>Tue, 21 Feb 2023 00:00:00 +0000</pubDate>
        
        <guid>https://muonkmu.github.io/p/ai-hw-design-chap06-07-in/near-memory-computation/</guid>
        <description>&lt;p&gt;6장은 In-Memory Computation, 7장은 Near-Memory Computation 에 대한 내용이다. 현재 개발하고자 하는 가속기와 동떨어지는 내용이라 판단해서 개요만 보고 스킵할 예정이다.
삼성이나 하이닉스를 다녀야 쓸모있지 않을까 싶다.&lt;/p&gt;
&lt;h2 id=&#34;in-memory-computation&#34;&gt;In-Memory Computation&lt;/h2&gt;
&lt;p&gt;여기서는 메모리와 로직을 stacking 하는 방식의 Processor-In-Memory(PIM)에 대해 설명한다. 다른 방식의 PIM도 있는 걸로 아는데&amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;neurocube-architecture&#34;&gt;Neurocube Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Nerocube는 parallel neural processing unit과 High Bandwidth Memory(HBM)을 스택한 Hybrid Memory Cube(HMC)를 이용&lt;/li&gt;
&lt;li&gt;이는 stacked memrory에서 PE로 직접 데이터 로드가 가능함(레이턴시 감소)&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap06-07-in/near-memory-computation/01_Neurocube_archi.png&#34;
	width=&#34;850&#34;
	height=&#34;484&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap06-07-in/near-memory-computation/01_Neurocube_archi_hu7e3ac3480d8fffeef819172836ca0123_89087_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap06-07-in/near-memory-computation/01_Neurocube_archi_hu7e3ac3480d8fffeef819172836ca0123_89087_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Neurocube architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;175&#34;
		data-flex-basis=&#34;421px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;teris-accelerator&#34;&gt;Teris Accelerator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Teris는 Eyeriss의 3D Memory와 함께 Row Stationary(RS) dataflow 채택&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap06-07-in/near-memory-computation/02_Tetris_archi.png&#34;
	width=&#34;1200&#34;
	height=&#34;814&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap06-07-in/near-memory-computation/02_Tetris_archi_hue0cec8dafc2424a6cf3e0101670db0cd_520795_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap06-07-in/near-memory-computation/02_Tetris_archi_hue0cec8dafc2424a6cf3e0101670db0cd_520795_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Tetris system architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;147&#34;
		data-flex-basis=&#34;353px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;neurostream-accelerator&#34;&gt;NeuroStream Accelerator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NeuroStream은 HMC의 modular extension인 Smart Memory Cube(SMC)를 이용&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap06-07-in/near-memory-computation/03_NeuroStream_arch.png&#34;
	width=&#34;1226&#34;
	height=&#34;1306&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap06-07-in/near-memory-computation/03_NeuroStream_arch_hucb12ff9120ba45e699b7649d69b4155b_231205_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap06-07-in/near-memory-computation/03_NeuroStream_arch_hucb12ff9120ba45e699b7649d69b4155b_231205_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;NeuroStream and NeuroCluster architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;93&#34;
		data-flex-basis=&#34;225px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;near-memory-computation&#34;&gt;Near-Memory Computation&lt;/h2&gt;
&lt;h3 id=&#34;didiannao-supercomputer&#34;&gt;DiDianNao Supercomputer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;대용량 eDRAM을 통해 DianNao의 memory bottleneck을 해결하고자 함&lt;/li&gt;
&lt;li&gt;모든 synapse를 수용할 후 있는 거대 storage를 제공하는 Neural Function Unit(NFU)를 지닌 16개의 tile로 구성&lt;/li&gt;
&lt;li&gt;NFU는 4개의 eDRAM bank와 time-interleaved 통신(?) 함 (eDRAM의 레이턴시가 크기 때문)&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap06-07-in/near-memory-computation/04_DaDianNao_archi.png&#34;
	width=&#34;1214&#34;
	height=&#34;622&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap06-07-in/near-memory-computation/04_DaDianNao_archi_hu3dc4a4aa5e68fb351920261bd2859b52_93958_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap06-07-in/near-memory-computation/04_DaDianNao_archi_hu3dc4a4aa5e68fb351920261bd2859b52_93958_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;DaDianNao system architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;195&#34;
		data-flex-basis=&#34;468px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cnvlutin-accelerator&#34;&gt;Cnvlutin Accelerator&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DiDianNao에서 파생되었으며 다수의 DiDianNao를 고속 인터페이스로 연결, 거대 parallel mutiplication lane 구조 채택&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>[AI HW Design] Chap05 Convolution Optimization (3/3)</title>
        <link>https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/</link>
        <pubDate>Sun, 19 Feb 2023 00:00:00 +0000</pubDate>
        
        <guid>https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/</guid>
        <description>&lt;p&gt;앞의 포스트가 너무 길어져서 Eyeriss version2 부분은 현 포스트로 나눔&lt;/p&gt;
&lt;h2 id=&#34;eyeriss-accelerator-ver2&#34;&gt;Eyeriss Accelerator Ver.2&lt;/h2&gt;
&lt;p&gt;irregular data pattern 과 network sparsity 지원을 위한 Eyeriss V2 공개&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;새로운 NoC 구조 : data reuse가 적을 땐 external memory에서 더 많은 데이터를 PE로 가져오고  data reuse가 많을 땐 spartial Data를 sharing&lt;/li&gt;
&lt;li&gt;CSC 엔코딩 적용&lt;/li&gt;
&lt;li&gt;RS+ 적용&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Eyeriss V1이 GLB&amp;lt;-&amp;gt;PE 연결을 위해 flat multicast NoC를 사용했지만 V2에서는 flexible and mesh NoC 사용,
이 hierarchical mesh는 GLB cluster, Router Cluster, PE cluster로 구성되며 하기 3가지 타입의 데이터 이동 지원&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ifmap은 GLB cluster에 로드됨, 이는 GLB 메모리에 저장되고 Router Cluster로 전송&lt;/li&gt;
&lt;li&gt;psum은 GLB 메모리에 저장 되고, ofmap은 external memory에 바로 저장 됨&lt;/li&gt;
&lt;li&gt;fmap은 Router Cluster로 전송되고 PE spad에 저장&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/01_Eyeriss_v2_system_archi.png&#34;
	width=&#34;1090&#34;
	height=&#34;622&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/01_Eyeriss_v2_system_archi_hu72a2311e75580f2efbca22981e0f948c_86395_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/01_Eyeriss_v2_system_archi_hu72a2311e75580f2efbca22981e0f948c_86395_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Eyeriss v2 system architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;175&#34;
		data-flex-basis=&#34;420px&#34;
	
&gt;
(책에 v1 과 v2 구조에 대한 비교 그림이 있으니 찾아보자)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hierarchical-mesh-network-hm-noc&#34;&gt;Hierarchical Mesh Network (HM-NoC)&lt;/h3&gt;
&lt;p&gt;전통적인 Network-on-Chip 구조는 하기와 같으며 장단점을 가지고 있음 (책 그림 참조)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Broadcast Network : high reuse / low bandwidth&lt;/li&gt;
&lt;li&gt;Unicast Network : high bandwidth / low reuse&lt;/li&gt;
&lt;li&gt;All-to-All Network : high reuse, high bandwidth / scale difficulty&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Eyeriss V2는 RS+를 지원하기 위해 HM-NoC 구조를 제안, all-to-all network에서 파생되었으나 4가지 모드를 가짐&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Broadcast: single input and single weight&lt;/li&gt;
&lt;li&gt;Unicast: multiple inputs and multiple weights&lt;/li&gt;
&lt;li&gt;Grouped multicast: shared weights&lt;/li&gt;
&lt;li&gt;Interleaved multicast: shared inputs&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/02_Mesh_network_config.png&#34;
	width=&#34;1095&#34;
	height=&#34;270&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/02_Mesh_network_config_hu7c65497e49921880cc047795eaed4cf9_175240_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/02_Mesh_network_config_hu7c65497e49921880cc047795eaed4cf9_175240_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Mesh network configuration&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;405&#34;
		data-flex-basis=&#34;973px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;HM-NoC는 source, destination, router로 구성되며 design phase에서 cluster로 그룹핑되고 operation mode에서는 고정됨.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Router cluster가 다른 cluster와 one-to-one, many-to-many, source/destination 구조로 연결&lt;/li&gt;
&lt;li&gt;Router cluster는 4개의 source/destination port를 가지며 4가지 routing mode (broadcast, unicate, grouped/interleaved multicast)를 가짐&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;책에서는 다음과 같이 예시를 듬&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Convolution layer : ifmap과 fmap이 reuse 되며 grouped multicast 또는 interleaved mode 로 구성&lt;/li&gt;
&lt;li&gt;Depth-wise Convolution layer : fmap만 reuse 되며 fmap이 PE로 broadcast, ifmap은 GLB에서 로드&lt;/li&gt;
&lt;li&gt;Fully connected layer : ifmap이 모든 PE로 broadcast, fmap은 unicast mode로 로드&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;input-activation-hm-noc&#34;&gt;Input Activation HM-NoC&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Router Cluster 안의 3개 ifmap router는 GLB cluster의 ifmap SRAM과 연결&lt;/li&gt;
&lt;li&gt;ifmap routerd의 3개의 source/destination port 다른 클러스터와 연결, 1개는 메모리에서 데이터로드, 1개는 PE 연결&lt;/li&gt;
&lt;li&gt;책에 그림과 상세 설명이 있으니 참조하자&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;filter-weight-hm-noc&#34;&gt;Filter Weight HM-NoC&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Router Cluster 안의 각 fmap router는 PE cluster 안의 PE row와 연결&lt;/li&gt;
&lt;li&gt;vertical mesh는 사라지고 horizontal mesh 만 데이터 reuse를 위해 남음&lt;/li&gt;
&lt;li&gt;책에 그림과 상세 설명이 있으니 참조하자&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;partial-sum-hm-noc&#34;&gt;Partial Sum HM-NoC&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Router Cluster 안의 4개의 psum router는 GLB cluster의 psum SRAM과 PE cluster의 PE column과 연결됨&lt;/li&gt;
&lt;li&gt;horizontal mesh는 사라지고  vertical mesh만 psum accumulation을 위해 남음&lt;/li&gt;
&lt;li&gt;책에 그림과 상세 설명이 있으니 참조하자&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;compressed--sparse-column-csc-format&#34;&gt;Compressed  Sparse Column (CSC) Format&lt;/h3&gt;
&lt;p&gt;Eyeriss V2는 ifmap과 fmap 둘 다에 CSC 포맷 적용 (zero operation skipping), CSC 포맷의 구조는 다음과 같다&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data vector : 0이 아닌 값&lt;/li&gt;
&lt;li&gt;Counter vector : Data vector의 item 기준, 해당 열에서 앞에 있는 0의 값의 갯수&lt;/li&gt;
&lt;li&gt;Address vector : 각 열을 기준으로 이전 열까지 0이 아닌 item의 누적 갯수&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/03_CSC_format.png&#34;
	width=&#34;988&#34;
	height=&#34;683&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/03_CSC_format_hub1bb5f53114ac8ca37a38a301b9de523_189171_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/03_CSC_format_hub1bb5f53114ac8ca37a38a301b9de523_189171_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Compressed sparse column format&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;347px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Eyeriss V2는 PE는 zero operation skip을 위해 7 pipeline stage와 5 spad (ifmap, fmap, psum 저장)로 수정&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;첫째로 non-zero data인지 확인하기 위해 address를 검사하고 fmap 로드 전 먼저 ifmap을 로드(zero ifmap skip을 위해)&lt;/li&gt;
&lt;li&gt;ifmap/fmap이 0이 아니면 계산 pipeline 수행, fmap이 0이면 pipeline disable&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/04_Eyeriss_v2_PE_archi.png&#34;
	width=&#34;1098&#34;
	height=&#34;520&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/04_Eyeriss_v2_PE_archi_hu6538d1528bc1b96b882dcb94c7cd426f_43122_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/04_Eyeriss_v2_PE_archi_hu6538d1528bc1b96b882dcb94c7cd426f_43122_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Eyeriss v2 PE architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;211&#34;
		data-flex-basis=&#34;506px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;row-stationary-plus-rs-dataflow&#34;&gt;Row Stationary Plus (RS+) Dataflow&lt;/h3&gt;
&lt;p&gt;PE utilization을 높이기 위해 RS+ dataflow 적용&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;model dimension을 다른 PE dimension에 매핑하기 위해 데이터를 tiling, spatial fragmentation 함&lt;/li&gt;
&lt;li&gt;depth-wise convolution 시 PE utilization이 낮은 문제점 해결&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/05_RS&amp;#43;_dataflow.png&#34;
	width=&#34;978&#34;
	height=&#34;348&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/05_RS&amp;#43;_dataflow_hud8aa43c1c4159e4637e897eeb4c09dc1_22311_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/05_RS&amp;#43;_dataflow_hud8aa43c1c4159e4637e897eeb4c09dc1_22311_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;row stationary plus dataflow&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;281&#34;
		data-flex-basis=&#34;674px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>[AI HW Design] Chap05 Convolution Optimization (2/3)</title>
        <link>https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/</link>
        <pubDate>Sat, 18 Feb 2023 00:00:00 +0000</pubDate>
        
        <guid>https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/</guid>
        <description>&lt;p&gt;유명한 MIT의 Eyeriss Accelerator 논문이다. 아직까지 관련 프로젝트가 진행 중인 것으로 보이며 찾아보면 관련 논문에 대하여 리뷰해논 자료가 꽤 많다.
잘 소개된 곳 한 곳 (허락없는 링크..)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&amp;amp;blogId=kangdonghyun&amp;amp;logNo=220990374125&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&amp;blogId=kangdonghyun&amp;logNo=220990374125&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;eyeriss-accelerator&#34;&gt;Eyeriss Accelerator&lt;/h2&gt;
&lt;p&gt;Eyeriss Accelerator는 data access를 최소화하기 위한 Row Stationary (RS) Dataflow를 제안하며 다음과 같은 특징을 지님&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spartial architecture with sequential processing configuration
&lt;ul&gt;
&lt;li&gt;Spartial architecture can exploit high compute parallelism using direct communication between an array of relatively simple processing engines (PEs).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Row Stationary (RS) Dataflow 구현&lt;/li&gt;
&lt;li&gt;Four level memory hierarchy : PE scratch pad와 inter-PE 통신을 최대한 이용하고 Global Buffer와 외부 메모리의 data transfer를 최소화&lt;/li&gt;
&lt;li&gt;point-to-point &amp;amp; multicast Network-on-Chip(NoC) 아키텍쳐 지원&lt;/li&gt;
&lt;li&gt;Run-Length Compression (RLC) 포맷 지원 : zero operation 제거&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;eyeriss-system-architecture&#34;&gt;Eyeriss System Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Eyeriss는 과 communication link clock 두가지 clock domain을 가짐
&lt;ul&gt;
&lt;li&gt;data processing core clock : 12X14 PE array, Global Buffer(GLB), RLC codec, ReLu 배치, PE가 local scratchpad를 이용하여 연산하거나 PE가 인접 PE 또는 GLB와 통신 하는것을 가능하게 함&lt;/li&gt;
&lt;li&gt;communication link clock&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Four level memory hierarchy
&lt;ul&gt;
&lt;li&gt;GLB &amp;lt;-&amp;gt; external memory : asynchronous FIFO 이용&lt;/li&gt;
&lt;li&gt;PE &amp;lt;-&amp;gt; GLB : NoC 이용&lt;/li&gt;
&lt;li&gt;ReLU &amp;lt;-&amp;gt; RLC codec&lt;/li&gt;
&lt;li&gt;local temporary data storage using scratchpad&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;분리된 clock으로 PE는 다른 PE와 같은 클럭 시간에 독립적으로 동작가능하고 link clock은 external memory와 64bit 양방향 버스로 data 전송을 제어&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/01_Eyeriss_system_archi.png&#34;
	width=&#34;882&#34;
	height=&#34;311&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/01_Eyeriss_system_archi_huf1524314c1b2ba265b52bb1365fc6338_34365_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/01_Eyeriss_system_archi_huf1524314c1b2ba265b52bb1365fc6338_34365_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Eyeriss system architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;283&#34;
		data-flex-basis=&#34;680px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;Eyeriss Acc는 Convolution network를 레이어 단위로 진행
&lt;ul&gt;
&lt;li&gt;첫째로 PE array를 레이어 function/size에 맞게 구성하고 매핑 수행 및 전송 패턴을 결정&lt;/li&gt;
&lt;li&gt;input feature map 및 filter map은 external memory에서 PE로 로드되고 output feature map은 다시 external memory로 쓰여짐&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2d-convolution-to-1d-multiplication&#34;&gt;2D convolution to 1D multiplication&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;convolution을 수행할 때 2D feature/filter map을 1D로 바꾸어 수행해서 PE에 순차적으로 로딩한다는 이야기를 길게 써놓음 (궁금하면 책을 보자)
&lt;ul&gt;
&lt;li&gt;2D convolution을 1D vector 와 Toeplitz 행렬(대각선의 성분이 모두 같은 매트릭스)의 곱으로 변환된다&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;책에 어떤 순서로 feature/filter 1D vector가 PE에 로드/계산되는지 그림으로 있다.&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/02_1D_row_convolution.png&#34;
	width=&#34;807&#34;
	height=&#34;555&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/02_1D_row_convolution_hu821e8a0c54af7140bd12bc2289e267fd_105196_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/02_1D_row_convolution_hu821e8a0c54af7140bd12bc2289e267fd_105196_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;145&#34;
		data-flex-basis=&#34;348px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;stationary-dataflow&#34;&gt;Stationary Dataflow&lt;/h3&gt;
&lt;p&gt;칩에 대한 이야기는 아니고 이전의 stationary 전략에 대해 소개한다. (연산을 어떤 데이터를 고정, 이동 시킬지)&lt;/p&gt;
&lt;h4 id=&#34;output-stationary&#34;&gt;Output Stationary&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Patial Sum의 read/write를 local accumulation을 통해 최소화&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;weight-stationary&#34;&gt;Weight Stationary&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;filter map을 local buffer에 두고 계속 활용&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;input-stationary&#34;&gt;Input Stationary&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;input feature map을 local buffer에 두고 계속 활용&lt;/li&gt;
&lt;li&gt;다른 전략보다 효율이 안좋은데 약점은 다른 전략보다 convolution연산에 더 많은 cycle이 필요&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;row-stationary-rs-dataflow&#34;&gt;Row Stationary (RS) Dataflow&lt;/h3&gt;
&lt;p&gt;Eyeriss는 1D vector multiplication 수행하는데 RS dataflow 전략을 사용&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;filter map 행은 PE들에서 수평하게 재사용&lt;/li&gt;
&lt;li&gt;input feature map 행은 PE들에서 대각적으로 재사용&lt;/li&gt;
&lt;li&gt;partial sum 행은 PE들에서 수직적으로 재사용&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/03_Eyeriss_RS_dataflow.png&#34;
	width=&#34;889&#34;
	height=&#34;301&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/03_Eyeriss_RS_dataflow_hu83a06e14ef6cd6bea9052c996e8c4779_80929_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/03_Eyeriss_RS_dataflow_hu83a06e14ef6cd6bea9052c996e8c4779_80929_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Eyeriss RS dataflow&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;295&#34;
		data-flex-basis=&#34;708px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;RS dataflow에서 데이터는 계산 동안 PE에 저장됨(데이터 이동 최소화)&lt;/li&gt;
&lt;li&gt;time-interleaved approach를 통해 fmap과 ifmap은 같은 clock cycle 내에서 재활용&lt;/li&gt;
&lt;li&gt;계산이 완료되면 Psum은 근접 PE들로 이동(다음 계산을 위해서)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;filter-reuse&#34;&gt;Filter Reuse&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;fmap이 spad에 로드 되고 고정된다. 다수의 ifmap도 spad에 로드되고 사슬처럼 연걸됨&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;input-feature-maps-reuse&#34;&gt;Input Feature Maps Reuse&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ifmap이 먼저 PE에 로드 되고 2개의 fmap은 time-interleaved(연산?) 됨&lt;/li&gt;
&lt;li&gt;1개의 ifmap으로 2개의 fmap과 1D 연산 수행&lt;/li&gt;
&lt;li&gt;이 것은 전체적인 스피드를 올려주지만 fmap과 psum의 time-interleave 연산을 지원하기 위해 큰 spad가 필요&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;partial-sums-reuse&#34;&gt;Partial Sums Reuse&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;fmap/ifmap 둘 다 PE에 로드되며 둘 다 time-interleaved 함&lt;/li&gt;
&lt;li&gt;psum은 같은 채널 끼리 합쳐짐&lt;/li&gt;
&lt;li&gt;fmap/ifmap 둘 다 PE에 로드되어야 하므로 필요한 spad의 용량이 증가&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;run-length-compression-rlc&#34;&gt;Run-Length Compression (RLC)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ReLU 연산 결과 0 값이 많아 지므로 이는 network의 sparsity가 도입됨&lt;/li&gt;
&lt;li&gt;zero computation을 피하기 위해 Eyeriss는 64 bit  RLC 포맷을 도입
&lt;ul&gt;
&lt;li&gt;([5bit]앞에 값이 0인 element 갯수 + [16bit]0이 아닌 값)*3 + [1bit]마지막 item인지 나타내는 flag&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/04_Eyeriss_RLC.png&#34;
	width=&#34;425&#34;
	height=&#34;200&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/04_Eyeriss_RLC_hub59b108cd57493ff3ac688d982967fe9_14291_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/04_Eyeriss_RLC_hub59b108cd57493ff3ac688d982967fe9_14291_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Eyeriss run-length compression&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;212&#34;
		data-flex-basis=&#34;510px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;첫번째 layer의 ifmap 값을 제회하고 모든 fmap/ifmap은 RLC 포맷으로 external memory에 저장&lt;/li&gt;
&lt;li&gt;External Memory에서 입출력 될 때, RLC encoder/decoder를 통과하게 됨&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;glabal-buffer-glb&#34;&gt;Glabal Buffer (GLB)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;external memory와 데이터 전송을 위해 GLB 채택&lt;/li&gt;
&lt;li&gt;GLB에는 fmap/ifmap/ofmap/psum 이 저장&lt;/li&gt;
&lt;li&gt;GLB는 PE가 연산하는 동안 다음 fmap을 preload&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;processing-element-pe-architecture&#34;&gt;Processing Element (PE) Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;PE는 fmap, ifmap, psum을 위한 3가지 타입의 spad를 가짐&lt;/li&gt;
&lt;li&gt;datapath는 3가지 pipeline stage에 의해 구성(spad access, fmap/ifmap multiplication, psum accumulation)&lt;/li&gt;
&lt;li&gt;16bit 연산을 사용하며 32bit 연산결과는 16bit로 절삭&lt;/li&gt;
&lt;li&gt;값이 O인 ifmap이 발견되면 spad에서 fmap 값을 읽는 것과 연산 로직을 끔(전력소모를 줄이기 위해)&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/05_Eyeriss_PE_archi.png&#34;
	width=&#34;876&#34;
	height=&#34;471&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/05_Eyeriss_PE_archi_hu6d4af46b5849dcef0bf48d3b94639878_43237_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/05_Eyeriss_PE_archi_hu6d4af46b5849dcef0bf48d3b94639878_43237_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Eyeriss processing element architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;185&#34;
		data-flex-basis=&#34;446px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;network-on-chip-noc&#34;&gt;Network-on-Chip (NoC)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NoC는 GLB와 PE 사이의 데이터 이동을 관리, 하기 2개로 구분
&lt;ul&gt;
&lt;li&gt;Global Input Network (GIN) : GLB &amp;lt;-&amp;gt; PE 간 single cycle multicast 이용 데이터 전송
&lt;ul&gt;
&lt;li&gt;Y-Bus는 12개의 X-bus와 연결되며 X-bus는 14개의 PE가 연결&lt;/li&gt;
&lt;li&gt;top level controller는 &amp;lt;row,col&amp;gt; 태그를 생성하고 Y-bus / PE의 Multicast controller가 tag를 비교하여 데이터 전송 결정&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/06_Eyeriss_GIN.png&#34;
	width=&#34;885&#34;
	height=&#34;454&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/06_Eyeriss_GIN_hudd45dfb58898f938211550fa285202e2_35533_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/06_Eyeriss_GIN_hudd45dfb58898f938211550fa285202e2_35533_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Eyeriss global input network&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;194&#34;
		data-flex-basis=&#34;467px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;책에 AlexNet의 예시 있음&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Global Output Network (GON) : 별다른 설명 없음&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>[AI HW Design] Chap05 Convolution Optimization (1/3)</title>
        <link>https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/</link>
        <pubDate>Sun, 12 Feb 2023 00:00:00 +0000</pubDate>
        
        <guid>https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/</guid>
        <description>&lt;p&gt;Convolution은 90% 이상의 Computing resource를 사용하며, data access를 줄이기 위해 feature maps reuse / filter weights reuse / partial sum reuse 같은 전략이 사용된다.
이번 챕터에서는 filter decomposition과 Row Stationary(RS) flow를 설명한다.&lt;/p&gt;
&lt;h2 id=&#34;deep-convolution-neural-network-accelerator-dcnn&#34;&gt;Deep Convolution Neural Network Accelerator (DCNN)&lt;/h2&gt;
&lt;p&gt;DCNN은 클라우드 타겟이 아닌 엣지 디바이스 타겟으로 UCLA에서 개발, 다음과 같은 특징을 지님&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Streaming data flow minimizes data access&lt;/li&gt;
&lt;li&gt;병렬 컴퓨팅을 위해 bandwidth  향상이 아닌 Interleaving architecture&lt;/li&gt;
&lt;li&gt;Large-size filter decomposition supports arbitrary convolution window&lt;/li&gt;
&lt;li&gt;추가적인 pooling functional unit을 통한 Convolution Unit(CU)의 workload 감소&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;system-architecture&#34;&gt;System Architecture&lt;/h3&gt;
&lt;p&gt;DCNN의 구성은 다음과 같다&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Buffer Bank
&lt;ul&gt;
&lt;li&gt;중간 데이터 저장 및 외부 메모리와 데이터 교환 목적&lt;/li&gt;
&lt;li&gt;Layer Input 용, Layer Output 용 2가지 셋으로 나누어짐&lt;/li&gt;
&lt;li&gt;또한 odd/even channel/feature를 위한 Bank A와 Bank B로 나누어짐(Interleaved)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Column Buffer
&lt;ul&gt;
&lt;li&gt;Buffer banck의 데이터를 CU engine의 input data type으로 remap&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Convolution Unint(CU) engine
&lt;ul&gt;
&lt;li&gt;CU engin은 kernel size 3X3까지 지원하는 16개의 Convolution unit으로 구성&lt;/li&gt;
&lt;li&gt;16bit fixed-point 연산&lt;/li&gt;
&lt;li&gt;local Prefetch unit이 DMA로 부터 weight/bias 값을 주기적으로 업데이트 함&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Accumulation (ACCU) buffer
&lt;ul&gt;
&lt;li&gt;convolution 동안 partial sum 연산 또는 Max pooling 연산 수행&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/01_DCNN_HW_Archi.png&#34;
	width=&#34;751&#34;
	height=&#34;463&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/01_DCNN_HW_Archi_hu3c580145d215e702491efc9736215254_29901_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/01_DCNN_HW_Archi_hu3c580145d215e702491efc9736215254_29901_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;DCNN hardware architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;162&#34;
		data-flex-basis=&#34;389px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;Control Command는 외부 메모리에서 128-depth FIFO로 로드 되며 하기 2가지로 분류됨
&lt;ul&gt;
&lt;li&gt;configure command : network layer를 구성하고 pooling/ReLU function 활성화&lt;/li&gt;
&lt;li&gt;excution command : convolution/pooing 초기화 및 필터 decompose 기술&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;filter-decomposition&#34;&gt;Filter Decomposition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;다양한 필터 커널 사이즈 대응을 위해 3X3 CU engine을 이용하여 filter decomposition 기술 이용&lt;/li&gt;
&lt;li&gt;커널 사이즈가 3의 배수가 아니면 zero-padding 이용&lt;/li&gt;
&lt;li&gt;convolution 후 결과는 one output feature map으로 재결합 됨&lt;/li&gt;
&lt;li&gt;상세 사항은 책 참조&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;streaming-architecture&#34;&gt;Streaming Architecture&lt;/h3&gt;
&lt;p&gt;데이터의 이동을 최소화하기위해 Filter Weights Reuse와 Input Channel Reuse 사용&lt;/p&gt;
&lt;h4 id=&#34;filter-weights-reuse&#34;&gt;Filter Weights Reuse&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;3X3 convolution : filter weight는 CU engine에 저장되고 input feature map이 CU engine으로 공급되며 연산이 완료 될 때까지 filter weight는 업데이트 되지 않음&lt;/li&gt;
&lt;li&gt;1X1 convolution : CU unit의 9개 곱셈기 중 7개는 off 되고 2개만 odd/even 채널의 partial sum 계산을 위해 사용&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/02_Data_streaming_archi_data_flow.png&#34;
	width=&#34;934&#34;
	height=&#34;693&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/02_Data_streaming_archi_data_flow_hu44efc47054aa1ffce71cbf0d622070e8_154287_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/02_Data_streaming_archi_data_flow_hu44efc47054aa1ffce71cbf0d622070e8_154287_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Data streaming architecture with the data flow&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;134&#34;
		data-flex-basis=&#34;323px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;Buffer Bank의 output bandwidth를 최대화하기 위해 구조는 하기와 같다(input cannel 및 Column buffer 구조 이야기 같은데)
&lt;ul&gt;
&lt;li&gt;16개의 row 데이터는 odd/even data set으로 나뉨&lt;/li&gt;
&lt;li&gt;2개의 FIFO는 각 데이터 셋에 페어링 됨 (8개의 row 데이터)&lt;/li&gt;
&lt;li&gt;8개의 input row data는 10개의 overlapped data로 매핑&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;input-channel-reuse&#34;&gt;Input Channel Reuse&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;1X1 convolution을 위해 interleaved architecture 사용 (16개 데이터 셋이 odd/even 채널로 2개 데이터셋 구분됨)&lt;/li&gt;
&lt;li&gt;2개의 데이터 셋은 다른 filter weight와 곱해져 32개 output이 나옴&lt;/li&gt;
&lt;li&gt;출력은 같은 odd/even 채널 끼리 더해짐&lt;/li&gt;
&lt;li&gt;위의 내용과 비슷한 것 같은데 filter weight가 이동하고 input 값이 고정이라는 걸 다시 설명한 듯&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pooling&#34;&gt;Pooling&lt;/h3&gt;
&lt;p&gt;pooling function은 average pooling과 max pooing 이 다른 구조로 분리되어 있음&lt;/p&gt;
&lt;h4 id=&#34;average-pooling&#34;&gt;Average Pooling&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Average Pooling function은 Convolution layer에서 Inpu/Output 채널이 같은 사이즈인 CU엔진에 의해 구현
&lt;ul&gt;
&lt;li&gt;kernel의 사이즈가 pooling window와 일치하는&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;대응되는 filter weight는 1/K^2으로 되고 나머지는 0으로 된 후 convolution 수행&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;max-pooling&#34;&gt;Max Pooling&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Max pooling은 ACCU에서 별도 모듈로 구현&lt;/li&gt;
&lt;li&gt;Max pooling 모듈은 scratch pad에 있는 8개의 output 값과 연결되며 이는 다른 stride를 지원하기 위해 MUX와 연결&lt;/li&gt;
&lt;li&gt;MUX의 출력은 MAX Pooling 계산기로 가는데 이는 3개의 입력과 1개의 output feedback 입력을 받아 계산하며 인풋이 없어질때까지 연산 반복&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/03_Max_pooling_archi.png&#34;
	width=&#34;924&#34;
	height=&#34;652&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/03_Max_pooling_archi_hud451dc32710866902d390d177efeafc4_54340_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/03_Max_pooling_archi_hud451dc32710866902d390d177efeafc4_54340_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Max pooling architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;141&#34;
		data-flex-basis=&#34;340px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convolution-unitcu-engine&#34;&gt;Convolution Unit(CU) Engine&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;3X3 CU engine은 9개의 PE(input feature와 filter weight를 곱함)와 1개의 ADDER로 구성&lt;/li&gt;
&lt;li&gt;다른 커널 사이즈 지원을 위해 PE는 On/Off 기능을 가짐&lt;/li&gt;
&lt;li&gt;상세 내용은 책 참조 (그림과 3X3일 때, 1X1일때 예시 있음)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;accumulation-accu-buffer&#34;&gt;Accumulation (ACCU) Buffer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ACCU는 scratch pad에 partial sums 과 stores output feature maps을 저장&lt;/li&gt;
&lt;li&gt;ACCU는 partial product accumulation을 위한 Ping-pong buffer, Max pooling을 위한 temporary storage, readout block으로 구성 (책에 그림 참조)&lt;/li&gt;
&lt;li&gt;Convolution이 진행되는 동안 1개 버퍼는 덧셈만 하고 다른 하나의 버퍼는 Max pooling을 진행, convolution이 완료된후 각 버퍼의 연결이 switch 됨&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model-compression&#34;&gt;Model Compression&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Model Compression을 하기 위해 training 과정에서 모델을 pruning하고 filter weights를 codebook으로 quantization 함.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>[AI HW Design] Chap04 Streaming Graph Theory (2/2)</title>
        <link>https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-2/2/</link>
        <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
        
        <guid>https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-2/2/</guid>
        <description>&lt;p&gt;본 챕터에서는 graph-based deep-learnig accelerator 중 Graphcore IPU에 대해 알아본다.
Graphcore IPU는 Microsoft와 Dell의 차세대 데이터 센터 딥러닝 가속기로 선정되었다.&lt;/p&gt;
&lt;h2 id=&#34;graphcore-intelligence-processing-unitipu&#34;&gt;Graphcore Intelligence Processing Unit(IPU)&lt;/h2&gt;
&lt;p&gt;Graphcore IPU는 fine-grained operation을 수행하기 위하여 graph theory를 적용하며 MIMD paralleism을 제공&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fine-Grained : 하나의 작업을 작은 단위의 프로세스로 나눈 뒤 다수의 호출을 통해, 작업 결과를 생성해내는 방식, 반대말은 Coarse-Grained.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;intelligence-processor-unit-architecture&#34;&gt;Intelligence Processor Unit Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;IPU는 tiles라 불리는 1216 PE로 구성&lt;/li&gt;
&lt;li&gt;PE는 256kb local memory를 가지며 레지스터 파일을 제외한 추가 memory storage를 가지지 않음&lt;/li&gt;
&lt;li&gt;tiles 간 IPU Exchange라 불리는 on-chip interconnect로 연결되어 있으며 IPU간 연결을 위한 IPU link를 제공&lt;/li&gt;
&lt;li&gt;IPU는 6개의 개별 processing thread를 제공하며 각 thread는 별개의 instruction과 excution flow를 제공&lt;/li&gt;
&lt;li&gt;각 tile은 static round-robin schedule에 따라 thread 들을 순환한다.&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-2/2/01_IPU_architecture.png&#34;
	width=&#34;617&#34;
	height=&#34;989&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-2/2/01_IPU_architecture_huc029ac9d4313a641df7f4585440b0aa8_991509_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-2/2/01_IPU_architecture_huc029ac9d4313a641df7f4585440b0aa8_991509_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Intelligence processing unit architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;62&#34;
		data-flex-basis=&#34;149px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;accumulating-matrix-product-amp-unit&#34;&gt;Accumulating Matrix Product (AMP) Unit&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;IPU는 pipelined structure AMP를 채택, AMP는 64bit mix-precison 또는 16bit single-point 연산을 클럭 사이클 마다 수행 가능
&lt;ul&gt;
&lt;li&gt;mix-precison : 훈련 중에 모델에서 16-bit 및 32-bit 부동 소수점 유형을 모두 사용하여 더 빠르게 실행하고 메모리를 적게 사용하는 것, 모델의 특정 부분을 32-bit 유형으로 유지&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;memory-architecture&#34;&gt;Memory Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;PE 당 256kb local memory를 가지며 IPU는 총 304Mb 메모리를 가짐&lt;/li&gt;
&lt;li&gt;각 tile은 21bit address space를 가지며 6개의 execution unit과 이를 공유함&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;interconnect-architecture&#34;&gt;Interconnect Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;IPU 간 연결은 IPU link를 사용하며 2개 IPU 연결은 3개의 IPU link를 사용 (65Gb/s)&lt;/li&gt;
&lt;li&gt;Host완s PCIE-4로 연결&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bulk-synchronous-parallel-bsp-model&#34;&gt;Bulk Synchronous Parallel (BSP) Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;IPU operation은 BSP 모델을 기반으로 하며, BSP 모델은 아래 3개의 Phase Operation으로 구분된다.
&lt;ul&gt;
&lt;li&gt;Computation Phase : 모든 프로세서가 로컬 메모리로 computation을 수행하며 프로세서간 어떤 통신도 없다.&lt;/li&gt;
&lt;li&gt;Communication Phase : 각 프로세서는 정보를 교환하며 어떤 computation도 없다.&lt;/li&gt;
&lt;li&gt;Barrier Synchronization : 모든 프로세서는 computation이나 communication 없이 모든 프로세서가 barrier에 도달할 때까지 대기한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;IPU는 computation이 시작되기 전에 instruction이 코어로 전송되는 BSP 모델을 실행한다. core는 computation을 수행하고 이가 끝난 뒤 다른 코어와 communication을 수행한다. 그후 모든 코어는 동기화를 수행한다.&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-2/2/02_IPU_BSP_model.png&#34;
	width=&#34;611&#34;
	height=&#34;307&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-2/2/02_IPU_BSP_model_hu0e2373d75f221ade042503bd4543a98b_34792_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-2/2/02_IPU_BSP_model_hu0e2373d75f221ade042503bd4543a98b_34792_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;IPU BSP model&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;199&#34;
		data-flex-basis=&#34;477px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;결론&#34;&gt;결론&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Blaize GSP와 Graphcore IPU는 분산 프로세서를 통해 거대 parallel operation을 처리할 수 있기에 Cloud-base application에 좋은 솔루션이다.&lt;/li&gt;
&lt;li&gt;그러나 이들은 power/area 문제로 임베디드 추론 application에는 적합하지 않다.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>[AI HW Design] Chap04 Streaming Graph Theory (1/2)</title>
        <link>https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-1/2/</link>
        <pubDate>Fri, 27 Jan 2023 00:00:00 +0000</pubDate>
        
        <guid>https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-1/2/</guid>
        <description>&lt;p&gt;본 챕터에서는 graph-based deep-learnig accelerator에 대해 공부한다 (Blaize GSP, Graphcore IPU).
이들은 Mutiple Instructions Multiple Data(MIMD) 처럼 동작한다.&lt;/p&gt;
&lt;h2 id=&#34;blaize-graph-streaming-processorgsp&#34;&gt;Blaize Graph Streaming Processor(GSP)&lt;/h2&gt;
&lt;p&gt;무언가 칩에 대한 설명보단 Graph Streaming 기본 이론에 대한 내용이 주를 이룬다.&lt;/p&gt;
&lt;h3 id=&#34;stream-graph-model&#34;&gt;Stream Graph Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Stream Graph는 네트워크 트래픽, 데이터베이스 등에 널리 쓰이는 모델로 dynamic stream data를 처리하기 위해 data stream model(TCS)을 사용
&lt;ul&gt;
&lt;li&gt;거대 그래프 스트리밍 Transit(T), 큰 데이터 처리 Compute(C), 일시/롱텀 데이터 저장 Store(S)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Turnstile 모델이 TCS모델 중에서 데이터 출발/도착과 같은 data behavior을 가장 잘 표현하며 task scheduling에 사용.&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-1/2/01_Data_streaming_TCS_Model.png&#34;
	width=&#34;317&#34;
	height=&#34;483&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-1/2/01_Data_streaming_TCS_Model_hud047163813c097f2344ae308b4f685e4_77661_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-1/2/01_Data_streaming_TCS_Model_hud047163813c097f2344ae308b4f685e4_77661_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Data streaming TCS Model&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;65&#34;
		data-flex-basis=&#34;157px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;depth-first-scheduling-approach&#34;&gt;Depth First Scheduling Approach&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Blaize GSP는 뉴럴넷모델을 Direct Acyclic Graph(DAG) format (V,E)로 변환
&lt;ul&gt;
&lt;li&gt;V는 PE vertex, E는 PE간 weighted connection을 위한 edge&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;scheduling을 위해 Depth First Scheduling (DFS)를 사용하며 dynamic graph excution을 가능하게 하고 sparse/conditional graph를 지원한다. (dfs 설명은 유명하니 생략)&lt;/li&gt;
&lt;li&gt;GSP는 4가지 Parallelism을 달성했다. 자세한 설명은 책 참조
&lt;ul&gt;
&lt;li&gt;Task parallelism, Thread parallelism, Data parallelism, Instructon parallelism&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;graph-streaming-processor-architecture&#34;&gt;Graph Streaming Processor Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;GSP는 다음 그림과 같은 구조로 되어 있음&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-1/2/02_Blaize_GSP_architecture.png&#34;
	width=&#34;617&#34;
	height=&#34;183&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-1/2/02_Blaize_GSP_architecture_hub0e573f6ff23a6411796e6d052794dc5_16408_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-1/2/02_Blaize_GSP_architecture_hub0e573f6ff23a6411796e6d052794dc5_16408_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Blaize GSP architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;337&#34;
		data-flex-basis=&#34;809px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;Streaming Processing은 Sequential Processing에 비해 하기와 같은 장점이 있음 (책에 두가지 방법에 대해 비교 그림있음)
&lt;ul&gt;
&lt;li&gt;Small intermediate buffer for local processing&lt;/li&gt;
&lt;li&gt;Cached data is easily supported&lt;/li&gt;
&lt;li&gt;Memory bandwidth is reduced to improve the performance with less power&lt;/li&gt;
&lt;li&gt;Support both task and data-parallel processing&lt;/li&gt;
&lt;li&gt;Data is sent to the next node when it is ready&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;GSP는 opeartion을 데이터가 준비되는 즉시 기다리지 않고 수행하도록 스케줄링 함으로써 성능을 향상시키고 메모리 access를 감소시킴&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>[AI HW Design] Chap03 Parallel Architecture (3/3)</title>
        <link>https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/</link>
        <pubDate>Fri, 13 Jan 2023 00:00:00 +0000</pubDate>
        
        <guid>https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/</guid>
        <description>&lt;p&gt;본 챕터에서는 몇가지 주요한 Paralle Architecture에 대하여 소개한다.
이 페이지에서는 Microsoft Catapult Fabric Accelerator에 대해서 기술한다.&lt;/p&gt;
&lt;h2 id=&#34;microsoft-catapult-fabric-accelerator&#34;&gt;Microsoft Catapult Fabric Accelerator&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;마이크로소프트는 Brainwave Project를 CPU와 FPGA를 쓰는 형태로 변경&lt;/li&gt;
&lt;li&gt;48개의 FPGA가 2개의 Half rack (pod)에 그룹화되고 네트워크로 연결&lt;/li&gt;
&lt;li&gt;Brainwave는 train된 DNN 모델을 Catapult Fabric이라 불리는 synthesized softcore에 컴파일 후 narrow precision approach를 적용&lt;/li&gt;
&lt;li&gt;모델의 파라메터는 softcore 내 상주&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/01_Microsoft_brainwave_cloud_architect.png&#34;
	width=&#34;951&#34;
	height=&#34;528&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/01_Microsoft_brainwave_cloud_architect_hu5005c84237c5c0d108925d87f29946ce_123068_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/01_Microsoft_brainwave_cloud_architect_hu5005c84237c5c0d108925d87f29946ce_123068_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;180&#34;
		data-flex-basis=&#34;432px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;system-configuration&#34;&gt;System Configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Catapult Fabric은 synthesized softcore이며 이는 RTL의 recompilation 없이 low level SW library를 통해 reconfigure 될 수 있다.&lt;/li&gt;
&lt;li&gt;Catapult Fabric은 Shell과 Role 두 파트로 나눌 수 있음 (상세사항은 책을 참조)
&lt;ul&gt;
&lt;li&gt;Shell : 모든 application에서 재사용 가능한 programmable logic (통신/off-chip 등의 인터페이스를 말하는 듯)&lt;/li&gt;
&lt;li&gt;Roll : Application logic (그림을 보면 Softcore를 지칭하는 듯)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;catapult-fabric-architecture&#34;&gt;Catapult Fabric Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Catapult Fabric은 Matrix-Vector Multiplier(MVM), MultiFunction Uint(MFU), Vector Arbitration Network로 구성
&lt;ul&gt;
&lt;li&gt;MVM : Matrix-Vector 및 Vector-Vector 연산 수행, PRF/VRF/MRF에 ifmap/fmap 저장&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;VAN : PRF/DRAM/IO-queue 간 데이터 전송 담당&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/02_The_Catapult_fabric_microarchitecture.png&#34;
	width=&#34;568&#34;
	height=&#34;543&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/02_The_Catapult_fabric_microarchitecture_hub0253fb48da88283ddf54bccb7267131_45778_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/02_The_Catapult_fabric_microarchitecture_hub0253fb48da88283ddf54bccb7267131_45778_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;104&#34;
		data-flex-basis=&#34;251px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;matrix-vector-multiplier&#34;&gt;Matrix-Vector Multiplier&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;FP16을 MS-FP8/MS-FP-9으로 변환하여 연산(mantissa가 2~3bit)&lt;/li&gt;
&lt;li&gt;input data는 VRF, filter weight는 MFR에 저장&lt;/li&gt;
&lt;li&gt;3개의 tile engine과 3개의 accumulator에서 병렬 연산을 지원(상세내용을 책을 참조)&lt;/li&gt;
&lt;li&gt;MVM의 출력은 MFU로 연결, MFU는 vector-vector operation, activation등을 수행&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hierarchical-decode-and-dispatch-hdd&#34;&gt;Hierarchical Decode and Dispatch (HDD)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Catapult Fabric은 전통적인 scalar processor(Single Instruction Single Data)를 채택&lt;/li&gt;
&lt;li&gt;scheduler는 6개의 Decoder가 4-layer구조로 배치 (상세 내용은 책 참조)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sparse-matrix-vector-multiplication-smvm&#34;&gt;Sparse Matrix-Vector Multiplication (SMVM)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SMVM을 위해 Condensed Interleaved Sparse Representation (CISR) encoding 사용
&lt;ul&gt;
&lt;li&gt;Compressed Sparse Raw (CSR) 포맷이 가변 row 길이로 인해 Parallel contorl이 어려움을 극복&lt;/li&gt;
&lt;li&gt;첫 번째 0이 아닌 요소가 첫 번째 슬롯에 배치, 해당 열 인덱스는 인덱스 배열에서 동일한 순서로 배치, 행 요소가 모두 사용되면 다음 두 행 요소가 빈 슬롯에 할당, 이를 반복
(상세 내용은 책 참조,사실 이해가 잘 안감)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>[AI HW Design] Chap03 Parallel Architecture (2/3)</title>
        <link>https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/</link>
        <pubDate>Fri, 30 Dec 2022 00:00:00 +0000</pubDate>
        
        <guid>https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/</guid>
        <description>&lt;p&gt;본 챕터에서는 몇가지 주요한 Paralle Architecture에 대하여 소개한다.
이 페이지에서는 NVDLA와 Google TPU에 대해서 기술한다.&lt;/p&gt;
&lt;h2 id=&#34;nvidia-deep-learning-accelerator-nvdla&#34;&gt;NVIDIA Deep Learning Accelerator (NVDLA)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA는 FPGA로 구성 가능한 추론을 위한 오픈소스 아키텍쳐 (&lt;a class=&#34;link&#34; href=&#34;http://nvdla.org&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://nvdla.org&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Primitive functional blocks으로 CNN을 지원 (convolution, activation, pooling, normalization)&lt;/li&gt;
&lt;li&gt;각 블럭은 next layer의 active와 configuration을 위한 double buffer를 가짐&lt;/li&gt;
&lt;li&gt;next layer의 operation은 active operation이 완료되어야 시작&lt;/li&gt;
&lt;li&gt;independent mode와 pipeline을 사용하는 fused mode가 있음
&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/NVDLA_core_architecture.png&#34;
	width=&#34;1142&#34;
	height=&#34;950&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/NVDLA_core_architecture_hu2cca1af52d961361ac30bee8eb97a9b7_113988_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/NVDLA_core_architecture_hu2cca1af52d961361ac30bee8eb97a9b7_113988_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;NVDLA core architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;120&#34;
		data-flex-basis=&#34;288px&#34;
	
&gt;
&lt;em&gt;Figure. NVDLA core architecture&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convolution-operation&#34;&gt;Convolution Operation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Direct convolution, Image input convolution, winograd convolution, Batch convolution 지원 (상세내역은 책 참조)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;single-data-point-operationsdp&#34;&gt;Single Data Point Operation(SDP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SDP는 linear functions와 Look-up Table nonlinear functions을 통해 activation과 normalizatin을 지원 (상세내역은 책 참조)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;planar-data-operationpdp&#34;&gt;Planar Data Operation(PDP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;PDP는 maximum/minimum/average pooling을 지원&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multiplane-operation&#34;&gt;Multiplane Operation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cross Channel Data Processor(CPD)은 Local Response Normalization(LRN)을 수행&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-memory-and-reshape-operations&#34;&gt;Data Memory and Reshape Operations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;bridge DMA는 외부 메모리와 메모리 인터페이스간 데이터 전송을 담당&lt;/li&gt;
&lt;li&gt;data reshape engine은 data trasnformations, splitting, slicing, merging, contraction, reshape transpose 를 담당&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;system-configuration&#34;&gt;System Configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA는 small/large system model로 구현할 수 있음
&lt;ul&gt;
&lt;li&gt;small system model : IoT 기기와 같이 작은 모델을 위한 모델, 복잡도와 storage를 낮추고 single task를 수행&lt;/li&gt;
&lt;li&gt;large system model : mutiple task를 위한 coprocessor와 메모리 인터페이스 추가&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;external-interface&#34;&gt;External Interface&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA는 외부와 통신을 위한 Configuration Space Bus(CSB), Data backbone(DBB), SRAM interface, Interrupt interface를 가짐 (상세내용은 책 참조)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;software-design&#34;&gt;Software Design&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA SW는 Trained model을 parser/compiler/optimizer를 통해 loadable로 변환&lt;/li&gt;
&lt;li&gt;User Mode Driver(UMD)에 의해 Loadalbe이 로딩 되고 Job이 Kernel Mode Driver(KMD)로 제출됨, KMD는 스케줄링 수행&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;google-tensor-processing-unittpu&#34;&gt;Google Tensor Processing Unit(TPU)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;구글은 speech recognition 수요 해결을 위해 TPU v1(stand alone)과 v2/v3(cloud)를 개발&lt;/li&gt;
&lt;li&gt;TPU v1은 하기 스펙으로 MLP 0/1, CNN 0/1, RNN 0/1 6가지 neural network application을 수행 가능
&lt;ul&gt;
&lt;li&gt;256 × 256 eight bits MAC unit&lt;/li&gt;
&lt;li&gt;4 Mb on-chip Accumulator Memory (AM)&lt;/li&gt;
&lt;li&gt;24 Mb Unified Buffer (UB) – activation memory&lt;/li&gt;
&lt;li&gt;8 Gb off-chip weight DRAM memory&lt;/li&gt;
&lt;li&gt;Two 2133 MHz DDR3 channels&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TPU는 6가지 neural network application을 수행할 수 있음
&lt;ul&gt;
&lt;li&gt;Multi-layer perceptron(MLP) 0/1, Convolution Neural Network(CNN) 0/1, Recurrent Neural Network(RNN) 0/1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;system-architecture&#34;&gt;System Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;TPU v1은 매트릭스 연산을 Matrix Multiply Unit(MMU)에서 수행&lt;/li&gt;
&lt;li&gt;MMU는 256 × 256 eight bits MAC unit이며 16bit 연산을 수행할 경우 성능은 8bit 대비 절반(Sparse matrix 연산을 지원하지 않음)&lt;/li&gt;
&lt;li&gt;Weight FIFO는 matrix weight를 8Gb DRAM에서 읽어오며 activation, pooling, normalization 후 중간 연산 결과를 24Mb Unified Buffer에 저장&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/tpu-15dly1.max-500x500.PNG&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;TPU System Architecture&#34;
	
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multipy-accumulatemac-systolic-array&#34;&gt;Multipy-Accumulate(MAC) Systolic Array&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Systolic array는 TPU의 핵심이자 High throughput / low latency를 가진 SIMD pipeline.&lt;/li&gt;
&lt;li&gt;책에 별 설명이 없으므로 이에 대한 내용은 더 찾아보는 것이 좋다(다른 많은 곳에 잘 나와 있음)&lt;/li&gt;
&lt;li&gt;단점은 전력 소모가 많다는 것(데이터 센터 등에 적합)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;new-brain-floating-point-format&#34;&gt;New Brain Floating-point Format&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;TPU v1은 input data를 FP32에서 INT8로 quantization하여 연산하며 이에 따라 안정성/정확도 문제 발생&lt;/li&gt;
&lt;li&gt;이를 위해 IEEE FP16 대신 Brain Floating Point format (BFP16) 사용
&lt;ul&gt;
&lt;li&gt;BFP16 : Mantissa를 7bit으로 줄이고 exponent를 FP32와 같은 8bit으로 늘림&lt;/li&gt;
&lt;li&gt;Sign 1bit, Exponent 8bit, Mantissa 7bit&lt;/li&gt;
&lt;li&gt;multiplier area와 power를 줄이고 Scaling loss 없이 FP32와 동일한 정확도를 얻음&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;performance-comparision&#34;&gt;Performance Comparision&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;roof-line model의 관점에서 볼 때 TPU가 가장 높은 peak performance를 달성했다.&lt;/li&gt;
&lt;li&gt;roof-line model은 Y축이 성능을 나타내며(평평한 부분이 최고 성능), X축이 byte 당 operation intensity&lt;/li&gt;
&lt;li&gt;부가 설명을 하자면 한번에 얼마나 많은 연산을 수행하게 할 때 성능이 어디까지 올라가는지 지표, loof-line은 메모리 Bandwith 때문에 걸림&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cloud-tpu-configuration&#34;&gt;Cloud TPU configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;TPU v2/v3는 v1에서 DDR을 HBM으로 바꾸고 v1을 Pod로 묶음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cloud-software-architecture&#34;&gt;Cloud Software Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;구글은 cloud computation을 위해 새로운 SW 아키텍쳐를 개발&lt;/li&gt;
&lt;li&gt;Model을 TensorFlow를 통해 computational graph로 해석&lt;/li&gt;
&lt;li&gt;상세 내용은 책을 참조&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>[AI HW Design] Chap03 Parallel Architecture (1/3)</title>
        <link>https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-1/3/</link>
        <pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate>
        
        <guid>https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-1/3/</guid>
        <description>&lt;p&gt;본 챕터에서는 몇가지 주요한 Paralle Architecture에 대하여 소개한다.
이 페이지에서는 CPU와 GPU에 대해서 우선 기술한다.&lt;/p&gt;
&lt;h2 id=&#34;intel-central-processing-unit-cpu&#34;&gt;Intel Central Processing Unit (CPU)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.intel.com/content/www/us/en/developer/articles/technical/xeon-processor-scalable-family-technical-overview.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.intel.com/content/www/us/en/developer/articles/technical/xeon-processor-scalable-family-technical-overview.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CPU는 병렬 프로세싱을 위해 Single Instruction Single Data (SISD) architecture에서 Single Instruction Multiple Data (SIMD)로 진화함.&lt;/li&gt;
&lt;li&gt;그러나 이는 딥러닝과 같은 거대 병렬 처리에 적합하지 못하여 2017년 딥러닝 어플리케이션을 위한 Xeon processor scalable family (purley platform) 발표&lt;/li&gt;
&lt;li&gt;Purley platform은 하기 특징을 가짐&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;skylake-mesh-architecture&#34;&gt;Skylake mesh architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;이전 Grantley platform에서는 Last-Level Chache(LLC)등이 Intel Quick Path Interconnect(QPI) ring achitecture로 연결&lt;/li&gt;
&lt;li&gt;상기 구조는 코어 증가 시 코어 마다 사용가능한 bandwidth가 줄어들어서 메모리 latency가 증가&lt;/li&gt;
&lt;li&gt;Grantley platform에서는 Intel Ultra Path Interconnect(UPI) mesh archictecture로 업그레이드&lt;/li&gt;
&lt;li&gt;Comnined Home Agent(CHA)가 통합, 이는 LLC 등의 주소 정보 지도를 작성하며 이는 mesh 연결에서 목적지까지의 라우팅 정보를 제공
&lt;img src=&#34;https://www.intel.com/content/dam/develop/external/us/en/images/xeon-processor-scalable-family-tech-overview-fig05-737410.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Intel Xeon processor Scalable family mesh architecture&#34;
	
	
&gt;
&lt;em&gt;Fig1. Intel Xeon processor Scalable family mesh architecture&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;intel-ultra-path-interconnect&#34;&gt;Intel Ultra Path Interconnect&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;UPI는 어드레스를 공유하는 mutiple processor coherent interconnect&lt;/li&gt;
&lt;li&gt;UPI는 vertical/horizontal path를 통한 한 코어에서 다른 코어로의 최단 경로를 제공&lt;/li&gt;
&lt;li&gt;2소켓, 4소켓 링, 8소켓+크로스바 등 다양한 구조 지원&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;subnon-unified-memory-access-clustering&#34;&gt;SubNon-Unified Memory Access Clustering&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;플랫폼은 모든 코어/LLC를 반씩 + 메모리 컨트롤를 1개씩 가진 SNC 0,1 도메인을 가짐&lt;/li&gt;
&lt;li&gt;각 도메인은 각 메모리 컨트롤러에 매핑되는 유니크한 LLC 주소를 가지며 이는 LLC access latency를 낮춤&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cache-hierarchy-change&#34;&gt;Cache Hierarchy Change&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;하기 그림과 같이 변경 LLC 및 MLC size 변경으로 hit rate 증가
&lt;img src=&#34;https://www.intel.com/content/dam/develop/external/us/en/images/xeon-processor-scalable-family-tech-overview-fig11-737410.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Generational cache comparison&#34;
	
	
&gt;
&lt;em&gt;Figure 11. Generational cache comparison&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;singlemultiple-socket-parallel-processing&#34;&gt;single/Multiple Socket Parallel Processing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;UPI와 sub-NUMA의 지원으로 딥러닝 worker process들은 코어셋이나 싱글소켓, 다중소켓에 assign 될 수 있음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;advanced-vector-software-extension&#34;&gt;Advanced vector software extension&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Intel Advanced Vector Extension 512(Intel AVX-512)가 Vector Neural Network Instruction(VNNI)를 지원하는 AVX-512)_VNNI로 발전&lt;/li&gt;
&lt;li&gt;대충 더 빨라지고 8/16/32 FP vector 연산을 지원한다는 듯(자세한 사항은 책 참조)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;math-kernel-library-for-deep-neural-networkmkl-dnn&#34;&gt;Math Kernel Library for Deep Neural Network(MKL-DNN)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Convolution, pooling, activation, batch normalization으로 구성된 최적화된 MKL-DNN 지원&lt;/li&gt;
&lt;li&gt;key feature는 prefetching, data reuse, cache blocking, data layout, vectorization, register blocking이며 자세한 사항은 책 참조&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;nvidia-graphics-processing-unit-gpu&#34;&gt;NVIDIA Graphics Processing Unit (GPU)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GPU 장점 : 효율적인 floating point 연산, high speed memory support&lt;/li&gt;
&lt;li&gt;Turing architecture를 개발함 (NVLink2를 위한 HBM2 적용, 캐시 구조 변경 등등)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tensor-core-architecture&#34;&gt;Tensor Core Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;tensor core란 : 행렬연산 및 MAC를 위한 전용 코어&lt;/li&gt;
&lt;li&gt;Turing Tensor core는 이전(Pascal)이 matrix row by row만 지원했으나 4X4X4 연산을 지원하도록 변경&lt;/li&gt;
&lt;li&gt;INT8, INT4를 지원하며 정확도를 낮추면 연산 속도 증가&lt;/li&gt;
&lt;li&gt;Matrix사이즈가 크면 이를 나누어 연산, 다양한 size의 매트릭스 연산에 대응 가능&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.nvidia.com/ko-kr/data-center/tensor-cores/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.nvidia.com/ko-kr/data-center/tensor-cores/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;winograd-transform&#34;&gt;Winograd Transform&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;곱셈 횟수를 줄일 수 있는 Winograd Transform을 지원&lt;/li&gt;
&lt;li&gt;상기 변환에 대한 연산식은 책과 다른 자료를 참조할 것&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;simultaneous-multithreading-smt&#34;&gt;Simultaneous Multithreading (SMT)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SMT의 경우 Matrix는 행렬을 여러 그룹으로 나누고 이를 병렬로 처리 (Single Instruction Multiple Thread, SIMT 방식)&lt;/li&gt;
&lt;li&gt;연산 후 하위 그룹을 재그룹 시킴&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;high-bandwidth-memory-hbm2&#34;&gt;High Bandwidth Memory (HBM2)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Memory Bottleneck해결을 위해 HBM2 적용 (memory die를 TSV로 뚫어서 스택함)&lt;/li&gt;
&lt;li&gt;HBM2는 GPU와 NVLink2로 연결됨&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;nvlink2-configuration&#34;&gt;NVLink2 Configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NVLink는 엔비디아가 개발한 와이어 기반 통신 프로토콜 시리얼 멀티 레인 근범위 통신 링크 (PCIE의 속도 문제 해결)&lt;/li&gt;
&lt;li&gt;Turing 아키텍쳐는 sing MIO를 two×8 bidirectional differential pair NVLink2로 대체&lt;/li&gt;
&lt;li&gt;CPU/GPU 메모리 간 directly load/store/atomic 가능 (데이터를 GPU메모리에서 바로 읽을 수 있고 CPU cache에 바로 저장 가능)&lt;/li&gt;
&lt;li&gt;다양한 구성을 지원한다. (책을 참조하자)&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
