<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>AI_HW_Design on MW Devlog</title>
        <link>https://muonkmu.github.io/categories/ai_hw_design/</link>
        <description>Recent content in AI_HW_Design on MW Devlog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Fri, 13 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://muonkmu.github.io/categories/ai_hw_design/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[AI HW Design] Chap03 Parallel Architecture (3/3)</title>
        <link>https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/</link>
        <pubDate>Fri, 13 Jan 2023 00:00:00 +0000</pubDate>
        
        <guid>https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/</guid>
        <description>&lt;p&gt;본 챕터에서는 몇가지 주요한 Paralle Architecture에 대하여 소개한다.
이 페이지에서는 Microsoft Catapult Fabric Accelerator에 대해서 기술한다.&lt;/p&gt;
&lt;h2 id=&#34;microsoft-catapult-fabric-accelerator&#34;&gt;Microsoft Catapult Fabric Accelerator&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;마이크로소프트는 Brainwave Project를 CPU와 FPGA를 쓰는 형태로 변경&lt;/li&gt;
&lt;li&gt;48개의 FPGA가 2개의 Half rack (pod)에 그룹화되고 네트워크로 연결&lt;/li&gt;
&lt;li&gt;Brainwave는 train된 DNN 모델을 Catapult Fabric이라 불리는 synthesized softcore에 컴파일 후 narrow precision approach를 적용&lt;/li&gt;
&lt;li&gt;모델의 파라메터는 softcore 내 상주&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/01_Microsoft_brainwave_cloud_architect.png&#34;
	width=&#34;951&#34;
	height=&#34;528&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/01_Microsoft_brainwave_cloud_architect_hu5005c84237c5c0d108925d87f29946ce_123068_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/01_Microsoft_brainwave_cloud_architect_hu5005c84237c5c0d108925d87f29946ce_123068_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;180&#34;
		data-flex-basis=&#34;432px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;system-configuration&#34;&gt;System Configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Catapult Fabric은 synthesized softcore이며 이는 RTL의 recompilation 없이 low level SW library를 통해 reconfigure 될 수 있다.&lt;/li&gt;
&lt;li&gt;Catapult Fabric은 Shell과 Role 두 파트로 나눌 수 있음 (상세사항은 책을 참조)
&lt;ul&gt;
&lt;li&gt;Shell : 모든 application에서 재사용 가능한 programmable logic (통신/off-chip 등의 인터페이스를 말하는 듯)&lt;/li&gt;
&lt;li&gt;Roll : Application logic (그림을 보면 Softcore를 지칭하는 듯)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;catapult-fabric-architecture&#34;&gt;Catapult Fabric Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Catapult Fabric은 Matrix-Vector Multiplier(MVM), MultiFunction Uint(MFU), Vector Arbitration Network로 구성
&lt;ul&gt;
&lt;li&gt;MVM : Matrix-Vector 및 Vector-Vector 연산 수행, PRF/VRF/MRF에 ifmap/fmap 저장&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;VAN : PRF/DRAM/IO-queue 간 데이터 전송 담당&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/02_The_Catapult_fabric_microarchitecture.png&#34;
	width=&#34;568&#34;
	height=&#34;543&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/02_The_Catapult_fabric_microarchitecture_hub0253fb48da88283ddf54bccb7267131_45778_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/02_The_Catapult_fabric_microarchitecture_hub0253fb48da88283ddf54bccb7267131_45778_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;104&#34;
		data-flex-basis=&#34;251px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;matrix-vector-multiplier&#34;&gt;Matrix-Vector Multiplier&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;FP16을 MS-FP8/MS-FP-9으로 변환하여 연산(mantissa가 2~3bit)&lt;/li&gt;
&lt;li&gt;input data는 VRF, filter weight는 MFR에 저장&lt;/li&gt;
&lt;li&gt;3개의 tile engine과 3개의 accumulator에서 병렬 연산을 지원(상세내용을 책을 참조)&lt;/li&gt;
&lt;li&gt;MVM의 출력은 MFU로 연결, MFU는 vector-vector operation, activation등을 수행&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hierarchical-decode-and-dispatch-hdd&#34;&gt;Hierarchical Decode and Dispatch (HDD)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Catapult Fabric은 전통적인 scalar processor(Single Instruction Single Data)를 채택&lt;/li&gt;
&lt;li&gt;scheduler는 6개의 Decoder가 4-layer구조로 배치 (상세 내용은 책 참조)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sparse-matrix-vector-multiplication-smvm&#34;&gt;Sparse Matrix-Vector Multiplication (SMVM)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SMVM을 위해 Condensed Interleaved Sparse Representation (CISR) encoding 사용
&lt;ul&gt;
&lt;li&gt;Compressed Sparse Raw (CSR) 포맷이 가변 row 길이로 인해 Parallel contorl이 어려움을 극복&lt;/li&gt;
&lt;li&gt;첫 번째 0이 아닌 요소가 첫 번째 슬롯에 배치, 해당 열 인덱스는 인덱스 배열에서 동일한 순서로 배치, 행 요소가 모두 사용되면 다음 두 행 요소가 빈 슬롯에 할당, 이를 반복
(상세 내용은 책 참조,사실 이해가 잘 안감)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>[AI HW Design] Chap03 Parallel Architecture (2/3)</title>
        <link>https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/</link>
        <pubDate>Fri, 30 Dec 2022 00:00:00 +0000</pubDate>
        
        <guid>https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/</guid>
        <description>&lt;p&gt;본 챕터에서는 몇가지 주요한 Paralle Architecture에 대하여 소개한다.
이 페이지에서는 NVDLA와 Google TPU에 대해서 기술한다.&lt;/p&gt;
&lt;h2 id=&#34;nvidia-deep-learning-accelerator-nvdla&#34;&gt;NVIDIA Deep Learning Accelerator (NVDLA)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA는 FPGA로 구성 가능한 추론을 위한 오픈소스 아키텍쳐 (&lt;a class=&#34;link&#34; href=&#34;http://nvdla.org&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://nvdla.org&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Primitive functional blocks으로 CNN을 지원 (convolution, activation, pooling, normalization)&lt;/li&gt;
&lt;li&gt;각 블럭은 next layer의 active와 configuration을 위한 double buffer를 가짐&lt;/li&gt;
&lt;li&gt;next layer의 operation은 active operation이 완료되어야 시작&lt;/li&gt;
&lt;li&gt;independent mode와 pipeline을 사용하는 fused mode가 있음
&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/NVDLA_core_architecture.png&#34;
	width=&#34;1142&#34;
	height=&#34;950&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/NVDLA_core_architecture_hu2cca1af52d961361ac30bee8eb97a9b7_113988_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/NVDLA_core_architecture_hu2cca1af52d961361ac30bee8eb97a9b7_113988_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;NVDLA core architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;120&#34;
		data-flex-basis=&#34;288px&#34;
	
&gt;
&lt;em&gt;Figure. NVDLA core architecture&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convolution-operation&#34;&gt;Convolution Operation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Direct convolution, Image input convolution, winograd convolution, Batch convolution 지원 (상세내역은 책 참조)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;single-data-point-operationsdp&#34;&gt;Single Data Point Operation(SDP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SDP는 linear functions와 Look-up Table nonlinear functions을 통해 activation과 normalizatin을 지원 (상세내역은 책 참조)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;planar-data-operationpdp&#34;&gt;Planar Data Operation(PDP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;PDP는 maximum/minimum/average pooling을 지원&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multiplane-operation&#34;&gt;Multiplane Operation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cross Channel Data Processor(CPD)은 Local Response Normalization(LRN)을 수행&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-memory-and-reshape-operations&#34;&gt;Data Memory and Reshape Operations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;bridge DMA는 외부 메모리와 메모리 인터페이스간 데이터 전송을 담당&lt;/li&gt;
&lt;li&gt;data reshape engine은 data trasnformations, splitting, slicing, merging, contraction, reshape transpose 를 담당&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;system-configuration&#34;&gt;System Configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA는 small/large system model로 구현할 수 있음
&lt;ul&gt;
&lt;li&gt;small system model : IoT 기기와 같이 작은 모델을 위한 모델, 복잡도와 storage를 낮추고 single task를 수행&lt;/li&gt;
&lt;li&gt;large system model : mutiple task를 위한 coprocessor와 메모리 인터페이스 추가&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;external-interface&#34;&gt;External Interface&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA는 외부와 통신을 위한 Configuration Space Bus(CSB), Data backbone(DBB), SRAM interface, Interrupt interface를 가짐 (상세내용은 책 참조)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;software-design&#34;&gt;Software Design&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA SW는 Trained model을 parser/compiler/optimizer를 통해 loadable로 변환&lt;/li&gt;
&lt;li&gt;User Mode Driver(UMD)에 의해 Loadalbe이 로딩 되고 Job이 Kernel Mode Driver(KMD)로 제출됨, KMD는 스케줄링 수행&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;google-tensor-processing-unittpu&#34;&gt;Google Tensor Processing Unit(TPU)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;구글은 speech recognition 수요 해결을 위해 TPU v1(stand alone)과 v2/v3(cloud)를 개발&lt;/li&gt;
&lt;li&gt;TPU v1은 하기 스펙으로 MLP 0/1, CNN 0/1, RNN 0/1 6가지 neural network application을 수행 가능
&lt;ul&gt;
&lt;li&gt;256 × 256 eight bits MAC unit&lt;/li&gt;
&lt;li&gt;4 Mb on-chip Accumulator Memory (AM)&lt;/li&gt;
&lt;li&gt;24 Mb Unified Buffer (UB) – activation memory&lt;/li&gt;
&lt;li&gt;8 Gb off-chip weight DRAM memory&lt;/li&gt;
&lt;li&gt;Two 2133 MHz DDR3 channels&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TPU는 6가지 neural network application을 수행할 수 있음
&lt;ul&gt;
&lt;li&gt;Multi-layer perceptron(MLP) 0/1, Convolution Neural Network(CNN) 0/1, Recurrent Neural Network(RNN) 0/1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;system-architecture&#34;&gt;System Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;TPU v1은 매트릭스 연산을 Matrix Multiply Unit(MMU)에서 수행&lt;/li&gt;
&lt;li&gt;MMU는 256 × 256 eight bits MAC unit이며 16bit 연산을 수행할 경우 성능은 8bit 대비 절반(Sparse matrix 연산을 지원하지 않음)&lt;/li&gt;
&lt;li&gt;Weight FIFO는 matrix weight를 8Gb DRAM에서 읽어오며 activation, pooling, normalization 후 중간 연산 결과를 24Mb Unified Buffer에 저장&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/tpu-15dly1.max-500x500.PNG&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;TPU System Architecture&#34;
	
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multipy-accumulatemac-systolic-array&#34;&gt;Multipy-Accumulate(MAC) Systolic Array&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Systolic array는 TPU의 핵심이자 High throughput / low latency를 가진 SIMD pipeline.&lt;/li&gt;
&lt;li&gt;책에 별 설명이 없으므로 이에 대한 내용은 더 찾아보는 것이 좋다(다른 많은 곳에 잘 나와 있음)&lt;/li&gt;
&lt;li&gt;단점은 전력 소모가 많다는 것(데이터 센터 등에 적합)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;new-brain-floating-point-format&#34;&gt;New Brain Floating-point Format&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;TPU v1은 input data를 FP32에서 INT8로 quantization하여 연산하며 이에 따라 안정성/정확도 문제 발생&lt;/li&gt;
&lt;li&gt;이를 위해 IEEE FP16 대신 Brain Floating Point format (BFP16) 사용
&lt;ul&gt;
&lt;li&gt;BFP16 : Mantissa를 7bit으로 줄이고 exponent를 FP32와 같은 8bit으로 늘림&lt;/li&gt;
&lt;li&gt;Sign 1bit, Exponent 8bit, Mantissa 7bit&lt;/li&gt;
&lt;li&gt;multiplier area와 power를 줄이고 Scaling loss 없이 FP32와 동일한 정확도를 얻음&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;performance-comparision&#34;&gt;Performance Comparision&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;roof-line model의 관점에서 볼 때 TPU가 가장 높은 peak performance를 달성했다.&lt;/li&gt;
&lt;li&gt;roof-line model은 Y축이 성능을 나타내며(평평한 부분이 최고 성능), X축이 byte 당 operation intensity&lt;/li&gt;
&lt;li&gt;부가 설명을 하자면 한번에 얼마나 많은 연산을 수행하게 할 때 성능이 어디까지 올라가는지 지표, loof-line은 메모리 Bandwith 때문에 걸림&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cloud-tpu-configuration&#34;&gt;Cloud TPU configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;TPU v2/v3는 v1에서 DDR을 HBM으로 바꾸고 v1을 Pod로 묶음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cloud-software-architecture&#34;&gt;Cloud Software Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;구글은 cloud computation을 위해 새로운 SW 아키텍쳐를 개발&lt;/li&gt;
&lt;li&gt;Model을 TensorFlow를 통해 computational graph로 해석&lt;/li&gt;
&lt;li&gt;상세 내용은 책을 참조&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>[AI HW Design] Chap03 Parallel Architecture (1/3)</title>
        <link>https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-1/3/</link>
        <pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate>
        
        <guid>https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-1/3/</guid>
        <description>&lt;p&gt;본 챕터에서는 몇가지 주요한 Paralle Architecture에 대하여 소개한다.
이 페이지에서는 CPU와 GPU에 대해서 우선 기술한다.&lt;/p&gt;
&lt;h2 id=&#34;intel-central-processing-unit-cpu&#34;&gt;Intel Central Processing Unit (CPU)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.intel.com/content/www/us/en/developer/articles/technical/xeon-processor-scalable-family-technical-overview.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.intel.com/content/www/us/en/developer/articles/technical/xeon-processor-scalable-family-technical-overview.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CPU는 병렬 프로세싱을 위해 Single Instruction Single Data (SISD) architecture에서 Single Instruction Multiple Data (SIMD)로 진화함.&lt;/li&gt;
&lt;li&gt;그러나 이는 딥러닝과 같은 거대 병렬 처리에 적합하지 못하여 2017년 딥러닝 어플리케이션을 위한 Xeon processor scalable family (purley platform) 발표&lt;/li&gt;
&lt;li&gt;Purley platform은 하기 특징을 가짐&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;skylake-mesh-architecture&#34;&gt;Skylake mesh architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;이전 Grantley platform에서는 Last-Level Chache(LLC)등이 Intel Quick Path Interconnect(QPI) ring achitecture로 연결&lt;/li&gt;
&lt;li&gt;상기 구조는 코어 증가 시 코어 마다 사용가능한 bandwidth가 줄어들어서 메모리 latency가 증가&lt;/li&gt;
&lt;li&gt;Grantley platform에서는 Intel Ultra Path Interconnect(UPI) mesh archictecture로 업그레이드&lt;/li&gt;
&lt;li&gt;Comnined Home Agent(CHA)가 통합, 이는 LLC 등의 주소 정보 지도를 작성하며 이는 mesh 연결에서 목적지까지의 라우팅 정보를 제공
&lt;img src=&#34;https://www.intel.com/content/dam/develop/external/us/en/images/xeon-processor-scalable-family-tech-overview-fig05-737410.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Intel Xeon processor Scalable family mesh architecture&#34;
	
	
&gt;
&lt;em&gt;Fig1. Intel Xeon processor Scalable family mesh architecture&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;intel-ultra-path-interconnect&#34;&gt;Intel Ultra Path Interconnect&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;UPI는 어드레스를 공유하는 mutiple processor coherent interconnect&lt;/li&gt;
&lt;li&gt;UPI는 vertical/horizontal path를 통한 한 코어에서 다른 코어로의 최단 경로를 제공&lt;/li&gt;
&lt;li&gt;2소켓, 4소켓 링, 8소켓+크로스바 등 다양한 구조 지원&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;subnon-unified-memory-access-clustering&#34;&gt;SubNon-Unified Memory Access Clustering&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;플랫폼은 모든 코어/LLC를 반씩 + 메모리 컨트롤를 1개씩 가진 SNC 0,1 도메인을 가짐&lt;/li&gt;
&lt;li&gt;각 도메인은 각 메모리 컨트롤러에 매핑되는 유니크한 LLC 주소를 가지며 이는 LLC access latency를 낮춤&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cache-hierarchy-change&#34;&gt;Cache Hierarchy Change&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;하기 그림과 같이 변경 LLC 및 MLC size 변경으로 hit rate 증가
&lt;img src=&#34;https://www.intel.com/content/dam/develop/external/us/en/images/xeon-processor-scalable-family-tech-overview-fig11-737410.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Generational cache comparison&#34;
	
	
&gt;
&lt;em&gt;Figure 11. Generational cache comparison&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;singlemultiple-socket-parallel-processing&#34;&gt;single/Multiple Socket Parallel Processing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;UPI와 sub-NUMA의 지원으로 딥러닝 worker process들은 코어셋이나 싱글소켓, 다중소켓에 assign 될 수 있음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;advanced-vector-software-extension&#34;&gt;Advanced vector software extension&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Intel Advanced Vector Extension 512(Intel AVX-512)가 Vector Neural Network Instruction(VNNI)를 지원하는 AVX-512)_VNNI로 발전&lt;/li&gt;
&lt;li&gt;대충 더 빨라지고 8/16/32 FP vector 연산을 지원한다는 듯(자세한 사항은 책 참조)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;math-kernel-library-for-deep-neural-networkmkl-dnn&#34;&gt;Math Kernel Library for Deep Neural Network(MKL-DNN)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Convolution, pooling, activation, batch normalization으로 구성된 최적화된 MKL-DNN 지원&lt;/li&gt;
&lt;li&gt;key feature는 prefetching, data reuse, cache blocking, data layout, vectorization, register blocking이며 자세한 사항은 책 참조&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;nvidia-graphics-processing-unit-gpu&#34;&gt;NVIDIA Graphics Processing Unit (GPU)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GPU 장점 : 효율적인 floating point 연산, high speed memory support&lt;/li&gt;
&lt;li&gt;Turing architecture를 개발함 (NVLink2를 위한 HBM2 적용, 캐시 구조 변경 등등)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tensor-core-architecture&#34;&gt;Tensor Core Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;tensor core란 : 행렬연산 및 MAC를 위한 전용 코어&lt;/li&gt;
&lt;li&gt;Turing Tensor core는 이전(Pascal)이 matrix row by row만 지원했으나 4X4X4 연산을 지원하도록 변경&lt;/li&gt;
&lt;li&gt;INT8, INT4를 지원하며 정확도를 낮추면 연산 속도 증가&lt;/li&gt;
&lt;li&gt;Matrix사이즈가 크면 이를 나누어 연산, 다양한 size의 매트릭스 연산에 대응 가능&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.nvidia.com/ko-kr/data-center/tensor-cores/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.nvidia.com/ko-kr/data-center/tensor-cores/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;winograd-transform&#34;&gt;Winograd Transform&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;곱셈 횟수를 줄일 수 있는 Winograd Transform을 지원&lt;/li&gt;
&lt;li&gt;상기 변환에 대한 연산식은 책과 다른 자료를 참조할 것&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;simultaneous-multithreading-smt&#34;&gt;Simultaneous Multithreading (SMT)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SMT의 경우 Matrix는 행렬을 여러 그룹으로 나누고 이를 병렬로 처리 (Single Instruction Multiple Thread, SIMT 방식)&lt;/li&gt;
&lt;li&gt;연산 후 하위 그룹을 재그룹 시킴&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;high-bandwidth-memory-hbm2&#34;&gt;High Bandwidth Memory (HBM2)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Memory Bottleneck해결을 위해 HBM2 적용 (memory die를 TSV로 뚫어서 스택함)&lt;/li&gt;
&lt;li&gt;HBM2는 GPU와 NVLink2로 연결됨&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;nvlink2-configuration&#34;&gt;NVLink2 Configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NVLink는 엔비디아가 개발한 와이어 기반 통신 프로토콜 시리얼 멀티 레인 근범위 통신 링크 (PCIE의 속도 문제 해결)&lt;/li&gt;
&lt;li&gt;Turing 아키텍쳐는 sing MIO를 two×8 bidirectional differential pair NVLink2로 대체&lt;/li&gt;
&lt;li&gt;CPU/GPU 메모리 간 directly load/store/atomic 가능 (데이터를 GPU메모리에서 바로 읽을 수 있고 CPU cache에 바로 저장 가능)&lt;/li&gt;
&lt;li&gt;다양한 구성을 지원한다. (책을 참조하자)&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
