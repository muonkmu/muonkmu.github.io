[{"content":"Introduction to Module Serialization Serialization Under the Hood of Serialization and Format Standard Deserialization Device/Target Interactions DeviceAPI Target Definition Target Code Generators ","date":"2025-05-20T00:00:00Z","permalink":"https://muonkmu.github.io/p/tvm_0.20-04-runtime-and-device-interaction-part2/","title":"[TVM_0.20] 04 Runtime and Device Interaction Part2"},{"content":"TVM Runtime System TVM runtime의 주요 요구사항은 다음과 같다.\nDeployment: 컴파일된 함수를 python/javscript/c++ 언어에서 호출 Debug: python으로 함수를 정의하고 이를 컴파일된 함수에서 호출 Link: device specific code (CUDA등)을 호출하도록 드라이버 코드를 작성하고 이를 컴파일된 호스트 함수에서 호출 Prototype: python에서 IR pass를 정의하고 이를 C++ backend에서 호출 Expose: python같은 front-end를 가지는 c++로 개발된 컴파일러 스택 Experiment: 컴파일된 함수를 타켓으로 전송하여 타겟에서 직접 실행\nPackedFunc PackedFunc 객체는 caller와 callee가 다른 언어가 될 수 있는 function call을 나타냄.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #include \u0026lt;tvm/runtime/packed_func.h\u0026gt; void MyAdd(TVMArgs args, TVMRetValue* rv) { // automatically convert arguments to desired type. int a = args[0]; int b = args[1]; // automatically assign value return to rv *rv = a + b; } void CallPacked() { PackedFunc myadd = PackedFunc(MyAdd); // get back 3 int c = myadd(1, 2); } 위의 코드에서 서 PackedFunc는 MyAdd이고 다음과 같은 특징을 지님\n두 개의 Argument(arg, rv)를 지니며 type-erased function 이다. 즉 함수의 입력 유형과 반환 유형을 제한하지 않음 내부적으로 PackedFunc를 호출 시, input argument를 스택의 TVMArg에 패킹하고 결과를 TVMRetValue을 통해 반환 C++의 Template 문법 덕택에 PackedFunc을 python 같은 dynamic language에서 glue 코드 없이 일반 함수 호출하듯이 콜할 수 있다.\n1 2 3 // register a global packed function in c++ TVM_REGISTER_GLOBAL(\u0026#34;myadd\u0026#34;) .set_body(MyAdd); 1 2 3 4 5 6 # packedfunction 호출 import tvm myadd = tvm.get_global_func(\u0026#34;myadd\u0026#34;) # prints 3 print(myadd(1, 2)) 상기와 같은 호출 구조가 가능한 이유는 TVMArgs와 TVMRetValue 구조 덕분인데 TVM은 두 구조의 type을 아래와 같이 제한함\nint, float and string PackedFunc itself Module for compiled modules DLTensor* for tensor object exchange TVM Object to represent any object in IR 위의 제약들은 serialization이 필요 없이 구현을 간편하게 하기 위함이다. 그러나 이러한 제약에도 불구하고 딥러닝 deploy에서 대부분 DLTensor 또는 int/float만 사용하므로 사용하기 충분\n하나의 PackedFunc가 다른 PackedFunc를 인수로 사용할 수 있으므로, 우리는 함수를 python(PackedFunc)에서 C++로 전달할 수 있습니다.\n1 2 3 4 5 TVM_REGISTER_GLOBAL(\u0026#34;callhello\u0026#34;) .set_body([](TVMArgs args, TVMRetValue* rv) { PackedFunc f = args[0]; f(\u0026#34;hello world\u0026#34;); }); 1 2 3 4 5 6 7 8 9 10 import tvm def callback(msg): print(msg) # convert to PackedFunc f = tvm.convert(callback) callhello = tvm.get_global_func(\u0026#34;callhello\u0026#34;) # prints hello world callhello(f) TVM은 PackedFunc을 모든 언어에 임베디 할 수 있는 minimum C API을 제공 (python, java, javascript)\n이는 Lua랑 비슷(새로운 언어 대신 C++을 이용한다는 것을 제외하면) PackedFunc은 compiler와 deployment stack 양쪽에서 모두 쓰임\nTVM의 모든 compiler pass function은 frontend에 PackedFunc으로 노출됨 (역자주) TVM에서 내부적으로 사용하는 컴파일 패스 함수들 (예: Relay 최적화, 타입 추론 등)을 사용자가 Python 등에서 쓸 수 있도록 PackedFunc 형태로 포장해서 노출, C++에서 만든 내부 패스를 PackedFunc로 감싸면 Python에서 마치 일반 함수처럼 사용할 수 있게 됨 1 2 3 4 5 6 7 8 9 from tvm import relay mod = ... # relay IRModule mod = relay.transform.InferType()(mod) # 여기서 InferType() 은 Relay의 타입 추론 패스입니다. #내부적으로는 C++에 정의된 InferTypePass 같은 함수를 # → TVM_REGISTER_GLOBAL(\u0026#34;relay._transform.InferType\u0026#34;) 를 통해 # → PackedFunc 형태로 Python에 노출한 것입니다. 컴파일된 module도 컴파일된 함수를 PackedFunc로 반환 (역자주) TVM으로 모델을 컴파일하면, 그 결과물은 Module 객체로 나오는데, 그 안에 있는 각 함수(예: main, tvmgen_default_fused_\u0026hellip;)도 PackedFunc 형태로 제공 1 2 3 4 5 6 lib = tvm.build(...) # 모델 컴파일 f = lib[\u0026#34;main\u0026#34;] # \u0026#34;main\u0026#34; 함수 가져오기 f(input_tensor) # 실행! #여기서 lib[\u0026#34;main\u0026#34;]은 실제로 PackedFunc 객체입니다. #즉, lib[\u0026#34;main\u0026#34;]을 호출하면 내부적으로 PackedFunc.__call__()이 실행되어 모델 추론이 일어납니다. 런타임 크기를 최소화 하기 위해 배포 런타임에서 IR Object 지원을 제외함. 이를 통해 배포 런타임의 크기를 200K~600K 정도로 만들 수 있었음\n(역자주) TVM은 크게 컴파일러 영역 (IR 사용)과 런타임 영역 (IR 제거)으로 나누는데 런타임영역에서는 IR기능을 분리, 예를 들어 라즈베리파이에 모델을 실행시키기 위해 libtvm_runtime.so만 넣음, 이 안에는 PackedFunc, DLTensor, CUDA 호출기 등 필요한 것만 포함하며 Relay 같은 IR 처리 로직은 빠져 있음 PackedFunc를 호출하는데 드는 오버헤드는 스택에 몇 가지 값만 저장하면 되기 때문에 적다.요약하자면, PackedFunc는 컴파일러와 배포를 지원하기 위해 광범위하게 사용하는 TVM의 범용 glue임\nModule TVM은 여러 유형의 장치를 지원하기 때문에 다양한 유형의 드라이버를 지원해야 하며 다음과 같은 일을 해야함\n드라이버 API를 사용하여 kernel을 로드 argument를 packed format으로 셋업하고 kernel을 실행 사용된 함수가 threadsafe하도록 kernel을 패치 이러한 driver glue를 C++로 구현하여 사용자에게 제공\n그러나 각 디바이스의 모든 함수에 대해 상기 작업을 할 수 없으므로 PackedFunc을 사용 TVM은 컴파일된 객체를 Module로 정의하며 사용자는 Module로 부터 컴파일된 함수를 PackedFunc형식으로 얻을 수 있음. 생성된 컴파일된 코드는 런타임에 Module에서 동적으로 함수를 가져올 수 있다. 첫 호출 시에 function handle을 캐시하고 subsequent call에서 이를 재사용한다. 우리는 이것을 생성된 코드에서 PackedFunc(e.g., python)과 디바이이스 코드를 연결하는데 사용할 수 있다.\nModuleNode는 각 유형의 장치에서 구현할 수 있는 abstract class 입니다. 지금까지는 CUDA, Metal, OpenCL 및 동적 공유 라이브러리 로드를 위한 모듈을 지원했습니다. 이 추상화를 통해 새로운 장치를 쉽게 도입할 수 있으며, 각 유형의 장치에 대해 Host Code 생성을 다시 할 필요가 없습니다.\nRemote Deployment PackedFunc / Module system은 argument를 직렬화하고 원격에서 계산을 수행하는 RPCModule을 사용하여 리모트 디바이스로 직접 전송이 가능.\nRPC server는 runtime에 번들로 제공될 수 있으며 iPhone, Android, Raspberry pi, 브라우저에서 시작할 수 있다.\n이러한 구조(즉각적인 피드백이 가능한)는 많은 이점을 제공하는데 예를 들어 RPC를 사용하여 iPhone에서 실행하고, 결과를 다시 복사한 후 numpy를 통해 호스트에서 검증이 가능하기에 host의 테스트 케이스를 다시 작성할 필요가 없다.\nTVM Object and Compiler Stack 앞서 말했듯 compiler stack API 는 PackedFunc을 이용하여 구성됨. 새로운 primitive를 추가 할 때마다 새로운 Language object나 IR node가 필요했으나 TVM 개발자들은 API를 변경하는 대신 다음 내용을 원함\n어떤 language object나 IR들을 직렬화 할 수 있을 것 front-end language(ex. python)에서 IR object를 탐색, 출력, 조작 할 수 있을 것 상기 내용을 해결하기 위해 Base Class인 Object를 도입, 컴파일 스택의 모든 language object는 Object의 서브클래스 임 (역자주) primitive는 TVM에서 새로운 연산(operators), IR 노드, 최적화 단위, 메모리 표현, 실행 스케줄링 전략을 의미 (역자주) language object TVM 내부에서 모델을 표현하거나 최적화하는 데 사용되는 모든 추상 구조물(TIR, Relax, Type)을 의미 (역자주) 새로운 연산(primitive)을 실험하고 싶다면 Relay, TIR, Relax 중 해당 영역의 새로운 IR 노드 또는 그 노드를 감싸는 \u0026ldquo;language object\u0026rdquo; (예: ObjectRef 타입)를 만들어야 한다 각 Object는 Object의 Type을 식별하기 위한 고유 식별자인 String 형식의 type_Key를 가지고 있음\nint 대신 String을 선택한 이유는 centeral repo에 코드를 추가하지 않고 분산 방식으로 새로운 Object 클래스를 추가할 수 있도록. 그러나 Runtime에서는 디스패치 속도를 높이기 위해 type_Key에 대응하는 integer type index를 할당 하나의 Object는 여러 곳에서 참조되므로 Object의 reference를 나타내는 ObjectRef 클래스를 사용\nObjectRef는 Object 컨테이너의 shared_ptr 라고 볼 수 있음 Object의 subtype을 유지하면서 ObjectRef의 서브클래스를 정의할 수 있으며 이 서브 클래스는 VisitAttr를 정의(가상함수 오버라이딩) 해야함 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class AttrVisitor { public: virtual void Visit(const char* key, double* value) = 0; virtual void Visit(const char* key, int64_t* value) = 0; virtual void Visit(const char* key, uint64_t* value) = 0; virtual void Visit(const char* key, int* value) = 0; virtual void Visit(const char* key, bool* value) = 0; virtual void Visit(const char* key, std::string* value) = 0; virtual void Visit(const char* key, void** value) = 0; virtual void Visit(const char* key, Type* value) = 0; virtual void Visit(const char* key, ObjectRef* value) = 0; // ... }; class BaseAttrsNode : public Object { public: virtual void VisitAttrs(AttrVisitor* v) {} // ... }; 각 Object의 서브클래스는 자신의 각 멤버를 방문하기 위해 VisitAttr을 오버라이딩. 하기는 TensorNode 예제\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class TensorNode : public Object { public: /*! \\brief The shape of the tensor */ Array\u0026lt;Expr\u0026gt; shape; /*! \\brief data type in the content of the tensor */ Type dtype; /*! \\brief the source operation, can be None */ Operation op; /*! \\brief the output index from source operation */ int value_index{0}; /*! \\brief constructor */ TensorNode() {} void VisitAttrs(AttrVisitor* v) final { v-\u0026gt;Visit(\u0026#34;shape\u0026#34;, \u0026amp;shape); v-\u0026gt;Visit(\u0026#34;dtype\u0026#34;, \u0026amp;dtype); v-\u0026gt;Visit(\u0026#34;op\u0026#34;, \u0026amp;op); v-\u0026gt;Visit(\u0026#34;value_index\u0026#34;, \u0026amp;value_index); } }; 상기 예제에서 Operation, Array\u0026lt;Expr\u0026gt;은 ObjectRef이다. VisitAttrs는 각 멤버 object를 방문하기 위한 reflection API를 제공함\nVisitAttrs을 node를 방문하거나 language object를 재귀적으로 직렬화 하는데 사용 가능 front-end언어에서 object 멤버를 얻는데 사용 가능(하기는 TensorNode의 예제임) 1 2 3 4 5 6 import tvm from tvm import te x = te.placeholder((3,4), name=\u0026#34;x\u0026#34;) # access the op field of TensorNode print(x.op.name) front-end runtime을 변경하지 않고 새로운 Object를 C++에 추가할 수 있고, 추가된 Object를 위한 extention을 쉽게 컴파일 스택에 만들 수 있음, 이것은 object member를 expose하는 가장 빠른 방법은 아니나 가장 심플한 방법 중 하나임(TVM이 테스트를 위한 프로토타이핑에 python을 사용하고 내부적으로 무거운 작업에 C++를 사용하므로)\nImplementation Details PackedFunc의 각 argument는 union value인 TVMValue와 type code를 가지고 있음\n(역자주) TVM은 python에서 c++ 코드 호출을 위한 tvm ffi를 제공하며 PackedFunc에서는 argument로 ffi를 사용 Foreign Function Interface(FFI) : 한 프로그래밍 언어(이하 A)에서 다른 프로그래밍 언어(이하 B)의 코드를 호출하기 위한 인터페이스 이 디자인은 dynamically typed language를 이에 대응하는 type으로 직접 변환 할 수 있게, statically typed language는 변환 과정에서 runtime type checking을 수행할 수 있게함. 아래는 관련 파일임 C++ API : packed_func.h C API and how to provide callback. : c_runtime_api.cc Extension type을 지원하기 위해 type 정보를 등록하는 registry system을 사용, Extension types을 참조\n(역자주) 동적 타입 언어(dynamically typed language) : 변수를 선언할 때 타입을 명시하지 않아도 되며, 런타임 시에 타입이 결정되는 언어(파이썬), 동적 타입 언어는 값 자체에 타입 정보가 포함되어 있기 때문에, TVM의 TVMValue + type_code 구조로 쉽게 매핑됨 역자해설 TVM에서 Object와 PackedFunc는 런타임 시스템의 두 핵심 구성 요소이며 TVM이 유연하고 확장 가능한 컴파일러/런타임 환경을 제공하는 기반이 됨\nObject는 데이터를 표현하며 Relay/TIR/Relax 등 모든 TVM 언어의 노드, 타입, 표현식, 속성 등은 모두 Object의 서브클래스 PackedFunc는 함수를 표현하며 TVM 런타임 상에서 함수를 호출할 수 있는 유니버설 함수 래퍼, 모든 함수는 TVMArgs → TVMRetValue 형태로 표현되며, 타입 정보 없이도 호출 가능 TVM에서 **사용자 정의 타입(custom object type)**을 등록하면, 이 타입도 Object 시스템 및 PackedFunc 시스템과 자연스럽게 통합되어 Python/C++ 경계를 넘을 수 있다. 이것은 하기의 예시임 1단계 : 사용자 정의 객체 정의 (Object 기반) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // my_expr.h #include \u0026lt;tvm/runtime/object.h\u0026gt; #include \u0026lt;tvm/runtime/registry.h\u0026gt; namespace tvm { namespace myns { class MyExprNode : public Object { public: int value; void VisitAttrs(AttrVisitor* v) { v-\u0026gt;Visit(\u0026#34;value\u0026#34;, \u0026amp;value); } static constexpr const char* _type_key = \u0026#34;myns.MyExpr\u0026#34;; TVM_DECLARE_FINAL_OBJECT_INFO(MyExprNode, Object); }; class MyExpr : public ObjectRef { public: TVM_DEFINE_OBJECT_REF_METHODS(MyExpr, ObjectRef, MyExprNode); }; } // namespace myns } // namespace tvm 2단계: 타입 등록 (TVM의 RTTI 시스템에) 1 2 3 4 5 6 7 8 9 10 11 // my_expr.cc #include \u0026#34;my_expr.h\u0026#34; namespace tvm { namespace myns { TVM_REGISTER_OBJECT_TYPE(MyExprNode); } // namespace myns } // namespace tvm 3단계: PackedFunc으로 전달 가능한 함수 등록 1 2 3 4 TVM_REGISTER_GLOBAL(\u0026#34;myns.print_my_expr\u0026#34;) .set_body_typed([](tvm::myns::MyExpr e) { std::cout \u0026lt;\u0026lt; \u0026#34;MyExpr.value = \u0026#34; \u0026lt;\u0026lt; e-\u0026gt;value \u0026lt;\u0026lt; std::endl; }); 4단계: Python에서 사용 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from tvm.runtime import Object # TVM 내부적으로 Object를 상속한 Python 래퍼 사용 가능 MyExpr = tvm._ffi.get_global_func(\u0026#34;myns.MyExpr\u0026#34;) # 생성자 등록했을 경우 # 또는 C++에서 만든 객체를 받는 경우 print_func = tvm.get_global_func(\u0026#34;myns.print_my_expr\u0026#34;) # 임의로 생성한 객체가 있다고 가정하고 전달 class MyExpr(Object): def __init__(self, value): self.__init_handle_by_constructor__( \u0026#34;myns.MyExpr\u0026#34;, # C++에서 등록한 type_key value ) e = MyExpr(42) print_func(e) ","date":"2025-05-01T00:00:00Z","permalink":"https://muonkmu.github.io/p/tvm_0.20-04-runtime-and-device-interaction-part-1/","title":"[TVM_0.20] 04 Runtime and Device Interaction Part 1"},{"content":"Graph Abstraction for ML Models Graph abstraction은 데이터 flow와 구조를 나타내기 위한 Machine Learning 컴파일러의 주요 기술이다. 모델을 Graph representaion으로 추상화 함으로써 컴파일러는 다양한 최적화나 성능 향상을 수행할 수 있다.\nWhat is Graph Abstraction? Graph abstraction은 ML model을 Graph로 나타내기 위한 프로세스이며 컴파일러가 모델의 파트 사이의 dependency와 relation을 분석하게 해준다.\nnode : computational operations (e.g., matrix multiplication, convolution) edge : Operation 간 data의 흐름을 나타냄 Key Features of Relax First-class symbolic shape : Relax 는 Tensor의 차원을 표현할 때 Symblic shape를 사용. tensor operators와 function calls 간의 dynamic shape relationship을 전역 추적할 수 있게 함 (역자주) First-class는 프로그래밍에서 해당 요소가 함수의 인자 리턴 값으로 자유롭게 사용될 수 있고 컴파일러가 최적화에 활용될 수 있음을 의미, 즉 relax는 동적입력을 지원하고 이것이 컴파일러 단에서 최적화가 가능하며 전체 모델에서 Shape 분석이 가능함을 의미한다. TIR도 Dynamic Shape를 지원하지만 이는 함수에서 단순한 변수 추적일뿐 shape 추론은 수동으로 해야한다. 즉 shape관계추적, 전역 최적화는 어렵다. 즉 Relax는 이 텐서의 크기를 나중에 정할 수 있으며 심볼로 최적화가 가능) Multi-level abstractions : Relax는 high-level neural network layer부터 low-level tensor operation까지 포함하는 cross-level abstraction을 지원 (역자주) [Relax] 신경망 레이어 단위(Dense, ReLU, Conv2D) -\u0026gt; [Relax Dataflow] 텐서 연산 단위 (matmul, add, relu) -\u0026gt; [TIR]루프/인덱싱 기반 연산 의 변환이 하나의 통합된 시스템 안에서 자유롭게 오갈 수 있음) Composable transformations : relax는 모델 컴포넌트에 선택적으로 적용가능한 transformation을 제공, partial lowering / partial specialization 같은 유연한 최적화 옵션을 포함 (역자주) 한 번에 전체 모델을 \u0026ldquo;한 가지 방식\u0026quot;으로 변환하거나, 특정 최적화를 적용하면 다른 최적화와 충돌하는 경우가 많으나 relax는 모델 전체가 아닌, 특정 함수/연산에만 적용 가능하다. 즉 모델을 하드웨어/용도에 맞춰 유연하게 최적화하고 커스터마이징 가능) Understand Relax Abstraction Relax는 ML모델에 대해 end-to-end optimize를 돕기 위한 graph abstraction. Relax는 ML모델의 structure와 data flow를 묘사한다.(모델 파트간의 dependency와 relationship 및 HW에서 실행되는 방법)\nEnd to End Model Execution 이제부터 linear-\u0026gt;relu-\u0026gt;linear 모델을 활용하여 Relax를 설명한다. High-Level Operations Representation 위 모델을 Numpy와 Relax 모델로 하기와 같이 표현할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # numpy def numpy_mlp(data, w0, b0, w1, b1): lv0 = data @ w0 + b0 lv1 = np.maximum(lv0, 0) lv2 = lv1 @ w1 + b1 return lv2 # Relax from tvm.script import relax as R @R.function def relax_mlp( data: R.Tensor((\u0026#34;n\u0026#34;, 784), dtype=\u0026#34;float32\u0026#34;), w0: R.Tensor((784, 128), dtype=\u0026#34;float32\u0026#34;), b0: R.Tensor((128,), dtype=\u0026#34;float32\u0026#34;), w1: R.Tensor((128, 10), dtype=\u0026#34;float32\u0026#34;), b1: R.Tensor((10,), dtype=\u0026#34;float32\u0026#34;), ) -\u0026gt; R.Tensor((\u0026#34;n\u0026#34;, 10), dtype=\u0026#34;float32\u0026#34;): with R.dataflow(): lv0 = R.matmul(data, w0) + b0 lv1 = R.nn.relu(lv0) lv2 = R.matmul(lv1, w1) + b1 R.output(lv2) return lv2 Low-Level Integration 머신러닝 컴파일러의 관점에서 array computation의 세부 사항을 살펴보기 위해 Numpy 코드를 low-level로 풀어보자(배열 함수 대신 루프 사용, numpy.empty를 통해 배열을 명시적으로 할당)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def lnumpy_linear(X: np.ndarray, W: np.ndarray, B: np.ndarray, Z: np.ndarray): n, m, K = X.shape[0], W.shape[1], X.shape[1] Y = np.empty((n, m), dtype=\u0026#34;float32\u0026#34;) for i in range(n): for j in range(m): for k in range(K): if k == 0: Y[i, j] = 0 Y[i, j] = Y[i, j] + X[i, k] * W[k, j] for i in range(n): for j in range(m): Z[i, j] = Y[i, j] + B[j] def lnumpy_relu0(X: np.ndarray, Y: np.ndarray): n, m = X.shape for i in range(n): for j in range(m): Y[i, j] = np.maximum(X[i, j], 0) def lnumpy_mlp(data, w0, b0, w1, b1): n = data.shape[0] lv0 = np.empty((n, 128), dtype=\u0026#34;float32\u0026#34;) lnumpy_matmul(data, w0, b0, lv0) lv1 = np.empty((n, 128), dtype=\u0026#34;float32\u0026#34;) lnumpy_relu(lv0, lv1) out = np.empty((n, 10), dtype=\u0026#34;float32\u0026#34;) lnumpy_matmul(lv1, w1, b1, out) return out 위의 코드를 활용해 Relax로 표현할 수 있다(TVMScript 구현)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 @I.ir_module class Module: @T.prim_func(private=True) def linear(x: T.handle, w: T.handle, b: T.handle, z: T.handle): M, N, K = T.int64(), T.int64(), T.int64() X = T.match_buffer(x, (M, K), \u0026#34;float32\u0026#34;) W = T.match_buffer(w, (K, N), \u0026#34;float32\u0026#34;) B = T.match_buffer(b, (N,), \u0026#34;float32\u0026#34;) Z = T.match_buffer(z, (M, N), \u0026#34;float32\u0026#34;) Y = T.alloc_buffer((M, N), \u0026#34;float32\u0026#34;) for i, j, k in T.grid(M, N, K): with T.block(\u0026#34;Y\u0026#34;): v_i, v_j, v_k = T.axis.remap(\u0026#34;SSR\u0026#34;, [i, j, k]) with T.init(): Y[v_i, v_j] = T.float32(0.0) Y[v_i, v_j] = Y[v_i, v_j] + X[v_i, v_k] * W[v_k, v_j] for i, j in T.grid(M, N): with T.block(\u0026#34;Z\u0026#34;): v_i, v_j = T.axis.remap(\u0026#34;SS\u0026#34;, [i, j]) Z[v_i, v_j] = Y[v_i, v_j] + B[v_j] @T.prim_func(private=True) def relu(x: T.handle, y: T.handle): M, N = T.int64(), T.int64() X = T.match_buffer(x, (M, N), \u0026#34;float32\u0026#34;) Y = T.match_buffer(y, (M, N), \u0026#34;float32\u0026#34;) for i, j in T.grid(M, N): with T.block(\u0026#34;Y\u0026#34;): v_i, v_j = T.axis.remap(\u0026#34;SS\u0026#34;, [i, j]) Y[v_i, v_j] = T.max(X[v_i, v_j], T.float32(0.0)) @R.function def main( x: R.Tensor((\u0026#34;n\u0026#34;, 784), dtype=\u0026#34;float32\u0026#34;), w0: R.Tensor((784, 256), dtype=\u0026#34;float32\u0026#34;), b0: R.Tensor((256,), dtype=\u0026#34;float32\u0026#34;), w1: R.Tensor((256, 10), dtype=\u0026#34;float32\u0026#34;), b1: R.Tensor((10,), dtype=\u0026#34;float32\u0026#34;) ) -\u0026gt; R.Tensor((\u0026#34;n\u0026#34;, 10), dtype=\u0026#34;float32\u0026#34;): cls = Module n = T.int64() with R.dataflow(): lv = R.call_tir(cls.linear, (x, w0, b0), out_sinfo=R.Tensor((n, 256), dtype=\u0026#34;float32\u0026#34;)) lv1 = R.call_tir(cls.relu, (lv0,), out_sinfo=R.Tensor((n, 256), dtype=\u0026#34;float32\u0026#34;)) lv2 = R.call_tir(cls.linear, (lv1, w1, b1), out_sinfo=R.Tensor((b, 10), dtype=\u0026#34;float32\u0026#34;)) R.output(lv2) return lv2 위으 코드는 primitive tensor functions (T.prim_func) 과 R.function (relax function)을 포함(Relax 함수는 high-level neural network execution를 나타내기 위한 새로운 타입)\nRelax Module이 symbolic shape를 지원하는 것이 중요 (main 함수의 n, linear 함수의 M,N,K), 이것은 tensor operators와 function calls 간의 dynamic shape 추적을 위한 key feature임\nnumpy 코드와 TVMScript를 1:1로 비교해보면 세부적인 사항을 알 수 있다.\nKey Elements of Relax Structure Info Structure info는 relax expression의 type을 표현하기 위한 새로운 컨셉 ( TensorStructInfo, TupleStructInfo 등)\n위의 예제어서는 inputs, outputs, 중간 결과의 shape와 dtype을 표현하기 위해 TensorStructInfo (R.Tensor)를 사용 R.call_tir R.call_tir 함수는 primitive tensor 함수 호출을 위한 새로운 추상화, cross-level abstraction을 위한 key feature.\n1 2 3 4 5 6 #Relax lv = R.call_tir(cls.linear, (x, w0, b0), out_sinfo=R.Tensor((n, 256), dtype=\u0026#34;float32\u0026#34;)) #Numpy lv0 = np.empty((n, 256), dtype=\u0026#34;float32\u0026#34;) lnumpy_linear(x, w0, b0, lv0) 위의 relax코드와 이에 대응하는 numpy 코드를 비교해보자. call_tir은 destination passing을 사용한다.\ninput / output은 low-level primitive function 외부에 명시적으로 할당(저수준 라이브러리 설계에서 일반적으로 사용되는 방법으로 고수준 프레임워크가 메모리 할당 결정을 처리할 수 있음) 모든 텐서 연산을 이 스타일로 표현할 수 있는 것은 아니나 (예를 들어 입력에 따라 출력 형태가 달라지는 연산) 일반적으로 가능하면 저수준 함수를 이 스타일로 작성하는 것이 일반적 Dataflow Block relax function의 중요한 다른 element는 R.dataflow()\n1 2 3 4 5 with R.dataflow(): lv = R.call_tir(cls.linear, (x, w0, b0), out_sinfo=R.Tensor((n, 256), dtype=\u0026#34;float32\u0026#34;)) lv1 = R.call_tir(cls.relu, (lv0,), out_sinfo=R.Tensor((n, 256), dtype=\u0026#34;float32\u0026#34;)) lv2 = R.call_tir(cls.linear, (lv1, w1, b1), out_sinfo=R.Tensor((b, 10), dtype=\u0026#34;float32\u0026#34;)) R.output(lv2) Relax의 dataflow를 설명하기 전 pure와 side-effect의 개념에 대해 알아야 한다\npure 함수 : 입력만을 읽고, 출력을 만들어내는 함수 (입력 및 외부 메모리 영역을 변경하지 않음) Side-effect : 함수가 단순히 결과를 반환하는 것 외에, 프로그램의 다른 부분(메모리, 전역 변수 등)에 영향을 미치는 것 즉 pure(side-effect free) 하다는 것은 입력을 읽어 출력을 내보낼때 입력이나 다른 외부 메모리를 변경하지 않는다.(inplace operations(A += 1)은 side-effet가 발생) dataflow block은 side-effect free 함수만 허용함, side-effect가 있는 함수는 dataflow block에서 처리해야함\n(역자주) Relax는 모델의 순수 계산(graph) 과 운영/제어(control flow) 를 명확히 분리하려고 설계되었다. 그래서 R.dataflow() 내부는 최적화에 최적화된 구간, 외부는 학습/관리/제어 코드가 들어가는 구간으로 구분하는 것이 자연스러운 패턴 Dataflow Block을 자동으로 나누지 않고 수동으로 표시해야 하는 이유는\nauto inference 는 부정확 할수 있음 : packed function 호출(cuBLAS, cuDNN 등 외부 라이브러리 호출) 같은 경우 컴파일러 입장에서 확실하게 pure or not을 판단하기 어렵다. 많은 최적화는 Dataflow Block 안에서만 가능 : Fusion optimization 등은 Dataflow Block안에서만 가능(pure function 들만 모여 있기 때문에 연산 순서를 바꾸거나 합치는(fusion) 것이 안전합니다.) (역자 주) 컴파일러가 잘못 Dataflow Block 경계를 잡으면 최적화가 잘못되거나 성능에 영항을 줄 수 있다.) Relax Creation Relax functions을 정의하는 다양한 방법에 대해 다룬다.\nCreate Relax programs using TVMScript TVMScript는 TVM IR을 표현하기 위한 domain-specific language이다.\npython 형태의 언어이며 TensorIR and Relax function 둘다 포함한다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from tvm import relax, topi from tvm.script import ir as I from tvm.script import relax as R from tvm.script import tir as T @I.ir_module class RelaxModule: @R.function def forward( data: R.Tensor((\u0026#34;n\u0026#34;, 784), dtype=\u0026#34;float32\u0026#34;), w0: R.Tensor((128, 784), dtype=\u0026#34;float32\u0026#34;), b0: R.Tensor((128,), dtype=\u0026#34;float32\u0026#34;), w1: R.Tensor((10, 128), dtype=\u0026#34;float32\u0026#34;), b1: R.Tensor((10,), dtype=\u0026#34;float32\u0026#34;), ) -\u0026gt; R.Tensor((\u0026#34;n\u0026#34;, 10), dtype=\u0026#34;float32\u0026#34;): with R.dataflow(): lv0 = R.matmul(data, R.permute_dims(w0)) + b0 lv1 = R.nn.relu(lv0) lv2 = R.matmul(lv1, R.permute_dims(w1)) + b1 R.output(lv2) return lv2 RelaxModule.show() #출력확인 Relax는 graph-level IR 뿐만 아니라 cross-level representation과 transformation도 지원한다. 구체적으로 말하자면 Relax 함수에서 TensorIR 함수를 직접 호출할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 @I.ir_module class RelaxModuleWithTIR: @T.prim_func def relu(x: T.handle, y: T.handle): n, m = T.int64(), T.int64() X = T.match_buffer(x, (n, m), \u0026#34;float32\u0026#34;) Y = T.match_buffer(y, (n, m), \u0026#34;float32\u0026#34;) for i, j in T.grid(n, m): with T.block(\u0026#34;relu\u0026#34;): vi, vj = T.axis.remap(\u0026#34;SS\u0026#34;, [i, j]) Y[vi, vj] = T.max(X[vi, vj], T.float32(0)) @R.function def forward( data: R.Tensor((\u0026#34;n\u0026#34;, 784), dtype=\u0026#34;float32\u0026#34;), w0: R.Tensor((128, 784), dtype=\u0026#34;float32\u0026#34;), b0: R.Tensor((128,), dtype=\u0026#34;float32\u0026#34;), w1: R.Tensor((10, 128), dtype=\u0026#34;float32\u0026#34;), b1: R.Tensor((10,), dtype=\u0026#34;float32\u0026#34;), ) -\u0026gt; R.Tensor((\u0026#34;n\u0026#34;, 10), dtype=\u0026#34;float32\u0026#34;): n = T.int64() cls = RelaxModuleWithTIR with R.dataflow(): lv0 = R.matmul(data, R.permute_dims(w0)) + b0 lv1 = R.call_tir(cls.relu, lv0, R.Tensor((n, 128), dtype=\u0026#34;float32\u0026#34;)) lv2 = R.matmul(lv1, R.permute_dims(w1)) + b1 R.output(lv2) return lv2 RelaxModuleWithTIR.show() #출력확인 show()로 출력을 확인해보면 작성한 TVMScript 코드와 출력이 다름을 볼수 있는데 이는 출력 시 syntax sugar 등이 표준 포맷으로 출력되기 때문이다. 예를 들어 작성 시 한라인에 여러 operation을 결합하여 작성할 수 있으나 출력시에는 한라인에 하나의 오퍼레이션이 결합되도록 출력된다.\n1 2 3 4 5 6 7 # writen lv0 = R.matmul(data, R.permute_dims(w0)) + b0 # printed lv: R.Tensor((784, 128), dtype=\u0026#34;float32\u0026#34;) = R.permute_dims(w0, axes=None) lv1: R.Tensor((n, 128), dtype=\u0026#34;float32\u0026#34;) = R.matmul(data, lv, out_dtype=\u0026#34;void\u0026#34;) lv0: R.Tensor((n, 128), dtype=\u0026#34;float32\u0026#34;) = R.add(lv1, b0) Create Relax programs using NNModule API TVM은 Relax 프로그래밍을 위한 PyTorch-like API인 Relax NNModule API 지원한다. NNModule을 정의한 후 이를 export_tvm을 활용하여 TVM IRModule로 변환할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from tvm.relax.frontend import nn class NNModule(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(784, 128) self.relu1 = nn.ReLU() self.fc2 = nn.Linear(128, 10) def forward(self, x): x = self.fc1(x) x = self.relu1(x) x = self.fc2(x) return x mod, params = NNModule().export_tvm({\u0026#34;forward\u0026#34;: {\u0026#34;x\u0026#34;: nn.spec.Tensor((\u0026#34;n\u0026#34;, 784), \u0026#34;float32\u0026#34;)}}) mod.show() 또한 NNModule에 customized function call을 삽입할 수 있다.\nTensor Expression(TE), TensorIR functions, other TVM packed functions 등 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 @T.prim_func def tir_linear(x: T.handle, w: T.handle, b: T.handle, z: T.handle): M, N, K = T.int64(), T.int64(), T.int64() X = T.match_buffer(x, (M, K), \u0026#34;float32\u0026#34;) W = T.match_buffer(w, (N, K), \u0026#34;float32\u0026#34;) B = T.match_buffer(b, (N,), \u0026#34;float32\u0026#34;) Z = T.match_buffer(z, (M, N), \u0026#34;float32\u0026#34;) for i, j, k in T.grid(M, N, K): with T.block(\u0026#34;linear\u0026#34;): vi, vj, vk = T.axis.remap(\u0026#34;SSR\u0026#34;, [i, j, k]) with T.init(): Z[vi, vj] = 0 Z[vi, vj] = Z[vi, vj] + X[vi, vk] * W[vj, vk] for i, j in T.grid(M, N): with T.block(\u0026#34;add\u0026#34;): vi, vj = T.axis.remap(\u0026#34;SS\u0026#34;, [i, j]) Z[vi, vj] = Z[vi, vj] + B[vj] class NNModuleWithTIR(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): n = x.shape[0] # We can call external functions using nn.extern x = nn.extern( \u0026#34;env.linear\u0026#34;, [x, self.fc1.weight, self.fc1.bias], out=nn.Tensor.placeholder((n, 128), \u0026#34;float32\u0026#34;), ) # We can also call TensorIR via Tensor Expression API in TOPI x = nn.tensor_expr_op(topi.nn.relu, \u0026#34;relu\u0026#34;, [x]) # We can also call other TVM packed functions x = nn.tensor_ir_op( tir_linear, \u0026#34;tir_linear\u0026#34;, [x, self.fc2.weight, self.fc2.bias], out=nn.Tensor.placeholder((n, 10), \u0026#34;float32\u0026#34;), ) return x mod, params = NNModuleWithTIR().export_tvm( {\u0026#34;forward\u0026#34;: {\u0026#34;x\u0026#34;: nn.spec.Tensor((\u0026#34;n\u0026#34;, 784), \u0026#34;float32\u0026#34;)}} ) mod.show() Create Relax programs using Block Builder API TVM은 Relax 프로그래밍을 위한 Block Builder API 를 제공한다. 이는 IR builder API로 좀 더 low-level이며 customized pass를 기술하기 위한 TVM 내부 로직에 널리 쓰인다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 bb = relax.BlockBuilder() n = T.int64() x = relax.Var(\u0026#34;x\u0026#34;, R.Tensor((n, 784), \u0026#34;float32\u0026#34;)) fc1_weight = relax.Var(\u0026#34;fc1_weight\u0026#34;, R.Tensor((128, 784), \u0026#34;float32\u0026#34;)) fc1_bias = relax.Var(\u0026#34;fc1_bias\u0026#34;, R.Tensor((128,), \u0026#34;float32\u0026#34;)) fc2_weight = relax.Var(\u0026#34;fc2_weight\u0026#34;, R.Tensor((10, 128), \u0026#34;float32\u0026#34;)) fc2_bias = relax.Var(\u0026#34;fc2_bias\u0026#34;, R.Tensor((10,), \u0026#34;float32\u0026#34;)) with bb.function(\u0026#34;forward\u0026#34;, [x, fc1_weight, fc1_bias, fc2_weight, fc2_bias]): with bb.dataflow(): lv0 = bb.emit(relax.op.matmul(x, relax.op.permute_dims(fc1_weight)) + fc1_bias) lv1 = bb.emit(relax.op.nn.relu(lv0)) gv = bb.emit(relax.op.matmul(lv1, relax.op.permute_dims(fc2_weight)) + fc2_bias) bb.emit_output(gv) bb.emit_func_output(gv) mod = bb.get() mod.show() Block Builder API는 유저 친화적이지 않지만 가장 낮은 수준의 API이며 IR definition과 밀접하게 작동한다. TVM은 ML 모델을 정의하거나 transform하고자 하는 사용자들은 TVMScript나 NNModule API를 사용하는 것을 추천한다. 그러나 복잡한 transformation을 원한다면 Block Builder API가 좀 더 유연한 선택이다.\nTransformation Transformation은 Hardware Backend와 최적화 및 통합하기 위한 컴파일 flow의 핵심 요소이다. 2항의 NNModule API 예제인 class NNModule(nn.Module)을 이용하여 예제를 진행한다.\nApply transformations Pass는 Transformation을 Relax 프로그램에 적용하기 위한 주요 방법이다. 첫번째 단계로 built-in pass인 LegalizeOps를 적용하여 high-level operator들을 low-level operator로 lowering 할 수 있다. (본문을 통해 pass가 적용된 결과를 확인하면 add, matmul등이 TensorIR로 변환되고 R.call_tir을 통해 호출되는 형태로 변환된 것을 확인할 수 있다.)\n1 2 mod = tvm.relax.transform.LegalizeOps()(origin_mod) mod.show() 결과로 부터 high-level operator(aka relax.op)가 이에 대응되는 low-level operator(aka relax.call_tir)로 교체된 것을 볼 수 있다. fusion optimization(연산자 융합)은 Pass의 집합을 적용하여 수행할 수 있다.\n1 2 3 4 5 6 7 8 mod = tvm.ir.transform.Sequential( [ tvm.relax.transform.AnnotateTIROpPattern(), tvm.relax.transform.FuseOps(), tvm.relax.transform.FuseTIR(), ] )(mod) mod.show() 본문의 결과로 부터 matmul, add, relu 연산자가 하나의 커널(aka call_tir)로 합쳐진 것을 볼 수 있다. 지원하는 Built-in pass들은 relax.transform을 참조하면 확인할 수 있다.\nCustom Passes Custom pass를 정의하는 방법을 확인하기 위해 relu를 gelu로 변환하는 예제를 수행해보자\n(역자주) gelu : transform 등 최신 모델의 활성함수로 GELU 함수는 표준 가우신안 누적 분포 함수 인 xΦ(x)로 정의 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # ReluRewriter클래스는 PyExprMutator클래스를 상속받아 visit_call_을 오버라이딩 from tvm.relax.expr_functor import PyExprMutator, mutator @mutator class ReluRewriter(PyExprMutator): def __init__(self, mod): super().__init__(mod) def visit_call_(self, call: relax.Call) -\u0026gt; relax.Expr: # visit the relax.Call expr, and only handle the case when op is relax.nn.relu if call.op.name == \u0026#34;relax.nn.relu\u0026#34;: return relax.op.nn.gelu(call.args[0]) return super().visit_call_(call) 위의 mutator를 적용한 pass를 이용해 trasnformation\n1 2 3 4 5 6 7 8 9 10 11 12 13 @tvm.transform.module_pass(opt_level=0, name=\u0026#34;ReluToGelu\u0026#34;) class ReluToGelu: # pylint: disable=too-few-public-methods def transform_module(self, mod: IRModule, _ctx: tvm.transform.PassContext) -\u0026gt; IRModule: \u0026#34;\u0026#34;\u0026#34;IRModule-level transformation\u0026#34;\u0026#34;\u0026#34; rewriter = ReluRewriter(mod) for g_var, func in mod.functions_items(): if isinstance(func, relax.Function): func = rewriter.visit_expr(func) rewriter.builder_.update_func(g_var, func) return rewriter.builder_.get() mod = ReluToGelu()(origin_mod) mod.show() 결과를 확인해보면 relax.nn.relu operator가 relax.nn.gelu 로 변경된 겻을 확인할 수 있다. 자세한 내용은 relax.expr_functor.PyExprMutator 참고\n","date":"2025-04-25T00:00:00Z","permalink":"https://muonkmu.github.io/p/tvm_0.20-03-relax/","title":"[TVM_0.20] 03 Relax"},{"content":"Tensor Program Abstraction TensorIR에 대해 알아보기 전에 primitive tensor function에 대해 먼저 알아보자 primitive tensor function은 computational operation의 single “unit”에 대응하는 function이다.\n예를 들어 convolution 또는 convolution+relu(fused) operation primitive tensor function의 추상화는 tensor computation을 위한 multi-dimensional buffers, loop nests을 포함한다.\n1 2 3 4 5 6 7 8 9 10 11 12 from tvm.script import tir as T @T.prim_func def main( A: T.Buffer((128,), \u0026#34;float32\u0026#34;), B: T.Buffer((128,), \u0026#34;float32\u0026#34;), C: T.Buffer((128,), \u0026#34;float32\u0026#34;), ) -\u0026gt; None: for i in range(128): with T.block(\u0026#34;C\u0026#34;): vi = T.axis.spatial(128, i) C[vi] = A[vi] + B[vi] Key Elements of Tensor Programs 위의 코드는 primitive tensor function의 예제로 다음의 Key Elements를 가진다.\nmulti-dimensional buffer (input 2개, output 1개) 계산을 용이하게 하는 단일 loop nest i 단일 compute statement (element-wise sum) Extra Structure in TensorIR (TVM의 TensorIR에서 annotation(추가적인 구조)가 필요한지를 설명하는 내용)\n어떤 프로그램들은 loop’s sequence에 의존하기 때문에 마음대로 루프를 재배열하거나 병렬화할 수는 없지만 우리가 주로 다루는 대부분의 primitive tensor function은 루프 반복 간의 독립성이라는 좋은 성질을 가지고 있다. 이전 예제에서는 block / iteration annotations을 들 수 있다.\nblock annotation : with T.block(\u0026quot;C\u0026quot;)은 블록이 스케줄링을 위해 지정된 기본 계산 단위임을 의미, block()은 single computation statement, multiple computation statements, opaque intrinsics(Tensor Core instructions) 을 포함할 수 있다. iteration annotation : T.axis.spatial은 vi가 에 매핑되고 모든 iterations이 독립적임을 의미한다. 블록과 축의 annotation는 프로그램 실행 자체에는 필수는 아니지만, 프로그램을 최적화하거나 변환할 때 매우 중요하다. (vi와 관련된 루프를 parallelize, reorder 할 수 있음을 나타낸다.) Understand TensorIR Abstraction tensor program abstraction의 주요 목적은 hardware acceleration options 및 loop를 묘사하는 것(threading, 특수 hardware instructions 적, memory access)\n예를 들어, 128*128 Matrix_Multiplication + Relu 를 python numpy code와 TensorIR로 나타내면 아래와 같다 (TVMScript 이용). 다음 각 sub-chapter에서 아래 코드에 대한 element에 대하여 설명한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def lnumpy_mm_relu(A: np.ndarray, B: np.ndarray, C: np.ndarray): Y = np.empty((128, 128), dtype=\u0026#34;float32\u0026#34;) for i in range(128): for j in range(128): for k in range(128): if k == 0: Y[i, j] = 0 Y[i, j] = Y[i, j] + A[i, k] * B[k, j] for i in range(128): for j in range(128): C[i, j] = max(Y[i, j], 0) @tvm.script.ir_module class MyModule: @T.prim_func def mm_relu(A: T.Buffer((128, 128), \u0026#34;float32\u0026#34;), B: T.Buffer((128, 128), \u0026#34;float32\u0026#34;), C: T.Buffer((128, 128), \u0026#34;float32\u0026#34;)): Y = T.alloc_buffer((128, 128), dtype=\u0026#34;float32\u0026#34;) for i, j, k in T.grid(128, 128, 128): with T.block(\u0026#34;Y\u0026#34;): vi = T.axis.spatial(128, i) vj = T.axis.spatial(128, j) vk = T.axis.reduce(128, k) with T.init(): Y[vi, vj] = T.float32(0) Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj] for i, j in T.grid(128, 128): with T.block(\u0026#34;C\u0026#34;): vi = T.axis.spatial(128, i) vj = T.axis.spatial(128, j) C[vi, vj] = T.max(Y[vi, vj], T.float32(0)) Function Parameters and Buffers TensorIR에서 A,B,C는 float32, (128,128) 형태의 T.Buffer를 취하며 이러한 추가 정보는 MLC(Machine Learning Compile)가 shape 와 data type에 특화된 코드를 생성하는 데 도움이 됩니다.\n1 2 3 4 5 6 7 8 # TensorIR def mm_relu(A: T.Buffer((128, 128), \u0026#34;float32\u0026#34;), B: T.Buffer((128, 128), \u0026#34;float32\u0026#34;), C: T.Buffer((128, 128), \u0026#34;float32\u0026#34;)): ... # NumPy def lnumpy_mm_relu(A: np.ndarray, B: np.ndarray, C: np.ndarray): ... 상기 내용과 비슷하게 중간 할당에서도 버퍼를 사용함\n1 2 3 4 # TensorIR Y = T.alloc_buffer((128, 128), dtype=\u0026#34;float32\u0026#34;) # NumPy Y = np.empty((128, 128), dtype=\u0026#34;float32\u0026#34;) Loop Iterations Loop Iterations은 python 구문과 바로 대응 되기에 for i in range(num)를 바로 사용하거나, T.grid를 사용하여 코드를 축약할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 # TensorIR with `T.grid` for i, j, k in T.grid(128, 128, 128): ... # TensorIR with `range` for i in range(128): for j in range(128): for k in range(128): ... # NumPy for i in range(128): for j in range(128): for k in range(128): ... Computational Block computational statements에서 중요 차이점은 T.block이라는 추가적인 요소를 포함한다. T.block은 TensorIR내부에서 기본적인 computation unit을 나타며 numpy코드보다 더 많은 정보(block axes 세트인 (vi, vj, vk)와 이를 둘러 싼 computations)를 포함한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 # TensorIR with T.block(\u0026#34;Y\u0026#34;): vi = T.axis.spatial(128, i) vj = T.axis.spatial(128, j) vk = T.axis.reduce(128, k) with T.init(): Y[vi, vj] = T.float32(0) Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj] # NumPy vi, vj, vk = i, j, k if vk == 0: Y[vi, vj] = 0 Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj] T.axis 코드(vi = T.axis.spatial(128, i) 3줄) block axes의 key properties를 나타낸다. 문법은 다음과 같다.\n[block_axis] = T.axis.[axis_type]([axis_range], [mapped_value]) 이 코드는 하기의 세부 정보를 전달한다. vi, vj, vk의 i, j, k와의 binding vi, vj, vk의 range 선언( = 128) iterators 의 속성 : spatial(독립적인 계산), reduce(축소/누적 계산) 또한 위의 코드는 T.axis.remap을 이용하여 하기와 같이 표현 가능하다. vi, vj, vk = T.axis.remap(\u0026ldquo;SSR\u0026rdquo;, [i, j, k]) 역자 주 spatial axis : 이 블록이 출력으로 쓰는 인덱스로 병렬화 가능 reduce axis : 이 블록이 누적에 사용하는 인덱스로 병렬화가 가능할 수도 있다.(race condition 조심) Reduction 연산 : 다차원 배열(tensor)에서 하나의 축을 따라 값을 누적(sum(x, axis=1), matmul(A, B), mean(x), softmax(x), conv2d) 하는 연산 Block Axis Properties T.axis는 해당 axis가 현재 수행 중인 computation과 어떤 관계가 있는지를 나타낸다. 이 블록은 세 개의 축(vi, vj, vk)을 가지고 있으며 A[vi, vk], B[vk, vj]를 읽어 Y[vi, vj]를 업데이트한다. vi, vj의 값이 고정되면 (Y[vi, vj]라는 한 점을 정하면) 이 점의 계산은 오직 vk를 반복하면서 이루어지며 다른 위치인 Y[vi’, vj’]와는 독립적이다.(안전하게 병렬 처리도 가능함) 그래서 vi, vj는 spatial axis (공간 좌표) vk는 reduce axis (누적을 위한 축) 라고 부른다. (챗GPT의 해석 : TVM은 블록 내에서 어떤 축이 \u0026ldquo;누적 대상\u0026quot;이고, 어떤 축이 \u0026ldquo;출력 대상\u0026quot;인지를 명시적으로 구분하며 구분을 통해 병렬화 가능성 판단, 메모리 스케줄링, 코드 자동 최적화 등을 가능하게 함.)\nWhy Extra Information in Block 블록 내부의 축 정보(축의 범위와 속성 등)가 포함되기 때문에, 블록은 외부 루프와 독립적인 \u0026ldquo;자기 완결(self-contained)\u0026rdquo; 구조가 된다 즉 vi = T.axis.spatial(128, i) 처럼 축의 역할과 범위를 명시적으로 선언함으로써 루프에 의존하지 않고, 자신만으로도 무슨 계산을 어디서 수행해야 하는지 알 수 있는 완전한 구조가 되는 것. 또한 블록의 축 정보는 외부 루프가 올바른지 검사하는 기준도 제공해준다.\nTensorIR Creation 이번 챕터에서는 TensorIR function을 코딩하는 방법을 기술한다.(이 챕터에서의 기술내용은 Relax모델을 컴파일하는 유저에게 필 수적인 것은 아니다.)\nCreate TensorIR using TVMScript 앞에서 사용한 mm_relu TVM Script를 다시 설명함(본 포스트에서는 스킵함, 필요하면 TVM DOC를 볼 것)\nTensorIR Function with Dynamic Shapes TVMScript가 Python 인터프리터에 의해 실행되지 않지만 Python과의 상호 작용은 제한적으로 가능하다. Python 변수를 사용하여 TensorIR의 shape과 Data type 변수 지정 가능하다. 또한 이를 통해 runtime dynamic shape inference가 가능하다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @I.ir_module class DynamicShapeModule: @T.prim_func def mm_relu(a: T.handle, b: T.handle, c: T.handle): # Dynamic shape definition M, N, K = T.int32(), T.int32(), T.int32() # Bind the input buffers with the dynamic shapes A = T.match_buffer(a, [M, K], dtype) B = T.match_buffer(b, [K, N], dtype) C = T.match_buffer(c, [M, N], dtype) Y = T.alloc_buffer((M, N), dtype) for i, j, k in T.grid(M, N, K): with T.block(\u0026#34;Y\u0026#34;): vi, vj, vk = T.axis.remap(\u0026#34;SSR\u0026#34;, [i, j, k]) with T.init(): Y[vi, vj] = T.cast(T.float32(0), dtype) Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj] for i, j in T.grid(M, N): with T.block(\u0026#34;C\u0026#34;): vi, vj = T.axis.remap(\u0026#34;SS\u0026#34;, [i, j]) C[vi, vj] = T.max(Y[vi, vj], T.cast(T.float32(0), dtype) def evaluate_dynamic_shape(lib: tvm.runtime.Module, m: int, n: int, k: int): A = tvm.nd.array(np.random.uniform(size=(m, k)).astype(\u0026#34;float32\u0026#34;)) B = tvm.nd.array(np.random.uniform(size=(k, n)).astype(\u0026#34;float32\u0026#34;)) C = tvm.nd.array(np.zeros((m, n), dtype=\u0026#34;float32\u0026#34;)) lib(A, B, C) return C.numpy() # Compile lib only once dyn_shape_lib = tvm.compile(DynamicShapeModule, target=\u0026#34;llvm\u0026#34;) # Able to handle different shapes print(evaluate_dynamic_shape(dyn_shape_lib, m=4, n=4, k=4)) print(evaluate_dynamic_shape(dyn_shape_lib, m=64, n=64, k=128)) Create TensorIR using Tensor Expression TensorIR을 더 간결하게 표현할 수 있는 Tensor Expression 표현 방식이 있다. 레거시 인듯 해서 우선은 넘어가기로 하겠다.\nTransformation primitive tensor functions의 transformation은 컴파일 flow의 주요 과정이다. 이제까지 사용한 mm_relu를 평가하는 함수를 만들어 TensorIR transformation 과정을 진행한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import numpy as np a_np = np.random.uniform(size=(128, 128)).astype(\u0026#34;float32\u0026#34;) b_np = np.random.uniform(size=(128, 128)).astype(\u0026#34;float32\u0026#34;) c_np = a_np @ b_np a_nd = tvm.nd.array(a_np) b_nd = tvm.nd.array(b_np) c_nd = tvm.nd.array(np.zeros((128, 128), dtype=\u0026#34;float32\u0026#34;)) def evaluate(mod: tvm.IRModule): lib = tvm.tir.build(mod, target=\u0026#34;llvm\u0026#34;) # check correctness lib(a_nd, b_nd, c_nd) np.testing.assert_allclose(c_nd.numpy(), c_np, rtol=1e-5) # evaluate performance f_timer = lib.time_evaluator(\u0026#34;main\u0026#34;, tvm.cpu()) print(f_timer(a_nd, b_nd, c_nd)) evaluate(MyModule) Initialization Schedule Schedule helper class를 사용하여 transformation 프로세스를 초기화 할 수 있다.\n1 sch = tvm.tir.Schedule(MyModule) Loop Tiling Loop Split 다음 명령을 수행하여 block Y와 이와 관련된 루프의 reference를 얻는다.\n1 2 block_Y = sch.get_block(\u0026#34;Y\u0026#34;) i, j, k = sch.get_loops(block_Y) 그 다음 j를 두개의 loop로 분리(inner loop의 길이가 8, 즉 128회를 16*8회로 나눔)하는 transformation을 수행할 수 있다. (이 프로세스는 절차?적 이기 때문에 위의 내용을 두번 수행하면 j가 존재하지 않는다는 에러가 출력됨, 역자주, 아마 첫번째 수행에서 j가 j_0, j_1로 분리되므로 j는 사라진다는 뜻일 듯)\n1 j0, j1 = sch.split(j, factors=[None, 8]) sch.mod.show() 로 transformation의 결과를 확인 할 수 있다. 원문을 보면 j에 관련된 루프가 j_0, j_1로 분리된 결과를 확인할 수 있다.\nLoop Reorder 루프 j_0, j_1를 reordering 할 수 있다.\n1 2 3 sch.reorder(j0, k, j1) sch.mod.show() evaluate(sch.mod) 원문을 보면 j에 관련된 루프가 j_0, k, j_1로 reorder 된 것을 확인할 수 있다.\n역자주 TensorIR에서 Transformation은 고성능 코드 생성을 위한 스케줄 최적화 작업이다.\nLoop Tiling은 큰 Loop를 작은 Loop(블록) 단위로 나누어 중첩 루프 구조로 바꾸는 최적화이다. 큰 배열이 L1/L2 캐시에 모드 들어가지 않을 때 이를 나누어 타일 단위로 계산하면 작은 덩어리의 데이터만 반복 사용하게 되어 캐시 히트율이 올라간다. 또한 벡터화나 병렬화가 가능해지며 loop fusion / compute_at 등의 고급 스케줄링을 하기 전 준비 단계로 쓰인다. Loop Reordering도 메모리 접근의 지역성(Locality) 향상시켜 CPU캐시 히트율 향상시킨다. 또한 벡터화의 가능성을 증진 시킨다. Leverage Localities block C를 loop Y 내부로 재배치 하는 reverse_compute_at이라는 primitive를 사용할 수 있다.\n1 2 3 block_C = sch.get_block(\u0026#34;C\u0026#34;) sch.reverse_compute_at(block_C, j0) sch.mod.show() 본문에 T.block(\u0026quot;C\u0026quot;) 블록이 j_1 하위로 이동한 결과를 확인 할 수 있다.\n역자주 compute_at, reverse_compute_at은 블록의 계산 위치를 다른 루프 또는 블록 내부로 이동시키는 스케줄링 compute_at : 앞선 블록 안에서 같이 계산, reverse_compute_at : 나중 블록을 앞 루프 안으로 옮김 연산을 필요한 시점에만 수행하여 메모리 접근 줄이고 연산 지역성(locality) 향상 버퍼 공유 또는 중간 결과를 바로 사용하는 구조에서 타일링 이후, 해당 블록을 타일 단위로 옮길 때 효과적 Rewrite Reduction 이제까지의 reduction initialization / update 는 single block body에서 수행되었으며 이러한 형태는 outer loops i, j의 initialization / update가 동기화를 필요. loop transformation 후 Y elements의 초기화를 reduction update로 부터 분리할 수 있다.\n1 2 3 sch.decompose_reduction(block_Y, k) sch.mod.show() evaluate(sch.mod) 역자 주 decompose_reduction은 TVM TensorIR에서 reduction 연산(누적 연산)을 초기화 단계와 갱신 단계로 분리해 주는 스케줄링 기법 T.init()이 loop body에 있으면 병렬화/재배열 어려워 진다. 또한 루프 제어를 단순화 시킬 수 있으며 메모리 접근이 최적화 된다. reorder, compute_at, unroll 등을 적용하기도 쉬워진다.\nTrace the Transformation TensorIR schedule은 절차적 언어로 단계별로 실행된다. TVM은 schedule 또는 schedule 이력을 추적할 수 있다.\nsch.trace.show() : schedule 이력 출력 sch.mod.show() : schedule 출력 ","date":"2025-04-17T00:00:00Z","permalink":"https://muonkmu.github.io/p/tvm_0.20-02-tensorir/","title":"[TVM_0.20] 02 TensorIR"},{"content":"TVM이 0.18버전부터 Transformer등으로 대표되는 LLM 등에 대응하기 위해 TVM Unity로 진화하였다.(가장 큰 변화는 relay 대신 relax를 사용)\nhttps://discuss.tvm.apache.org/t/relax-co-designing-high-level-abstraction-towards-tvm-unity/12496 TVM을 다룰 수 있어야 신규 ACCELERATOR를 개발하더라도 이에 맞는 컴파일러를 구축할 수 있기에 현재 TVM에 대한 이론을 정리하고자 한다.\nOverall Flow 전체적인 Compile flow를 다룬다.\nModel Creation compile/optimize 될 모델을 생성, IRModule의 형태이며 NNModule, TVMScript을 사용하여 직접 코딩하거나 기존 pre-trained model(pytorch 같은)을 Relax frontend를 이용하여 import 한다. Transformation IRModule을 다른 IRModule 형태로 변환 (대표적으로 quantization), Target과 독립적이지만 타겟이 transformation pipeline의 설정에 영향을 끼치도록 할 수 있다. Target Translation IRModule을 Target에 실행 가능한 codegen 과정 Runtime Execution 컴파일된 모듈을 runtime.Module에 적재하여 실행시키는 것 Key data structures IRModule은 primary data structure이며 2가지 주요한 형태의 function을 제공\nrelax::Function high-level functional program representation. 주로 end-to-end model 또는 전체 모델의 sub-graph 에 대응, control-flow와 complex data structure를 지원하는 computational graph로 생각할 수 있다.\ntir::PrimFunc low-level program representation이며 loop-nest choice, multi-dimensional load/store, threading, vector/tensor instructions을 포함. 모델 레이어를 실행하는 operator program에 대응\nCompilation/Transformation과정에서 모든 Relax operator는 Target device에서 바로 실행 가능한 tir::PrimFunc 또는 TVM PackedFunc으로 lowering 된다. 반면 Relax operator의 Call은 R.call_tir / R.call_dps 같은 low-level function으로 lowering 된다.\nTransformations Key data structure 간 변환 작업을 Transformations이라 하며 하기 2가지 목표를 가진다.\noptimization : 동등하거나 최적화된 변환 lowering : 타겟에 가까운 lower-level representation로 변환 relax transformations Relax function에 적용되는 graph-level optimizations. constant folding / dead-code elimination / library dispatch 등\ntir transformations tir function에 적용되며 하기 2가지 타입의 변환이 있음\nTensorIR schedule TensorIR functions을 target 코드를 생성하기 위한 user-guided instructions과 control을 가지고 최적화 하는 방법을 정의 TVM unity는 TensorIR schedule을 자동으로 찾아주는 MetaSchedule을 제공 TensorIR 챕터에서 더 자세히 다룰 예정 (역자 주 : Tensor는 다차원 배열을 의미한다. TensorIR schedule이란 Tensor를 어떻게 분할하여 처리할 것인가를 정의하는 것이라 생각할 수 있다.) Lowering Passes TensorIR schedule 적용 후 TIR PrimFunc을 좀 더 Target에 적합한 TIR PrimFunc로 변환 예를 들어 multi-dimensional access를 one-dimensional pointer access로 flatten, 내장함수를 타겟에 맞도록 확장, runtime calling convention에 맞는 function decorate. (내장함수 : __enable_interrupt() 같은 컴파일러 제공함수?) TVM은 low-level optimization은 타겟 컴파일러에가 맞기기 때문에 거의 다루지 않는다. cross-level trasnsformation TVM은 end-to-end 모델을 최적화 하기 위한 단일 전략을 채용. 즉 IRModule이 relax와 tir function을 둘다 가지고 있으므로 이들에게 각각 다른 Transformation을 적용한다. 예를 들어 relax.LegalizeOps는 relax operator를 lowering 하고 TIR PrimFunc을 IRModule에 추가한다음, relax연산자를 TIR PrimFunc로 호출하는 것으로 대체한다. 다른 예로, fusion pipeline(relax.FuseOps,relax.FuseTIR 등)은 이는 여러 연속적인 텐서 연산을 하나로 융합함.\nTarget Translation IRModule을 Target 실행 바이너리로 변환, TVM은 Relax function (sub-graph)를 외부 코드 제너레이터를 이용하여 translation 하는 것을 지원한다.\nRuntime Execution TVM’s runtime의 주요 목적은 컴파일된 artifact의 로딩과 실행을 위한 간소한 API를 제공하는 것(C++, Rust, Go 등) tvm.runtime.Module은 컴파일 결과를 캡슐화하고 PackedFuncs을 얻기 위한 GetFunction method를 가짐 tvm.runtime.PackedFunc은 생성 코드를 위한(실행을 위한) type-erased function 좀 더 자세한 내용은 runtime chapter에서 다룸 Summary and Discussions 컴파일 flow에서 key data structure IRModule: relax.Function, tir.PrimFunc 포함 runtime.Module: runtime.PackedFunc 포함 컴파일에서 주요한 파트는 key data structure간 Transformation relax/transform, tir/transform는 determinstic rule-based transformations meta-schedule은 search-based transformations TVM은 key data structures와 transformations을 python, C++ API로 노출 시키므로 TVM을 numpy 사용하듯 사용할 수 있다. IRModule을 python API를 이용하여 직접 구축할 수 있다. custom transformations(e.g. customize quantization) 작성할 수 있다. TVM의 파이썬 API를 사용하여 직접 IR을 조작할 수 있다. ","date":"2025-04-15T00:00:00Z","permalink":"https://muonkmu.github.io/p/tvm_0.20-01-design-and-architecture/","title":"[TVM_0.20] 01 Design and Architecture"},{"content":"Result TVM에서 VTA를 레거시 코드로 인식하여 매뉴얼대로 실행하여도 정상적으로 진행이 되지 않고 수많은 에러에 봉착하게 된다. 이를 해결하기 위해\nCompiler 및 runtime 빌드 시 자신의 보드 설정을 추가 해야 한다. 필자의 경우 ZCU104 보드를 추가하고 이에 맞는 LLVM 버전 및 다른 설정들을 추가하였다. 필자는 RPC 서버를 활용하지 않고 TARGET(ZCU104)에 바이너리를 카피해두고 로컬환경에서 바로 카메라 입력을 받아 처리함 TVM에서 제공하는 RPC서버의 경우 FPGA Bit 파일이 PL영역 로드 명령이 수행되지 않는다.(Pynq 버전 차이 인듯) TVM에서 제공하는 RPC서버의 경우 런타임 수행명령에 의해 런타임이 재구성 되는데 이때 정상적으로 런타임이 재구성 되지 않아 오류가 난다. TVM에서 제공하는 예제의 경우 OpenCV명령이 예전 버전이라 현재의 Pynq와 호환이 되지 않는다. 2항을 위해 Target을 위한 SW를 제작하였다. FPGA Bit파일 / TVM Runtime / 모델 바이너리를 로딩 카메라 입력을 VTA에 전송하고 출력을 받아옴 VTA 출력에 NMS를 적용하고 Box와 class Tag를 그리기 상세한 과정은 다음에 기회가 되면 정리하겠다. 다만 TVM 컴파일러가 대대적으로 개편이 되고 FPGA 지원이 빠지면서 TVM을 조금 다시 공부할 필요가 있어 이를 다시 공부하고자 한다.\n","date":"2025-04-14T00:00:00Z","permalink":"https://muonkmu.github.io/p/tvmvta-project-result/","title":"[TVM\u0026VTA] Project result"},{"content":"VTA Concept Versatile Tensor Accelerator(VTA)는 Washington 대학에서 시작된 RISC 형태의 오픈소스 deep learning accelerator 이다. TVM 프로젝트의 하위 프로젝트로 TVM에 기반한 hardware design, drivers, a JIT runtime, and an optimizing compiler stack을 제공한다. https://tvm.apache.org/docs/topic/vta/index.html VTA Parameter VTA는 HLS기반으로 합성 시 사용할 tensor intrinsic, clock frequency, pipelining, data type width, on-chip buffer sizes 등의 파라메터를 제공한다. 3rdparty/vta-hw/config/vta_config.json 파일에 사용할 파라메터가 있으며 https://tvm.apache.org/docs/topic/vta/dev/config.html 에 파라메터에 대한 설명이 기술되어 있다 다음의 그림은 파라메터가 VTA core에 어떻게 적용되는지 나태낸 그림이며 필자가 ZCU104에서 사용 중인 파일 예시이다. 합성 시 vta_config.json을 바탕으로 vta_config.py이 vta_config.tcl 파일을 생성한다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;TARGET\u0026#34; : \u0026#34;zcu104\u0026#34;, \u0026#34;HW_FREQ\u0026#34; : 300, \u0026#34;HW_CLK_TARGET\u0026#34; : 3, \u0026#34;HW_VER\u0026#34; : \u0026#34;0.0.1\u0026#34;, \u0026#34;LOG_INP_WIDTH\u0026#34; : 3, \u0026#34;LOG_WGT_WIDTH\u0026#34; : 3, \u0026#34;LOG_ACC_WIDTH\u0026#34; : 5, \u0026#34;LOG_BATCH\u0026#34; : 0, \u0026#34;LOG_BLOCK\u0026#34; : 4, \u0026#34;LOG_UOP_BUFF_SIZE\u0026#34; : 15, \u0026#34;LOG_INP_BUFF_SIZE\u0026#34; : 15, \u0026#34;LOG_WGT_BUFF_SIZE\u0026#34; : 18, \u0026#34;LOG_ACC_BUFF_SIZE\u0026#34; : 17 } VTA Overview VTA는 dense liniar algebra 연산을 위한 RISC-like 프로세서로 memory access latency를 줄이기 위해 access-execute가 decouple된 디자인을 가진다. VTA는 4개의 모듈로 구성되며 각각의 모듈은 local memory block과 FIFO로 통신한다. fetch module : DRAM으로 부터 Instruction stream을 loading하고 Decoding 한다 load module: input과 weight를 DRAM으로 부터 on-chip 메모리에 로딩한다. compute module : GEMM core로 선형대수 연산, ALU로 일반 연산 수행. DRAM으로 부터 데이터(?)를 로딩하며 micro-op kernel을 micro-op cache에 로딩한다. store module : compute 모듈의 결과를 DRAM에 저장한다. VTA Archictectural Overview Instruction Set Architecture(ISA) VTA의 ISA는 4개의 CISC Instruction으로 구성됨 (각각 다른 실행시간을 가지며, GEMM/ALU는 micro-code instruction임) (micro-code : 하나의 기계어의 동작을 더 작은 동작들의 조합으로 구현한 것) LOAD : DRAM의 2D tensor를 input/weight buffer 및 register file에 로드. micro-kernel을 micro-op cache에 로딩하기도 함, input/weight 로딩 시 dynamic padding을 지원 GEMM : input과 weight의 매트릭스 곱연산 micro-op 수행, register-file에 결과를 더함 ALU : register-file 텐서 데이터에 대해 매트릭스 ALU(덧셈) micro-op 수행 STORE : output buffer로 부터 DRAM에 2D tensor를 저장 LOAD는 load와 compute 모듈에 의해 실행, GEMM/ALU은 compute모듈에 의해 실행, STORE는 load 모듈에 의해 실행 (상세 내용은 다음 섹션에) Dataflow Excution VTA는 concurrent task 실행 동기화를 위해 각 producer/consumer 모듈이 read-after-write (RAW) and write-after-read (WAR) dependence queues로 연결되어 있음 하기의 그림과 pseudo-code는 모듈이 주어진 instruction을 실행하는 방법을 기술하고 있다(chatGPT 해석) 명령어(insn)가 명령어 큐에서 꺼내집니다. MODULE이 생산자 또는 소비자의 처리를 기다려야 할지를 결정하는 플래그가 설정됩니다. MODULE이 현재 작업을 마친 후 생산자 또는 소비자에게 알릴지를 결정하는 플래그가 설정됩니다. MODULE이 생산자를 기다려야 한다면, 생산자의 원시 큐에 항목이 있을 때까지 기다립니다. 큐가 비어 있으면 건너뛰고, 아니면 큐에서 항목을 꺼냅니다. 마찬가지로, MODULE이 소비자를 기다려야 한다면 소비자의 대기 후 읽기(war) 큐를 확인합니다. 대기 조건이 충족되면 MODULE이 생산자에게 알려야 한다면 생산자의 war 큐에 항목을 넣습니다. 소비자에게 알려야 한다면 소비자의 원시 큐에 항목을 넣습니다. Pipeline Expandability default VTA design은 3-stage load-compute-store task pipeline을 가진 4개의 모듈로 구성되어 있다. VTA pipeline을 확장가능하지만 이는 cost(logic overhead)를 가져오므로 설계자는 3-stage pipeline을 채용했다 예를들어 TPU처럼 텐서 GEMM/ALU를 분리하여 load-gemm-activate-store로 구성할 수 도 있다. Microarchitectural Overview 각각의 모듈에 대하여 설명한다.(3rdparty/vta-hw/hardware/xilinx/sources/vta.cc 참조)\nFetch Module Fetch 모듈은 DRAM의 Instruction Stream을 읽어서 Instruction을 decode하고 다른 모듈의 command queue에 명령을 전송한다. Fetch모듈의 제어를 위해 insn_count, insns, control 레지스터를 가진다.(HLS 코드로 만들어 져 있다.) insns : DRAM 내부 instruction stream의 주소 insns_count : fetch할 insturction의 갯수(DMA로 이동할 instruction 의 갯수) control : fetch 모듈의 시작 Decode 된 instruction은 각 내용에 따라 load, compute, store 모듈의 command queue에 제공됨 STORE insn : Store CMD_QUE에 입력 GEMM/ALU insn : Compute CMD_QUE에 입력 LOAD insn : micro-op kernel이나 레지스터 파일 데이터를 load하는 명령은 Compute CMD_QUE에 입력 LOAD insn : Input이나 weight를 load하는 명령은 Compute CMD_QUE에 입력 CMD_QUE가 full이면 full이 해제될 때 까지 fetch모듈은 stall 됨 (queue를 충분히 크게하라, 병렬 처리 보장 하라) 코드분석 Fetch 모듈의 코드는 간단하게 DRAM으로 부터 Instruction을 읽어 opcode를 확인한 후 instruction을 적절한 모듈의 CMD_QUEUE에 push한다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 /*! log2 of instruction data type width */ #define VTA_LOG_INS_WIDTH 7 /*! Instruction data type width */ #define VTA_INS_WIDTH (1 \u0026lt;\u0026lt; VTA_LOG_INS_WIDTH) typedef ap_uint\u0026lt;VTA_INS_WIDTH\u0026gt; insn_T; typedef struct { /*! \\brief The instruction opcode */ uint64_t opcode : VTA_OPCODE_BIT_WIDTH; /*! \\brief Unused in this instruction */ uint64_t pop_prev_dep : 1; /*! \\brief Pop dependence token from GEMM stage */ uint64_t pop_next_dep : 1; /*! \\brief Unused in this instruction */ uint64_t push_prev_dep : 1; /*! \\brief Push dependence token to GEMM stage */ uint64_t push_next_dep : 1; /*! \\brief Padding */ uint64_t pad_0 : 64 - VTA_OPCODE_BIT_WIDTH - 4; /*! \\brief Padding */ uint64_t pad_1 : 64; } VTAGenericInsn; union VTAInsn { /*! \\brief VTA generic instruction */ VTAGenericInsn generic; /*! \\brief VTA load/store instruction */ VTAMemInsn mem; /*! \\brief VTA GEMM instruction */ VTAGemInsn gemm; /*! \\brief VTA ALU instruction */ VTAAluInsn alu; }; void fetch( uint32_t insn_count, volatile insn_T *insns, hls::stream\u0026lt;insn_T\u0026gt; \u0026amp;load_queue, hls::stream\u0026lt;insn_T\u0026gt; \u0026amp;gemm_queue, hls::stream\u0026lt;insn_T\u0026gt; \u0026amp;store_queue) { PRAGMA_HLS(HLS INTERFACE s_axilite port = insn_count bundle = CONTROL_BUS offset = VTA_FETCH_INSN_COUNT_OFFSET) #pragma HLS INTERFACE m_axi port = insns offset = slave bundle = ins_port #pragma HLS INTERFACE axis port = load_queue #pragma HLS INTERFACE axis port = gemm_queue #pragma HLS INTERFACE axis port = store_queue #pragma HLS INTERFACE s_axilite port = return bundle = CONTROL_BUS INSN_DECODE: for (int pc = 0; pc \u0026lt; insn_count; pc++) { #pragma HLS PIPELINE // Read instruction fields insn_T raw_insn = insns[pc]; VTAInsn insn; insn.generic = *((VTAGenericInsn *) \u0026amp;raw_insn); // Do some partial decoding opcode_T opcode = insn.generic.opcode; memop_id_T memory_type = insn.mem.memory_type; // Push to appropriate instruction queue if (opcode == VTA_OPCODE_STORE) { store_queue.write(raw_insn); } else if (opcode == VTA_OPCODE_LOAD) { if (memory_type == VTA_MEM_ID_INP || memory_type == VTA_MEM_ID_WGT) { load_queue.write(raw_insn); } else { gemm_queue.write(raw_insn); } } else { gemm_queue.write(raw_insn); } } } Compute Module compute 모듈은 tensor 연산을 위한 RISC형태의 모듈로 ALU/GEMM으로 구성 micro-op cache로 부터 micro-op를 읽어서 수행하며 micro-op는 ALU, GEMM operation이 있음 compute 모듈은 footprint를 줄이기 위해 2단 nested loop에서 micro-op secqunce를 실행(조건 분기등을 피하기 위해) 코드분석 instruction을 읽어 Load/Store 모듈에 dependancy가 있으면 DEP_QUEUE를 pop을 대기한다 opcode가 FINISH, LOAD 또는 uOP/ACC 메모리 로드, GEMM/ALU 연산 수행인지 확인하여 해당 동작을 수행한다 Load/Store 모듈이 dependancy를 기다린다면 DEP_QUEUE를 push을 대기한다 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 void compute( volatile uint32_t \u0026amp;done, volatile uop_T *uops, volatile bus_T *biases, hls::stream\u0026lt;insn_T\u0026gt; \u0026amp;gemm_queue, hls::stream\u0026lt;bool\u0026gt; \u0026amp;l2g_dep_queue, hls::stream\u0026lt;bool\u0026gt; \u0026amp;s2g_dep_queue, hls::stream\u0026lt;bool\u0026gt; \u0026amp;g2l_dep_queue, hls::stream\u0026lt;bool\u0026gt; \u0026amp;g2s_dep_queue, bus_T inp_mem[VTA_INP_BUFF_DEPTH][INP_MAT_AXI_RATIO], bus_T wgt_mem[VTA_WGT_BUFF_DEPTH][WGT_MAT_AXI_RATIO], bus_T out_mem[VTA_ACC_BUFF_DEPTH][OUT_MAT_AXI_RATIO]) { PRAGMA_HLS(HLS INTERFACE s_axilite port = done bundle = CONTROL_BUS offset = VTA_COMPUTE_DONE_WR_OFFSET) #pragma HLS INTERFACE m_axi port = uops offset = slave bundle = uop_port #pragma HLS INTERFACE m_axi port = biases offset = slave bundle = data_port #pragma HLS INTERFACE axis port = gemm_queue #pragma HLS INTERFACE axis port = l2g_dep_queue #pragma HLS INTERFACE axis port = s2g_dep_queue #pragma HLS INTERFACE axis port = g2l_dep_queue #pragma HLS INTERFACE axis port = g2s_dep_queue #pragma HLS INTERFACE bram port = inp_mem #pragma HLS INTERFACE bram port = wgt_mem #pragma HLS INTERFACE bram port = out_mem #pragma HLS INTERFACE s_axilite port = return bundle = CONTROL_BUS #pragma HLS RESOURCE variable = inp_mem core = RAM_1P #pragma HLS RESOURCE variable = wgt_mem core = RAM_1P #pragma HLS RESOURCE variable = out_mem core = RAM_1P // Micro-op storage static uop_T uop_mem[VTA_UOP_BUFF_DEPTH]; // Accumulator storage static bus_T acc_mem[VTA_ACC_BUFF_DEPTH][ACC_MAT_AXI_RATIO]; #pragma HLS ARRAY_RESHAPE variable = acc_mem complete dim=2 // This is necessary to obtain II=1 #pragma HLS DEPENDENCE variable = acc_mem inter false // Pop GEMM instruction insn_T raw_insn = gemm_queue.read(); // Cast to GenericInsn VTAInsn insn; insn_T raw_copy = raw_insn; insn.generic = *((VTAGenericInsn *) \u0026amp;raw_copy); // Pop dependence token if instructed if (insn.generic.pop_prev_dep) { l2g_dep_queue.read(); } if (insn.generic.pop_next_dep) { s2g_dep_queue.read(); } // Set done value done = 0; // Perform action based on opcode if (insn.generic.opcode == VTA_OPCODE_FINISH) { // Set done flag if we reach a FINISH instruction done = 1; } else if (insn.generic.opcode == VTA_OPCODE_LOAD) { // Initialize indices memop_sram_T sram_idx = insn.mem.sram_base; memop_dram_T dram_idx = insn.mem.dram_base; memop_sram_T x_width = (insn.mem.x_pad_0 + insn.mem.x_size + insn.mem.x_pad_1); memop_sram_T y_offset_0 = x_width * insn.mem.y_pad_0; memop_sram_T y_offset_1 = x_width * insn.mem.y_pad_1; if (insn.mem.memory_type == VTA_MEM_ID_UOP) { // Perform data transfer memcpy(\u0026amp;uop_mem[sram_idx], (const uop_T*) \u0026amp;uops[dram_idx], insn.mem.x_size * sizeof(uop_T)); } else if (insn.mem.memory_type == VTA_MEM_ID_ACC) { // Perform data transfer from DRAM load_pad_2d\u0026lt;bus_T, ACC_MAT_AXI_RATIO, VTA_ACC_ELEM_BYTES\u0026gt;( biases, acc_mem, sram_idx, dram_idx, insn.mem.y_size, insn.mem.x_size, insn.mem.x_stride, insn.mem.x_pad_0, insn.mem.x_pad_1, y_offset_0, y_offset_1); } } else if (insn.generic.opcode == VTA_OPCODE_GEMM) { gemm(raw_copy, uop_mem, acc_mem, inp_mem, wgt_mem, out_mem); } else if (insn.generic.opcode == VTA_OPCODE_ALU) { alu(raw_copy, uop_mem, acc_mem, inp_mem, wgt_mem, out_mem); } // Push dependence token if instructed if (insn.generic.push_prev_dep) { g2l_dep_queue.write(1); } if (insn.generic.push_next_dep) { g2s_dep_queue.write(1); } } GEMM core GEMM core는 2-level nested loop 상에서 GEMM instruction을 수행을 위한 micro-code sequence를 실행 cycle 당 1개 input-weight matrix multiplication를 수행하며 행렬 연산 dimension은 hardware tensorization intrinsic으로 결정됨 tensorization intrinsic은 input, weight, accumulate tensor의 dimension으로 결정되며 overflow를 막기 위해 accumulator tensor가 더 큰 타입을 가짐 일반적으로 input/weight가 low-precision (8-bits or less), accumulator tenson가 32 bit core의 utilization을 높이기 위해 input buffer, weight buffer, register file 충분한 Read/write bandwidth를 가져야 함 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 void gemm( insn_T insn_raw, uop_T uop_mem[VTA_UOP_BUFF_DEPTH], bus_T acc_mem[VTA_ACC_BUFF_DEPTH][ACC_MAT_AXI_RATIO], bus_T inp_mem[VTA_INP_BUFF_DEPTH][INP_MAT_AXI_RATIO], bus_T wgt_mem[VTA_WGT_BUFF_DEPTH][WGT_MAT_AXI_RATIO], bus_T out_mem[VTA_ACC_BUFF_DEPTH][OUT_MAT_AXI_RATIO]) { #pragma HLS INLINE VTAGemInsn insn = *((VTAGemInsn *) \u0026amp;insn_raw); // Loop offset acc_idx_T dst_offset_out = 0; inp_idx_T src_offset_out = 0; wgt_idx_T wgt_offset_out = 0; // Outer Loop EXE_OUT_LOOP: for (int it_out = 0; it_out \u0026lt; insn.iter_out; it_out++) { acc_idx_T dst_offset_in = dst_offset_out; inp_idx_T src_offset_in = src_offset_out; wgt_idx_T wgt_offset_in = wgt_offset_out; // Inner Loop EXE_IN_LOOP: for (int it_in = 0; it_in \u0026lt; insn.iter_in; it_in++) { // Iterate over micro op READ_GEMM_UOP: for (int upc = insn.uop_bgn; upc \u0026lt; insn.uop_end; upc++) { #pragma HLS PIPELINE II = 1 // Read micro-op fields uop_T uop = uop_mem[upc]; // Decode indices acc_idx_T dst_idx = uop.range(VTA_UOP_GEM_0_1, VTA_UOP_GEM_0_0) + dst_offset_in; inp_idx_T src_idx = uop.range(VTA_UOP_GEM_1_1, VTA_UOP_GEM_1_0) + src_offset_in; wgt_idx_T wgt_idx = uop.range(VTA_UOP_GEM_2_1, VTA_UOP_GEM_2_0) + wgt_offset_in; // Read in weight tensor wgt_T w_tensor[VTA_BLOCK_OUT][VTA_BLOCK_IN]; read_tensor\u0026lt;bus_T, wgt_T, wgt_idx_T, VTA_BUS_WIDTH, VTA_WGT_WIDTH, VTA_BLOCK_OUT, VTA_BLOCK_IN\u0026gt;(wgt_idx, wgt_mem, w_tensor); // Read in input tensor inp_T i_tensor[VTA_BATCH][VTA_BLOCK_IN]; read_tensor\u0026lt;bus_T, inp_T, inp_idx_T, VTA_BUS_WIDTH, VTA_INP_WIDTH, VTA_BATCH, VTA_BLOCK_IN\u0026gt;(src_idx, inp_mem, i_tensor); // Read in accum tensor acc_T a_tensor[VTA_BATCH][VTA_BLOCK_OUT]; read_tensor\u0026lt;bus_T, acc_T, acc_idx_T, VTA_BUS_WIDTH, VTA_ACC_WIDTH, VTA_BATCH, VTA_BLOCK_OUT\u0026gt;(dst_idx, acc_mem, a_tensor); // Output tensor out_T o_tensor[VTA_BATCH][VTA_BLOCK_OUT]; // Inner GEMM loop for (int b = 0; b \u0026lt; VTA_BATCH; b++) { for (int oc = 0; oc \u0026lt; VTA_BLOCK_OUT; oc++) { // Initialize the accumulator values acc_T accum = a_tensor[b][oc]; // Dot product sum sum_T tmp = 0; // Inner matrix multiplication loop (input channel/feature) for (int ic = 0; ic \u0026lt; VTA_BLOCK_IN; ic++) { wgt_T w_elem = w_tensor[oc][ic]; inp_T i_elem = i_tensor[b][ic]; mul_T prod_dsp = i_elem * w_elem; tmp += (sum_T) prod_dsp; } // Update summation accum += (acc_T) tmp; // Write back result acc_mem a_tensor[b][oc] = insn.reset_reg ? (acc_T) 0 : accum; // And output vector o_tensor[b][oc] = (out_T) accum.range(VTA_OUT_WIDTH - 1, 0); } } // Write the results back into accumulator write_tensor\u0026lt;bus_T, acc_T, acc_idx_T, VTA_BUS_WIDTH, VTA_ACC_WIDTH, VTA_BATCH, VTA_BLOCK_OUT\u0026gt;(dst_idx, a_tensor, acc_mem); // Write the results back in the output buffer write_tensor\u0026lt;bus_T, out_T, acc_idx_T, VTA_BUS_WIDTH, VTA_OUT_WIDTH, VTA_BATCH, VTA_BLOCK_OUT\u0026gt;(dst_idx, o_tensor, out_mem); } // Update offsets dst_offset_in += insn.dst_factor_in; src_offset_in += insn.src_factor_in; wgt_offset_in += insn.wgt_factor_in; } // Update offsets dst_offset_out += insn.dst_factor_out; src_offset_out += insn.src_factor_out; wgt_offset_out += insn.wgt_factor_out; } } ALU core Tensor ALU는 ctivation, normalization, pooling을 위한 표준 연산자를 지원 VTA는 모듈식 설계 이므로 Operator coverage를 높이기 위해 지원 연산을 확장할 수 있다고 기술하고 있음 tensor-tensor operations과 tensor-scalar operation(immediate value가 있는)을 지원 micro-code는 오직 지정된 data access pattern만 다룸 Tensor ALU는 읽기 포트가 부족해서 II = 2이고 레지스터 파일이 32bit이므로 tensor-tensor 연산을 한번에 수행하는 것은 비용이 많이듬, 여러 사이클의 vector-vector 연산으로 tensor-tensor 연산을 수행 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 void alu( insn_T insn_raw, uop_T uop_mem[VTA_UOP_BUFF_DEPTH], bus_T acc_mem[VTA_ACC_BUFF_DEPTH][ACC_MAT_AXI_RATIO], bus_T inp_mem[VTA_INP_BUFF_DEPTH][INP_MAT_AXI_RATIO], bus_T wgt_mem[VTA_WGT_BUFF_DEPTH][WGT_MAT_AXI_RATIO], bus_T out_mem[VTA_ACC_BUFF_DEPTH][OUT_MAT_AXI_RATIO]) { #pragma HLS INLINE VTAAluInsn insn = *((VTAAluInsn *) \u0026amp;insn_raw); // Loop offset acc_idx_T dst_offset_out = 0; inp_idx_T src_offset_out = 0; // Outer Loop EXE_OUT_LOOP: for (int it_out = 0; it_out \u0026lt; insn.iter_out; it_out++) { acc_idx_T dst_offset_in = dst_offset_out; inp_idx_T src_offset_in = src_offset_out; // Inner Loop EXE_IN_LOOP: for (int it_in = 0; it_in \u0026lt; insn.iter_in; it_in++) { // Iterate over micro op READ_ALU_UOP: for (int upc = insn.uop_bgn; upc \u0026lt; insn.uop_end; upc++) { #pragma HLS PIPELINE II = 2 // Read micro-op fields uop_T uop = uop_mem[upc]; // Decode acc_idx_T dst_idx = uop.range(VTA_UOP_ALU_0_1, VTA_UOP_ALU_0_0) + dst_offset_in; acc_idx_T src_idx = uop.range(VTA_UOP_ALU_1_1, VTA_UOP_ALU_1_0) + src_offset_in; // Read in src tensor acc_T src_tensor[VTA_BATCH][VTA_BLOCK_OUT]; read_tensor\u0026lt;bus_T, acc_T, acc_idx_T, VTA_BUS_WIDTH, VTA_ACC_WIDTH, VTA_BATCH, VTA_BLOCK_OUT\u0026gt;(src_idx, acc_mem, src_tensor); // Read in dst tensor acc_T dst_tensor[VTA_BATCH][VTA_BLOCK_OUT]; read_tensor\u0026lt;bus_T, acc_T, acc_idx_T, VTA_BUS_WIDTH, VTA_ACC_WIDTH, VTA_BATCH, VTA_BLOCK_OUT\u0026gt;(dst_idx, acc_mem, dst_tensor); // Output tensor out_T o_tensor[VTA_BATCH][VTA_BLOCK_OUT]; // Perform ALU op over matrix elements for (int i = 0; i \u0026lt; VTA_BATCH; i++) { for (int b = 0; b \u0026lt; VTA_BLOCK_OUT; b++) { // Read in operands acc_T src_0 = dst_tensor[i][b]; acc_T src_1 = insn.use_imm ? (acc_T) insn.imm : src_tensor[i][b]; aluop_shr_arg_T shft_by = src_1.range(VTA_SHR_ARG_BIT_WIDTH - 1, 0); aluop_mul_arg_T mul_by = src_1.range(VTA_MUL_ARG_BIT_WIDTH - 1, 0); if (insn.alu_opcode == VTA_ALU_OPCODE_MIN || insn.alu_opcode == VTA_ALU_OPCODE_MAX) { // Compute Min/Max acc_T mix_val = src_0 \u0026lt; src_1 ? (insn.alu_opcode == VTA_ALU_OPCODE_MIN ? src_0 : src_1) : (insn.alu_opcode == VTA_ALU_OPCODE_MIN ? src_1 : src_0); dst_tensor[i][b] = mix_val; o_tensor[i][b] = (out_T) mix_val.range(VTA_OUT_WIDTH - 1, 0); } else if (insn.alu_opcode == VTA_ALU_OPCODE_ADD) { // Compute Sum acc_T add_val = src_0.range(VTA_ACC_WIDTH - 1, 0) + src_1.range(VTA_ACC_WIDTH - 1, 0); dst_tensor[i][b] = add_val; o_tensor[i][b] = (out_T) add_val.range(VTA_OUT_WIDTH - 1, 0); } else if (insn.alu_opcode == VTA_ALU_OPCODE_SHR) { // Compute Shift Right acc_T shr_val = src_0 \u0026gt;\u0026gt; shft_by; dst_tensor[i][b] = shr_val; o_tensor[i][b] = (out_T) shr_val.range(VTA_OUT_WIDTH - 1, 0); } } } // Write the results back into accumulator write_tensor\u0026lt;bus_T, acc_T, acc_idx_T, VTA_BUS_WIDTH, VTA_ACC_WIDTH, VTA_BATCH, VTA_BLOCK_OUT\u0026gt;(dst_idx, dst_tensor, acc_mem); // Write the results back in the output buffer write_tensor\u0026lt;bus_T, out_T, acc_idx_T, VTA_BUS_WIDTH, VTA_OUT_WIDTH, VTA_BATCH, VTA_BLOCK_OUT\u0026gt;(dst_idx, o_tensor, out_mem); } // Update offsets dst_offset_in += insn.dst_factor_in; src_offset_in += insn.src_factor_in; } // Update offsets dst_offset_out += insn.dst_factor_out; src_offset_out += insn.src_factor_out; } } Load and Store Modules Load\u0026amp;Store 모듈은 DRAM에서 SRAM으로 데이터를 전송하기 위한 2D DMA 수행 stride와 2D Pading을 지원하므로 CPU에서 data를 re-lay하는 오버헤드를 줄일 수 있음 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 void load( volatile bus_T *inputs, volatile bus_T *weights, hls::stream\u0026lt;insn_T\u0026gt; \u0026amp;load_queue, hls::stream\u0026lt;bool\u0026gt; \u0026amp;g2l_dep_queue, hls::stream\u0026lt;bool\u0026gt; \u0026amp;l2g_dep_queue, bus_T inp_mem[VTA_INP_BUFF_DEPTH][INP_MAT_AXI_RATIO], bus_T wgt_mem[VTA_WGT_BUFF_DEPTH][WGT_MAT_AXI_RATIO]) { #pragma HLS INTERFACE m_axi port = inputs offset = slave bundle = data_port #pragma HLS INTERFACE m_axi port = weights offset = slave bundle = data_port #pragma HLS INTERFACE axis port = load_queue #pragma HLS INTERFACE axis port = g2l_dep_queue #pragma HLS INTERFACE axis port = l2g_dep_queue #pragma HLS INTERFACE bram port = wgt_mem #pragma HLS INTERFACE bram port = inp_mem #pragma HLS INTERFACE s_axilite port = return bundle = CONTROL_BUS #pragma HLS RESOURCE variable = inp_mem core = RAM_1P #pragma HLS RESOURCE variable = wgt_mem core = RAM_1P // Pop load instruction insn_T raw_insn = load_queue.read(); // Cast to MemInsn insn_T raw_copy = raw_insn; VTAMemInsn insn = *((VTAMemInsn *) \u0026amp;raw_copy); // Pop dependence token if instructed if (insn.pop_next_dep) { g2l_dep_queue.read(); } // Pre-processing memop_sram_T x_width = (insn.x_pad_0 + insn.x_size + insn.x_pad_1); memop_sram_T y_offset_0 = x_width * insn.y_pad_0; #pragma HLS RESOURCE variable = y_offset_0 core = Mul_LUT latency = 4 memop_sram_T y_offset_1 = x_width * insn.y_pad_1; #pragma HLS RESOURCE variable = y_offset_1 core = Mul_LUT latency = 4 if (insn.memory_type == VTA_MEM_ID_INP) { load_pad_2d\u0026lt;bus_T, INP_MAT_AXI_RATIO, VTA_INP_ELEM_BYTES\u0026gt;( inputs, inp_mem, insn.sram_base, insn.dram_base, insn.y_size, insn.x_size, insn.x_stride, insn.x_pad_0, insn.x_pad_1, y_offset_0, y_offset_1); } else if (insn.memory_type == VTA_MEM_ID_WGT) { load_2d\u0026lt;bus_T, WGT_MAT_AXI_RATIO, VTA_WGT_ELEM_BYTES\u0026gt;( weights, wgt_mem, insn.sram_base, insn.dram_base, insn.y_size, insn.x_size, insn.x_stride); } // Push dependence token if instructed if (insn.push_next_dep) { l2g_dep_queue.write(1); } } void store( volatile bus_T *outputs, hls::stream\u0026lt;insn_T\u0026gt; \u0026amp;store_queue, hls::stream\u0026lt;bool\u0026gt; \u0026amp;g2s_dep_queue, hls::stream\u0026lt;bool\u0026gt; \u0026amp;s2g_dep_queue, bus_T out_mem[VTA_ACC_BUFF_DEPTH][OUT_MAT_AXI_RATIO]) { #pragma HLS INTERFACE m_axi port = outputs offset = slave bundle = data_port #pragma HLS INTERFACE axis port = store_queue #pragma HLS INTERFACE axis port = g2s_dep_queue #pragma HLS INTERFACE axis port = s2g_dep_queue #pragma HLS INTERFACE bram port = out_mem #pragma HLS INTERFACE s_axilite port = return bundle = CONTROL_BUS #pragma HLS RESOURCE variable = out_mem core = RAM_1P // Pop store instruction insn_T raw_insn = store_queue.read(); // Cast to MemInsn insn_T raw_copy = raw_insn; VTAMemInsn insn = *((VTAMemInsn *) \u0026amp;raw_copy); // Pop dependence token if instructed if (insn.pop_prev_dep) { g2s_dep_queue.read(); } // Initialize indices memop_sram_T sram_idx = insn.sram_base; memop_dram_T dram_idx = insn.dram_base; // Copy along y dimension for (int y = 0; y \u0026lt; insn.y_size; y++) { #pragma HLS PIPELINE // Perform data transfer memcpy( const_cast\u0026lt;bus_T*\u0026gt;(\u0026amp;outputs[dram_idx * OUT_MAT_AXI_RATIO]), (const bus_T*) \u0026amp;out_mem[sram_idx][0], insn.x_size * VTA_OUT_ELEM_BYTES); #pragma HLS RESOURCE variable = sram_idx core = Mul_LUT sram_idx += insn.x_size; dram_idx += insn.x_stride; } // Push dependence token if instructed if (insn.push_prev_dep) { s2g_dep_queue.write(1); } } Vivado 구조 분석 실제 HLS 파일에는 vta top 모듈이 정의 되어 있지만 실제로 vivado에서 합성한 모듈은 top모듈을 사용하지 않고 Fetch, Compute, Load, Store 모듈과 FIFO IP를 이용하여 연결하였다. 이유는 글쎄? ","date":"2024-07-07T00:00:00Z","permalink":"https://muonkmu.github.io/p/tvmvta-vta-concept-and-design/","title":"[TVM\u0026VTA] VTA Concept and Design"},{"content":"사전 설치 파일 필요 패키지를 설치한다 cmake는 3.18 이상 버전이 필요하다(우분투 20.04에서는 apt로 설치 시 3.10버전이 설치되므로 repo를 등록하여 업데이트 하자) llvm을 설치하기를 권장하고 있다. 최신버전은 llvm-18은 호환성 문제로 tvm이 빌드가 안된다. apt를 이용시 llvm-10이 설치되므로 이를 이용하자. 1 2 sudo apt-get update sudo apt-get install -y python3 python3-dev python3-setuptools gcc libtinfo-dev zlib1g-dev build-essential cmake libedit-dev libxml2-dev llvm conda를 설치하고 python=3.8 버전의 환경을 구성한다.(option) 필자는 패키지 의존성 꼬이는게 싫어서 conda 환경을 구성했다. 그러나 문제가 많이 생겨서 삽질 많이함 1 2 conda create -n tvm python=3.8 conda active tvm 빌드 및 설치 소스를 다운로드 한다. 1 git clone --recursive https://github.com/apache/tvm tvm 공식 매뉴얼에 conda 환경을 꾸미고 conda 환경에서 tvm 빌드 및 설치하는 방법이 나와 있으나 매뉴얼 대로 진행시 정상적으로 동작하지 않는다. 따라서 Cmake를 이용하여 빌드할 예정이다. tvm/conda 폴더에 보면 해당 파일들이 확인 가능하다. build 폴더 생성, config파일 수정 복사를 진행한다. 1 2 mkdir build cp cmake/config.cmake build llvm을 사용할 경우 set(USE_LLVM ON), 가상환경을 사용하므로 set(USE_LLVM \u0026quot;/path/to/llvm-config --link-static\u0026quot;) 디버깅을 위해 set(USE_GRAPH_EXECUTOR ON) 및 set(USE_PROFILER ON) IR 디버깅시 set(USE_RELAY_DEBUG ON) 및 환경변수 선언 export TVM_LOG_DEBUG=\u0026quot;ir/transform.cc=1,relay/ir/transform.cc=1\u0026quot; tvm 빌드 1 2 3 cd build cmake .. make -j4 python dependecies 패키지 설치(설치중에 패키지가 필요할 수 있으므로 미리 설치한다) tvm/python 폴더에 gen_requirements.py를 실행하면 필수 패키지들을 볼 수 있다. 1 2 3 4 conda install numpy decorator attrs //필수 패키지 conda install typing-extensions psutil scipy //tvmc 사용시 conda install tornado //RPC talker 사용시 conda install tornado psutil \u0026#39;xgboost\u0026gt;=1.1.0\u0026#39; cloudpickle // auto-tuning 모듈 사용시 tvm 패키지 설치 환경 변수 설정, 환경변수를 선언해 놓으면 변경사항이 즉시 반영된다고 한다(변경 시setup 호출이 필요하지 않음)(필요 시 ~./bashrc 추가) 1 2 export TVM_HOME=/path/to/tvm export PYTHONPATH=$TVM_HOME/python:${PYTHONPATH} 라이브러리 복사 1 cd python; python setup.py install; cd .. pynq2.7 설치 cmake 지우고 재설치 (sudo ln -s /opt/cmake-3.29.0-linux-aarch64/bin/* /usr/local/bin/) tvm 다운로드 () git tag version\nzcu104 플래그 추가하고 빌드\n에러 해결 AttributeError: /home/xilinx/tvm/vta/python/vta/../../../build/libvta.so: undefined symbol: VTARuntimeShutdown 리빌드 링크 https://discuss.tvm.apache.org/t/error-vtaruntimeshutdown-symbol-is-undefined/6832/10\n","date":"2024-02-22T00:00:00Z","permalink":"https://muonkmu.github.io/p/tvmvta-tvm-install-base-on-conda/","title":"[TVM\u0026VTA] TVM Install (base on CONDA)"},{"content":"Abtract 딥러닝 추론을 위한 대부분의 컴퓨팅 작업은 convolutions, activations, pooling, normalization로 그룹화 될 수 있음, 이는 예측가능한 메모리 접근 패턴과 병렬화 하기 적합한 특성을 지님 NVDLA 간단하고 유연하며 견고한 추론 가속화 솔루션을 제공(다양한 성능 수준을 제공,) NVDLA는 HW/SW 스택을 제공 HW : Verilog model(합성/시뮬레이션용 RTL), TLM SystemC(소프트웨어 개발 /시스템통합 / 검증용) SW : On-device SW stack(동작 바이너리?), training intrastructure(신규모델 개발용), parser(컴파일러 역활) Accelerating Deep Learning Inference using NVDLA NVDLA는 모듈러 구조이며 각 모듈은 독립적으로 구성 및 확장 가능하다(scalalbe, CONV core를 더 넣거나 average engine을 뺄 수 있다.) Convolution Core : convolution engine Single Data Processor : Activation function을 위한 single point lookup engine Planar Data Processor : pooling을 위한 planar averaging engine Channel Data Processor : Normalization을 위한 Multi-channel averaging engine Dedicated Memory and Data Reshape Engine : tensor reshape 와 copy를 위한 mem-to-mem 가속기(DMA?) 각 유닛의 스케줄링은 co-processor나 CPU에 의해 동작(fine-grained scheduling) NVDLA 모듈 내 co-processor가 있는 걸 headed, CPU가 스케쥴링 하는 걸 Headless라고 하는 듯 (현재는 릴리즈 된건 headless 만) NVDLA 컨트롤 채널은 AXI bus를 사용(레지스터, 인터럽트 포함), primary memory IF는 시스템 DRAM 연결용, HBM 연결을 위한 2nd-mem-IF를 optional로 제공 일반적 인터페이싱 플로우는 Command-excute-interrupt 구조 CPU나 co-processor가 layer의 hardware configuration 정보를 active command와 전송(이는 데이터 디펜던시가 없다면 multiple layer를 병렬로 처리 가능) 모든 엔진은 config register를 위한 더블 버퍼를 가지고 있어서 active layer가 끝나는 직시 second layer가 실행 가능(다음 레이어 정보를 미리 로드할 수 있다는 듯) 엔진 동작이 끝나면 인터럽트로 알림 NVDLA는 두가지 모델을 제공(제안?) small system 모델은 headless로 저사양, large system model은 co-processor/sram-IF가 추가된 고성능 IoT용(sram은 NVDLA의 캐시로 사용) 사이트에 더 상세한 설명(그러나 일반적인 내용)이 기술되어 있음 Hardware Architecture NVDLA는 두 가지 operation 모드로 프로그램 될 수 있음 Independent : 각 블럭은 독립적으로 동작, 각 블럭은 지정된 메모리 영역 읽으면서 동작 시작, 끝나면 지정된 메모리 영역에 데이터를 씀 Fused : Independent와 비슷하나 몇몇 블럭이 파이프라인(FIFO)을 통해 통합될 수 있음 Connection NVDLA는 다른 시스템과 연결을 위한 3가지 인터페이스를 제공 Configureation Space Bus(CSB) : NVDLA config register 접근을 위한 32비트 버스(low BW/Power), AMBA나 OCP 등으로 쉽게 변환 가능 Interrupt : 1bit level driven, task의 완료나 에러를 통지 Data Backbone (DBB) : 메인 시스템 메모리 연결을 위한 유사 AXI 버스, address-size / bus-width / burst-size 등을 조절 가능 DBB는 On-Chip SRAM 등을 연결하기 위한 optional 인터페이스를 구현할 수 있음 Components Convolution : tf.nn.conv2d input \u0026ldquo;feature\u0026quot;와 \u0026ldquo;weight\u0026quot;의 convolution engine, 다양한 사이즈에 매핑 시키기 위해서 파라메터를 expose함(무슨 의미?) 일반적 콘볼루션을 위한 optimiztion 포함, sparse weight compression / Winograd Convolution / Batch convolution 지원 weight 와 input feature 저장을 위한 convolution buffer(RAM)을 내장 Single Data Point Processor : tf.nn.batch_normaliztion, tf.nn.bias_add, tf.nn.elu, tf.nn.relu, tf.sigmoid, tf.tanh Convloution 이후에 쓰임, non-linear function 구현을 위한 lookup table을 가짐, linear-funtion은 bias와 scaling 지원 위의 두 함수의 조합으로 대부분의 activation function 및 element-wise 연산을 지원 Planar Data Processor : tf.nn.avg_pool, tf.nn.max_pool, tf.nn.pool runtime 중 다른 사이즈의 pool group config 지원, max/avg/min pool 지원 Cross-channel Data Processor : tf.nn.local_response_nomaliztion local response nomalization(LRN)을 위한 유닛 LRN : Alexnet에 사용된 것으로 activation map의 같은 위치에 있는 픽셀끼리 정규화, 요즘은 Batch normalization을 써서 잘안쓴다. Data Reshape Engine : tf.nn.conv2d_transpos, tf.concat, tf.slice, tf.transpose data format transformation(slicing, merging, reshape-transpose 같은)을 수행 Bridge DMA System DRAM과 high-performance memory interface 간 data copy / move engine Configurability (NVDLA는 광범위한 하드웨어 파라메터를 제공)\nData type : int4/8/16/32, fp16/32/64 지원 (선택가능한 데이터 타입은 바이너리를 포함?) Input image memory formats: planar/semi-planar image와 packaged memory format을 지원 RGB 이미지가 RGB RGB RGB 형태이면 packaged, RRR BBB GGG 형태이면 planar Weight compression : sparsely storing convolution weight를 on/off 가능 Winograd convolution : winograd convolution 기능을 on/off 가능 Batched Convolution : Batched convolution(메모리 밴드위스를 줄이기 위한) 기능 on/off 가능 Convolution buffer size : buffer bank 2~32, 각 bank의 size 4/8KiB 선택 가능 MAC array size : MAC은 C(width dimension, 8~64) * K(depth dimension, 4~64) 형태, 조절 가능 Second memory interface : high-speed access를 위한 추가 메모리 인터페이스 포함 여부 선택 가능 Non-linear activation function : 비선형 함수(sigmoid, tanh등)을 위한 lookup table 삭제 가능 Active engine size : 한 사이클당 생산되는 activation output이 1~16까지 조절 가능 Bridge DMA : 삭제 가능 Data reshape engine : 삭제 가능 Pooling engine presence : pooling engine 삭제 가능 Pooling engine size : 한 사이클당 1~4개의 출력을 생성하도록 조절 가능 Local response nomalization engine presence : 삭제가능 Local response nomalization engine size : 한 사이클당 1~4개의 출력을 생성하도록 조절 가능 Memory interface bit width : 외부 메모리 인터페이스 width에 따라 조절 가능 Memory read latency tolerance : memory latency time(read request 부터 return까지 cycle) 조절 가능 (read DMA의 latency buffer size에 영향) Software Design NVDLA는 on-device sw stack, full training infrastructure를 포함하는 sw 생태계를 지원 compilation tool(model conversion)과 runtime environement(runtime sw load/excute) 그룹으로 나뉨 Compilation Tools : Model Creation and Compilation copilation tool은 compiler와 parse를 포함 compiler : NVDLA config에 따른 최적화된 HW layer sequence를 생성 parser : caffe model을 읽어서 \u0026ldquo;intermediate representation\u0026quot;을 생성 compiler는 intermediate representation과 NVDLA HW config를 입력받아 HW layer network를 생성 Runtime Environment : Model Inference on Device NVDLA RTE는 running model을 포함하며 2개의 layer로 구분됨 User Mdoe Driver(UMD) : compiler는 NVDLA loaderble(binary?)를 생성하는데 이를 로드하고 Kernel mode 드라이버에 interfernce job을 제공 Kernel Mode Driver(KMD) : 펌웨어와 드라이버로 구성되며 layer를 스케줄링하고 register를 설정 HW의 NVDLA function block처럼 SW 관점에서는 모듈을 \u0026ldquo;layer\u0026quot;로 불림(UMD, KMD), layer에는 layer간 dependency, tensor, config에 대한 정보를 포함 각 layer는 dependancy graph를 통해 연결(KMD가 각 operation을 위해 이를 이용) UMD : image load, input/output memory binding, inference run을 위한 API, 리눅스에서는 ioctl()일 수 있음 KMD : interferece job을 수신 및 선택(multi-processing일때)하여 core engine scheduler에 전송 core engine scheduler는 인터럽트를 핸들링/ function block 스케줄링 / dependency 업데이트를 함, 스케쥴러는 dependency graph를 이용하여 subsequent layer가 스케쥴링 준비가 되었는지 결정 KMD/UMD 둘다 API 형태로 제공되므로 이를 wrapping하여 portability layer를 integrator가 작성해야 함 NVDLA System Integration NVDLA는 요구사항을 맞출 수 있는 광범위한 파라메터를 제공, 원하는 성능을 위해 MAC unit의 수 / convolution buffer size / on-chip SRAM size를 선택하는 것은 중요한 스텝임 Tuning Question What math precision is required for the workload expected for any given instantiation? NVDLA의 사용 면적은 대부분이 MAC 유닛과 conv buffer에 의해 결정 트레이닝 시에는 32bit fp를 사용하지만 8bit로 줄일 수도 있다..(그냥 양자화 하라는 이야기인듯) What are the number of MAC unit, and the required memory bandwidth? MAC throughput이나 memory bandwidth가 bottleneck이 될 수 있다. MAC unit의 수는 결정하기 쉬운데 layer의 input/output size, kernel size가 결정되 있기 때문이다. 요구되는 operation 수를 MAC unit의 수로 나누면 레이어를 처리하는데 필요한 클럭 사이클이 나옴 Memory BW 경우 input image, output image, weigth를 한번씩 읽어야 하므로 최소 cycle은 상기 합을 읽고 써야하는 샘플의 수로 나눈 값이다. 그러나 convolution buffer가 작아서 iput이나 weight를 한번에 저장할 수 없다면 작업은 나누어 져야 한다. Is there a need for on-chip SRAM? external memory bandwidth가 성능/비용 코스트가 많이 든다면 on-chip SRAM이 second level cache로써 도움이 될 수 있다.(SRAM이 시스템 메모리보다 빨라야 함) convolution buffer를 키우는 것보다 SRAM이 싸지만 convolution buffer limited 상황에서는 SRAM이 큰 효과를 주는 factor는 아니다. 즉, 레이어의 대역폭이 제한된 경우, 시스템 메모리의 2배수로 동작하는 입력이미지를 저장하기에 충분한 SRAM이 있다면 성능은 두배가 됨, 그러나 컨볼루션 버퍼의 크기에 의하여 성능이 제한된다면 대역폭 문제가 아니므로 SRAM 추가해도 별 소용 없다. 결국, Convolution buffer 크기는 Bandwidth 요구량을 줄여주고, SRAM은 가용한 대역폭을 늘려준다. Example Area and Performance with NVDLA 사이트에 MAC unit 수, Conv buffer 사이즈 등에 따른 NVDLA에서 ResNet-50의 성능 및 power estimation/silicon area이 나와 있다. Sample platform Simulation GreenSocs QBOX 기반의 시뮬레이션 플랫폼 QEMU CPU 모델(x86, ARMv8)은 NVDLA SystemC와 결합하여 SW 개발/디버깅을 위한 register-accurate system 제공 FPGA FPGA를 위한 NVDLA verilog model(합성 가능) PPA 최적화를 위한 노력이 들어가있지 않으므로 SoC 성능과 비교하지 말라고 되어 있음 Amazon EC2 \u0026ldquo;F1\u0026rdquo; 환경을 사용(xilinx base) model verilog model RTL form의 synthesis, simulation model 4개의 functional interface를 가짐 : slave host interface, interrupt line, two master interface(내/외부 메모리 접근용) host와 메모리 인터페이스는 soc 통합을 위한 어댑터가 필요(AXI4 어댑터 제공) NVDLA core는 single clock domain이나 bus adaptor는 clock domain crossing을 허용(버스 어댑터에에서 클럭 도메인 크로싱이 된다는 뜻인듯) Simulation model and verification suite 소프트웨어 개발, 시스템 통합 및 테스트를 위한 TLM2 SystemC 모델(Synopsys VDK나 GreenSocs QBox플랫폼 같은 시뮬레이션 환경에서 사용되도록 작성) software 초기 NVDLA 오픈소스는 \u0026ldquo;headless\u0026quot;를 위한 소프트웨어만 포함(리눅스 환경), Kernel mode driver, user mdoe test utility가 소스형태로 제공됨 ","date":"2023-12-16T00:00:00Z","permalink":"https://muonkmu.github.io/p/nvdla-nvdla-primer/","title":"[NVDLA] NVDLA Primer"},{"content":" 시그널은 비동기 이벤트 처리를 위한 메커니즘을 제공하는 소프트웨어 인터럽트(시그널의 발생/처리 모두 비동기) 시그널은 초창기 부터 있었고 신뢰성 측면에서 향상을 이루었고(사라지거나 하지 않게) 사용자 데이터를 전달할 수 있도록 발전 POSIX에서 시그널 처리를 표준화함 시그널 개념 시그널의 생명 주기 : 시그널 발생 -\u0026gt; 커널은 해당 시그널 전달 가능 시까지 쌓아둠 -\u0026gt; 커널은 가능 시점에 시그널 처리 커널은 프로세스 요청에 따라 세가지 중 한가지 동작 수행 시그널 무시 : 아무동작 하지 않음(SIGKILL, SIGSTOP은 무시 안됨) 시그널 처리 : 커널은 프로세스 현재 코드 실행 중지 후 등록된 시그널 핸들러 수행(SIGKILL, SIGSTOP은 잡을 수 없음) 기본동작 수행 : 기본 동작은 시그널에 따라 다름(대부분 프로세스 종료) 시그널 식별자 모든 시그널은 SIG라는 접두어로 시작하는 상징적 이름이 있으며 \u0026lt;signal.h\u0026gt;파일에 정의 됨(#define 양의 정수 형태) 시그널 번호는 1에서 시작, 대략 31개의 시그널이 있음(0은 NULL Signal, 아무런 행동도 정의되지 않음) 리눅스에서 지원하는 시그널 책에 리눅스에 지원하는 시그널 종류 및 자세한 설명 시그널 관리 기초 시그널 관리를 위한 가장 단순하고 오래된 인터페이스는 signal() 함수 1 2 3 4 5 #include \u0026lt;signal.h\u0026gt; typedef void (*sighandler_t)(int) sighandler_t signal (int signo, sighandler_t handler); signal()은 호출이 성공하면 signo 시그널을 받았을 때 수행할 handler를 설정 handler의 인자는 signo와 같은 시그널 식별자 handler는 함수 포인터 외 SIG_DFL(동작을 기본값으로 설정)과 SIG_IGN(시그널 무시)도 설정 가능 signal() 함수는 호출 성공 시 이전 handler 또는 SIG_DFL, SIG_IGN을 반환, 에러 발생 시 SIG_ERR 반환(errn 설정 없음) 모든 시그널 기다리기 1 2 3 #include \u0026lt;unistd.h\u0026gt; int pause(void); pause() 시스템 콜은 프로세스를 종료시키는 시그널을 받을 때까지 해당 프로세스를 잠재움(디버깅/테스트용 코드 작성 시 유용) pause()는 붙잡을 수 있는 시그널을 받았을 때만 반환, -1을 반환하고 errno를 EINTR로 설정(무시된 시그널을 받은 경우 안깨어남) 예제 책에 예제 있음 실행과 상속 fork() 시스템 콜을 통해서 프로세스 생성 시 자식 프로세스는 부모 프로세스의 시그널에 대한 동작(무시, 기본동작, 핸들러)을 상속, 대기중인 시그널은 상속되지 않음 exec 시스템 콜을 통해서 프로세스 생성 시 모든 시그널은 부모 프로세스가 무시하는 경우를 제외하고 기본동작으로 설정, 대기중인 시그널은 상속 쉘이 백그라운드에서 프로세스 실행 시 새로 실행되는 프로세스는 인터럽트 문자와 종료 문자를 무시해야 함 (SIGINT와 SIGQUIT가 SIG_IGN으로 설정해야함, 두시그널이 무시되지 않음을 확인하기 위해 시그널 동작을 설정해봐야 한다는 것은 singal() 인터페이스의 결점) 시그널 번호를 문자열에 맵핑하기 1 extern const char* const sys_siglist[]; sys_siglist는 시그템에서 지원하는 시그널 이름을 담고 있는 문자열 배열, 시그널 번호를 색인으로 이용 대안으로 BSD/linux에서 지원하는 psignal(), strsignal()이 있으나 보통 sys_siglist가 최선임 시그널 보내기 1 2 3 4 #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;signal.h\u0026gt; int kill (pid_t pid, int signo); kill() 시스템콜은 특정 프로세스에서 다른 프로세스로 시그널 전송 pid \u0026gt; 0이면 큰 경우 pid가 가리키는 프로세스에 signo시그널 전송 pid = 0이면 호출한 프로세스의 프로세스 그룹에 속한 모든 프로세스에 signo 시그널 전송 pid = -1이면 호출한 프로세스가 시그널을 보낼 권한이 있는 모든 프로세스에 signo를 보냄(호출한 프로세스 자신과 init는 제외) pid \u0026lt; -1이면 프로세스 그룹-pid에 signo 시그널 전송 호출이 성공하면 0을 반환(시그널 하나라도 전송 시 성공으로 간주) 실패하면 -1을 반환 및 errno를 설정(EINVAL, EPERM, ESRCH) 권한 CAP_KILL 기능이 있는 프로세스(보통 root 프로세스)는 모든 프로세스에 시그널을 보낼 수 있음 상기 기능이 없다면 사용자는 자신이 소유하고 있는 프로세스에만 시그널을 보낼 수 있음 시그널을 보내는 프로세스의 유효 사용자 ID나 실제 사용자 ID는 시그널을 받는 프로세스의 실제 사용자 ID나 저장된 사용자 ID와 동일해야 함 signo가 0(NULL signal)이면 호출은 시그널을 보내지 않지만 에러검사를 수행, 시그널을 보낼 수 있는 권한이 있는지 검사할 때 유용 예제 책에 시그널을 보내는 예제와 시그널을 보낼 수 있는 권한이 있는지 검사하는 예제 있음 자신에게 시그널 보내기 1 2 3 4 #include \u0026lt;signal.h\u0026gt; int raise(int signo); // kill( getpid(), signo) 와 같다 raise()는 자기 자신에게 시그널을 보내는 함수 호출 성공시 0, 실패하면 0이 아닌 값을 반환, errno는 설정하지 않음 프로세스 그룹 전체에 시그널 보내기 1 2 3 4 #include \u0026lt;signal.h\u0026gt; int killpg(int pgrp, int signo); // kill(-pgrp, signo)와 같다 killpg()는 프로세스 그룹에 속한 모든 프로세스에 시그널을 보냄 (프로세스 그룹ID를 음수로 바꿔서 kill() 사용하는 대신) pgrp가 0인 경우 호출하는 프로세스의 그룹에 속한 모든 프로세스에 signo로 지정한 시그널을 보낸다. 호출이 성공하면 0을 반환, 실패시 -1반환 및 errno를 설정(EINVAL, EPERM, ESRCH) 재진입성 시그널은 소프트웨어 인터럽트이므로 시그널 핸들러에서 글로벌 데이터/공유데이터를 손대지 않는 것이 바람직(다음절에서 일시적으로 공유데이터를 안전하게 처리하는법(시그널 블록)을 설명) 일부 함수는 확실히 재진입이 가능하지 않으므로 주의 재진입이 가능한 함수 시그널 핸들러 작성시 중단된 프로세스가 재진입이 불가능한 함수를 수행하는 중이었다고 가정할 것 그래서 시그널 핸들러는 반드시 재진입이 가능한 함수만 이용해야 함 책에서 시그널 사용 시 안전하게 재진입이 가능한 함수 목록을 기술하고 있음 시그널 모음 시그널 집합 연산은 프로세스가 블록한 시그널 모음이나 프로세스에 대기 중인 시그널 모음을 관리 1 2 3 4 5 6 7 8 9 #include \u0026lt;signal.h\u0026gt; int sigemptyset (sigset_t* set); int sigfillset (sigset_t* set); int sigaddset(sigset_t* set, int signo); int sigdelset(sigset_t* set, int signo); int sigismember(const sigset_t* set, int signo); 하기 함수는 sigset 초기화 함수, 두 함수 모두 0 반환하며 시그널 모음을 사용하려면 두 함수 중 하나를 먼저 호출해야 함 sigemptyset()은 set으로 지정한 시그널 모음을 비어있다고 표시하여 초기화 sigfillset()은 set으로 지정한 시그널 모음을 가득 차 있다고 표시하여 초기화 하기 함수는 sigset 추가 삭제 함수, 두 함수 모두 성공 시 0 반환, 실패 시 -1 반환 및 errno 설정(EINVAL) sigaddset()은 set으로 지정한 시그널 모음에 signo를 추가 sigdelset()은 set으로 지정한 시그널 모음에 signo를 삭제 sigismember()는 set으로 지정한 시그널 모음에 signo가 있으면 1을 반환, 없으면 0반환, 에러시 -1 반환 및 errno 설정(EINVAL) 추가적인 시그널 모음 함수 리눅스는 POSIX외 비표준 함수 제공 (POSIX 호환이 중요한 프로그램에서는 사용하지 말 것) 1 2 3 4 5 6 #define _GNU_SOURCE #include \u0026lt;signal.h\u0026gt; int sigisemptyset(sigset_t* set); int sigorset(sigset_t* dest, sigset_t* left, sigset_t* right); int sigandset(sigset_t* dest, sigset_t* left, sigset_t* right); sigisemptyset()은 set으로 지정된 시그널 모음이 비어 있는 경우에는 1을, 아닌 경우 0을 반환 sigorset()은 시그널 모음인 left와 right의 합집함(이진 OR)을 dest에 넣고 sigandset()은 교집함(이진 AND)을 dest에 넣음 두 함수 모두 성공하면 0을 반환, 에러 발생 시 -1 반환 및 errno를 설정(EIVAL) 시그널 블록 시그널 핸들러와 프로그램의 다른 부분이 데이터를 공유해야 할 때(크리티컬 섹션) 시그널 전달을 보류하여 영역을 보호(= 시그널 블록) 프로세스가 블록한 시그널 모음을 해당 프로세스의 시그널 마스크라 함 1 2 3 #include \u0026lt;signal.h\u0026gt; int sigprocmask(int how, const sigset_t* set, sigset_t* oldset); sigprocmask()는 how 값에 따라 다르게 동작 SIG_SETMASK : 호출한 프로세스의 시그널 마스크를 set으로 변경 SIG_BLOCK : 호출한 프로세스의 시그널 마스크에 set에 포함된 시그널 추가 SIG_UNBLOCK : 호출한 프로세스의 시그널 마스크에서 set에 포함된 시그널을 제거 oldset이 NULL이 아니면 이전 시그널 모음을 oldset에 넣음 set이 NULL인 경우, how를 무시하고 마스크를 변경하지 않으나 oldset에는 넣음 호출이 성공하면 0을 반환, 실패하면 -1을 반환 및 errno를 설정(EINVAL, EFAULT) SIGKILL이나 SIGSTOP은 블록할 수 없으며 sigprocmask()는 두 시그널을 추가하려는 시도는 무시함 대기 중인 시그널 조회하기 커널에서 블록된 시그널이 발생할 경우, 이 시그널은 전달되지 않음, 이러한 시그널을 pending 시그널 pending 시그널은 시그널 블록이 해제되면 커널은 이른 프로세스에 넘겨 처리하게 함 1 2 3 #include \u0026lt;signal.h\u0026gt; int sigpending (sigset_t* set); sigpending()은 호출이 성공하면 대기중인 시그널 모음을 set에 넣고 0을 반환, 실패 시 -1 반환 및 errno 설정(EFAULT) 여러 시그널 기다리기 1 2 3 #include \u0026lt;signal.h\u0026gt; int sigsuspend (const sigset_t* set); sigsuspend()는 프로세스가 자신의 시그널 마스크를 일시적으로 변경 후 시그널 발생시 까지 대기, 시그널이 프로세스를 종료시키는 경우 반환되지 않음 시그널이 발생해서 이를 처리한 경우 시그널 핸들러가 반환한 후에 sigsuspend()는 -1를 반환 및 errno를 EINTR로 설정 sigsuspend()의 활용 방법은 프로그램이 크리티컬 섹션에 머물러 있을 때 도착해서 블록되었던 시그널 조회 sigprosmask()를 호출 시 이전 마스크를 oldset에 저장, 크리티컬 섹션 빠져나온 후 oldset으로 sigsuspend() 호출 고급 시그널 관리 signal() 함수는 매우 기초적이나 sigaction()이 더 많은 능력(POSIX 표준) 사용하면 핸들러가 동작하는 동안 지정한 시그널 블럭, 시그널 수신 시점의 시스템과 프로세스에 대한 넓은 데이터 조회 등 1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;signal.h\u0026gt; struct sigaction { void (*sa_handler)(int); /* 시그널 핸들러 또는 동작 */ void (*sa_sigaction)(int, siginfo_t*, void*) sigset_t sa_mask; /* 블록할 시그널 */ int sa_flags; /* 플래그 */ void (*sa_restorer)(void); /* 사용되지 않으며 POSIX표준이 아님 */ } int sigaction(int signo, const struct sigaction* act, struct sigaction* oldact) sigaction() 호출하면 signo로 지정한 시그널의 동작 방식을 변경(signo에는 SIGKILL/SIGSTOP 제외한 모든 시그널 설정 가능) act가 NULL이 아닌 경우 시스템 콜은 해당 시그널의 현재 동작 방식을 act가 지정한 내용으로 변경 oldact가 NULL이 아닌 경우 해당 호출은 이전의 동작방식을 oldact에 저장 sigaction 구조체는 시그널을 세세히게 제어 가능 sa_handler 필드는 해당 시그널을 받았을 때 수행할 동작을 지정, sighandler_t와 동일 sa_flag에 SA_SIGINFO를 설정하면 sa_handler가 아니라 sa_sigaction이 시그널을 처리하는 함수를 명시(형식은 다름, 책을 보자) sa_mask 필드는 시그널 핸들러를 실행하는 동안 시스템이 블록해야할 시그널 모음을 제공(SA_NODEFER 플래그 미설정 시 현재 처리중인 시그널도 블록됨) sa_flags 필드는 플래그에 대한 비트 마스크, signo로 지정한 시그널의 처리를 변경(SA_NOCLDSTOP 등 책에 리스트/설명 있으므로 확인 할 것) sigaction() 호출이 성공하면 0을 반환, 실패 시 -1을 반환 및 errno 설정(EFAULT, EINVAL) siginfo_t 구조체 sa_sighandler 대신 sa_sigaction을 이용하는 경우 siginfo_t 구조체는 시그널에서 훨씬 많은 기능 및 정보(시그널 원인 등)를 제공 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 typedef struct siginfo_t{ int si_signo; /* 시그널 번호 */ int si_errno; /* errno 값 */ int si_code; /* 시그널 코드 */ pid_t si_pid; /* 보내는 프로세스의 pid */ uid_t si_uid; /* 보내는 프로세스의 실제 uid */ int si_status; /* 종료 값이나 시그널 */ clock_t si_utime; /* 소비된 사용자 시간 */ clock_t si_stime; /* 소비된 시스템 시간 */ sigval_t si_value; /* 시그널 페이로드 값 */ int si_int; /* POSIX.1b 시그널 */ void* si_ptr; /* POSIX.1b 시그널 */ void* si_addr; /* 장애가 발생한 메모리 위치 */ int si_band; /* 대역(band) 이벤트 */ int si_fd; /* 파일 디스크립터 */ } 상세한 설명은 책에 있음 POSIX는 처음 세 필드많이 모든 시그널이 유효하다고 보증, 다른 필드는 적절한 시그널을 다룰 때만 접근할 것 si_code의 멋진 세계 si_code 필드는 시그널을 일으킨 원인을 알려 줌(사용자가 보낸 시그널의 경우 시그널을 어떻게 보냈는지, 커널이 시그널을 보낼 경우 왜 시그널을 보냈는지 확인 가능) 상세 내용은 책에 있음 si_code는 값을 담고 있는 필드이며 비트 필드가 아님 페이로드와 함께 시그널 보내기 SA_SIGINFO 플래그와 함께 등록된 시그널 핸들러는 siginfo_t 인자를 전달하고 이의 si_value 필드를 통해 페이로드를 전달할 수 있다. 1 2 3 4 5 6 7 8 #include \u0026lt;signal.h\u0026gt; union sigval { int sival_int; void* sival_ptr; } int sigqueue (pid_t pid, int signo, const union sigval value); sigqueue()sms kill()과 유사하게 호출이 성공하면 signo 시그널은 pid 프로세스나 프로세스 그룹 큐에 들어가고 0을 반환 호출이 실패하면 -1을 반환 및 errno를 설정(EAGAIN, EINVAL, EPERM, ESRCH) kill()처럼 권한을 가지고 있는지 검사하기 위해 signo로 NULL 시그널을 전달 할 수 있다. 시그널 페이로드 예제 책에 예제 있음 sigqueue() 시그널을 보내면 받는 프로세스에서 sa_sigaction 핸들러(SA_SIGINFO 용)으로 처리 시 siginfo_t의 si_int 또는 si_ptr 필드로 페이로드를 받고 si_code가 SI_QUEUE임을 확인한ㄷ. 시그널은 미운오리 새끼? 시그널은 유닉스 프로그래머 사이에서 환영받지 못함(커널과 사용자 간 통신을 위한 구식 메커니즘, 멀티스레딩과 이벤트루프 세계에서 적절하지 않음) 시그널은 커널에서 수많은 통지를 수신할 유일한 방법이며, 프로세스를 종료하고 부모/자식 프로세스 관계를 관리하는 방법이므로 이해하고 사용해야 함 시그널이 평가절하되는 원인 중 하나는 재진입성에 대한 우려가 없는 시그널 핸들러 작성이 쉽지 않음 -\u0026gt; 시그널 핸들러를 간결하게, 재진입성이 보장된 함수만 사용 사용할 거면 sigaction()과 sigqueue를 사용하자 ","date":"2023-11-12T00:00:00Z","permalink":"https://muonkmu.github.io/p/linux_system_programming-10-%EC%8B%9C%EA%B7%B8%EB%84%90/","title":"[Linux_System_programming] 10 시그널"},{"content":"Vitis-AI 양자화, 컴파일러 실습 사용자 DPU 하드웨어 플랫폼 실습 사용자 소프트웨어 플랫폼 실습 ","date":"2023-10-19T00:00:00Z","permalink":"https://muonkmu.github.io/p/vitisai-02-practice/","title":"[VitisAI] 02 Practice"},{"content":"Introduction Vitis-AI는 xilinx에서 FPGA기반 적응형 컴퓨팅 유닛을 개발하기 위한 개발 플랫폼 Vitis-AI git 사이트에서 모든 개발환경 다운 가능(모델, 컴파일러 등) 전용 DPU(deep-leanring processing unit)을 사용하여 구성 모델을 구성(DPU는 FPGA내 LUT, DSP 등을 이용) 지원하는 플랫폼 (플랫폼에서 bere-metal은 지원하지 않는다.) Edge : zynq, zynq ultrascale+, versal, versal Edge cloud : Alveo, VCK5000 Docker 기반 Vitis-AI 개발환경은 학습기능, 검증기능, 최적화/양자화, 인공지능 배포 기능을 제공 개발 전에 개발 환경 버전을 맞추는 것은 필수(vivado 2020.2 - vitis AI 2.0 - nVidia 2xxx 그래픽 카드) 개발 환경 버전 정보는 각 모델의 Readme에서 확인하자 Vitis-AI 개발 순서 모델 개발 Vitis-AI Git에서 제공 모델 확인(ML layer 구성 중 지원하는 명령어 셋 확인) 소수점으로 구성된 모델을 DPU에서 사용가능한 정수형으로 스케일링(양자화) 양자화된 모델을 DPU에서 구동하기 위한 포맷으로 컴파일(opcode/스케줄링 데이터 생성) 모델 최적화도 지원(프루닝 등) : 유료임 하드웨어 빌드 Vitis-AI에서 입력 가능한 하드웨어 가속 플랫폼 설계(클럭, AXI 버스 설정) 사용자의 SOC에 맞게 DPU 설정 및 Vitis V++을 통한 컴파일 비트 스트림 및 DPU arch 파일 생성 구동 SW 개발 리눅스 OS에서 DPU 드라이버를 통한 구동 SW 개발 Vitis-AI에서 어플리케이션을 작성을 위한 C++, python API 및 예제 제공 Vitis AI Framework / Model Vitis AI는 Caffe, Pytorch, TensorFlow 인공지능 프레임워크를 지원 (교재에 지원 버전 있음) Vitis-AI Model Zoo에서 다양한 모델을 지원(교재에 지원 모델 리스트 있음, github에서 확인 가능) Vitis-AI 모델 이름 형식 : F_M_(D)H_W(P)_C_V F: 프레임워크 종류, M: 인공지능 모델 종료, D: 학습에 사용된 데이터 세트 H,W: 입력 데이터 높이, 넓이, P: 프루닝 비율, C: 모델 초당연산량, V: Vitis-AI지원 버전 각 프레임 워크별 모델 디렉터리 구조 교재에 있음 Vitis-AI Quantizer Vitis-AI Quantizer는 Post Training Quantization(PTQ), Quantization Aware Training(QAT) 지원 PTQ : FP32 모델로 학습을 완료한 후 최종 Weight, Activation 값을 양자화 QAT : FP32 모델로 기준 학습 그래프 생성 후 추론 계산에 Fake Quantizaion을 추가, 목표에 도달할 때 까지 Fake 양자화 계수를 조절 모델 프레임 워크에 따라 지원 범위가 다름(교재 확인, caffe 2.5 이상 미지원) 교재에 각 모델 프레임 워크별 양자화 방법 나와 있음 Pytorch Quantization 필요한 파일 model.pth : 사전 학습된 pytorch 모델 model.py : float 모델 정의를 포함하고 있는 파이썬 스크립트 calibration dataset : 학습에 사용한 데이터 또는 별도 검증 셋 양자화 과정 (교재에 자세한 설명) 사전 훈련된 float 모델 준비(forward 방식만 지원, 다른 함수는 CPU이용 처리) vai_q_pytorch API를 잉용하여 float 모델을 양자화 양자화된 모델과 float 모델에 데이터를 입력, 출력을 비교하여 양자화 모델의 보정 실행 보정이 완료된 후 평가 데이터를 양자화된 모델에 다시 입력하여 정밀도/성능 평가 양자화 전 Inspector를 이용하여 어떤 operator가 어떤 장치(DPU/CPU)에서 실행될지 나타내는 파티션 정보를 출력할 수 있다. Vitis-AI Compiler vitis-AI 컴파일러는 DPU에서 신경망을 동작시키기 위해 데이터 순서를 스케줄링하고 dataflow를 적용한 바이너리를 생성 vitis-AI 컴파일러는 Xilinx Intermediate Representation(XIR) 기반, DPU 아키텍쳐에 맞게 모델 생성 입력된 프레임 워크별 모델에서 공통으로 사용 가능한 통합된 XIR 형식 변환 XIR형식은 sub graph로 분할, 실행할 명령어 스트림 생성 및 스트림 연결 sub graph 통합하고 컴파일 하여 VART(Vitis AI Runtime)에서 사용할 수 있는 직렬화된 xmodel 생성 컴파일 시 arch.json(DPU 아키텍쳐 정보) 필요 (vivado에서 생성 가능) 모델 프레임 워크별 별도 명령 사용(pyton: vai_c_xir, 사용법은 교재 참조) DPU(Deep Learning Processor Unit) DPU는 xilinx 칩별 지원되는 종류가 다름(zynqMpSoc: DPUCZDX8G, 교재에 DPU버전 정보 및 플랫폼 별 지원 DPU리스트 참조) DPU를 이용한 Development Flow vitis 또는 vivado에서 DPU 하드웨어 플랫폼 생성 생성된 HW 정보파일(XSA)를 이용 Petalinux Project 생성 DPU 디바이스 트리 작성, 개발하고자 하는 신경망 모델 작성 및 검증 신경망 양자회 및 컴파일 신경망 모델용 어플리케이션 개발 DPU Hardware Architecture DPU는 다차원 병렬 처리 지원, 사용자에 의한 설정 및 확장 가능 DPU는 다음의 항목으로 구성 (교재에 그림 있음, 교재에 DPU별 상세 아키텍쳐 있음) On chip buffer control(AXI Master, AXI Lite) : PS DDR 메모리에서 연산하고자 하는 입력 데이터를 BRAM에 이동, 결과를 DDR로 이동 Instruction Scheduler : 병렬 처리를 위한 분할 및 결과 병합 처리 스케줄러, DPU에서 불가능한 연산은 CPU에서 처리하도록 Computing Engine : PE(컨볼루션)와 misc engine(pooling, batch normalize) DPU 설정 파라메터(DPUCZDX8G) (Vivado/vitis에서 DPU에 대한 설정 파라메터를 기술)\nDPU 코어 갯수 : 1~4 선택가능 (각 DPU는 기본적으로 M_AXI 두개 포트가 필요, 포트와 리소스 수에 맞추어 구성) DPU 아키텍쳐 : B512~B4096선택 (뒷 숫자가 성능, 채널수 * 입력 채널 * 출력 채널이 숫자를 나타냄, 교재에 설명 참조) RAM Usage : PL의 BRAM/URAM에 weight,bias,intermediate feature map이 저장되며 이 양을 선택(low, high 선택가능) channel Augmentation : 처리 가능한 채널 수보다 입력 채널의 수가 적을 때 입력채널을 복사하여 인식률을 높임 Save Argmax : 수행된 연산 결과의 최대값 위치 저장 여부를 결정 ALU Parallel : 컨볼루션 아키텍쳐의 PP의 절반값으로 ALU parallel 값으로 선택(권장 사항) ReLU Type : 활성함수 종류 선택(LeakyReLU 선택 여부) softmax : Softmax의 하드웨어 구현 여부 옵션(1023클래스 까지 지원하며 enable시 SFM_M_AXI와 인터럽트 가 추가됨) 그 외 DPU advance config옵션(S-AXI clock mode, DSP48 usage,URAM use등이 있으며 교재 참조) DPU AXI PORT S_AXI : DPU register RW M_AXI : DATA(2ch) DDR에 있는 Xmodel 파라메터 및 데이터 fetch, INSTR은 명령어 Fetch, SFM(옵션)은 softmax 출력 DATA 채널은 별도의 포트로 구성하고 INSTR은 다른 채널과 interconnect를 통해 연결 가능(사용량에 따라 결정 됨) 교재에 각 포트에 대한 자세한 설명 있음, 레지스터 설명도 있음 Hardware Platform DPU 사용 클럭은 하기 3가지 이며 이 중 Data control clock과 computing clock은 동기 클럭이여야 함 register clock : DPU IP 레지스터 설정을 위한 클럭, 높은 클럭이 필요없음(권장 100Mhz) Data controller clock : DPU M_AXI가 DDR데이터 RW시 사용 300Mhz이상 권장 Computation clock : DPU내 DSP에서 사용되는 클럭(곱셈, 누산연산), M_AXI의 2배를 입력 PL을 많이 사용할 수록 클럭을 낮추어 Timing closure를 맞출 것 교재에 Clock Wizard로 클럭을 생성하는 법, 각 포트의 연결법이 있으니 참조 Software Platform 교재에 Petalinux로 Vitis-AI를 포함한 SW 플랫폼 만드는 방법을 기술 ","date":"2023-10-18T00:00:00Z","permalink":"https://muonkmu.github.io/p/vitisai-01-theory/","title":"[VitisAI] 01 Theory"},{"content":"프로세스 주소 공간 리눅스는 메모리를 가상화(물리주소에 직접 접근 x)\n페이지와 페이징 페이지 : MMU에서 관리할 수 있는 최소 단위(가상 주소 공간은 페이지로 구성, 32bit 시스템 4K, 64bit 시스템 8K) 프로세스는 모든 물리 페이이제 접근하지 못하며 어떤 페이지는 유효하지 않을 수 있다. 유효한 페이지 : RAM / swap파티션 / 디스크의 실제 페이지와 연관됨 유효하지 않은 페이지 : 접근 시 세그멘테이션 폴트 유효한 페이지가 2차 저장소(swap, 디스크)에 연결 시 접근하려고 하면 페이지 폴트 발생 페이징 아웃 : 새로운 데이터 페이징을 위한 데이터 공간을 만들기 위해 RAM페이지 내용을 2차 저장소로 이동 공유와 copy-on-wirte 가상 메모리의 페이지는 다른 프로세스의 가상 주소 공간에 존재할 지라도 하나의 물리페이지로 맵핑 될 수 있음 상기 방식으로 물리 메모리에 있는 데이터를 다른 가상 주소 공간에서 공유(ex. 공유 C 라이브러리, 데이터 베이스) 프로세스가 쓰기가 가능한 공유페이지에 데이터를 쓰면 2가중 한가지 사건 발생 커널이 쓰기를 허용해 그 페이지를 공유하고 이쓴ㄴ 모든 프로세스가 쓰기 결과가 반영된 페이지 공유(여러 프로세스가 쓰려고 하면 조정/동기화 필요) MMU가 쓰기 요청을 가로채서 예외를 던짐, 커널은 쓰기 요청한 프로세스를 위해 그 페이지의 복사본을 만들고 쓰기 요청 진행(COW) 메모리 영역 커널은 접근 권한과 같은 특정 속성을 공유하는 블록 내부에 페이지를 배열, 이를 맵핑, 메모리영역이라고 함 메모리 영역 종류 : 텍스트 세그먼트(코드), 스택영역(스택), 힙(동적할당), bss(초기화 되지 않은 전역변수) 리눅스는 두 단계로 변수를 최적화 초기화 하지 않은 데이터는 전용 공간인 bss 세그먼트에 할당(링커는 오브젝트 파일에 특수 값을 저장하지 않음) BSS세그먼트가 메모리에 적재되면 커널은 단순히 이 세그먼트를 COW 기법을 통에 0으로 채워진 페이지에 맵핑 하여 기본 값을 설정 동적 메모리 할당하기 메모리 관리 시스템의 기본은 동적 메모리의 할당과 사용, 해제를 어떻게 하느냐 책에 malloc()에 대한 설명 있음 1 2 3 #include \u0026lt;stdlib.h\u0026gt; void* malloc(size_t size); 책에서 void포인터를 반환하는 함수에 대한 타입 캐스팅에 대한 위험성을 언급(책을 읽어보자) malloc()이 NULL을 반환할 경우 에러메세지를 출력하고 프로그램을 종료하도록 하는 사용자 래퍼함수 xmalloc()에 대해 소개 있음 배열 할당하기 1 2 3 #include \u0026lt;stdlib.h\u0026gt; void* calloc(size_t nr, size_t size) 동적메모리 할당에서 할당하려는 크기가 유동적일 경우 calloc()사용 가능(ex. 고정 item에 대한 가변 길이 배열 할당) calloc()이 성공 시 크기가 size바이트인 원소 nr개 크기의 메모리 블록에 대한 포인터 반환 calloc()은 메모리 영역을 모두 0으로 채움(memset 사용보다 빠른데 커널이 이미 0으로 채워진 메모리를 제공하기 때문) 호출 실패 시 NULL 반환 및 errno를 ENOMEM으로 변경 할당 크기 변경 1 2 3 #include \u0026lt;stdlib.h\u0026gt; void* realloc(void* ptr, size_t size); realloc() 호출이 성공시 ptr이 가리키는 메모리 영역을 size 바이트 크기로 새로 조정 후 반환(반환 값은 ptr이 아닐 수 있음) 메모리 영역을 키울 때 원래 위치에서 기존 메모리 영역을 확장 할 수 없다면 새로운 메모리를 할당하고 이전 내용을 복사 후 이전 영역을 해제(상대적으로 비용이 많이듬) size가 0이면 free()와 같고 ptr이 NULL이면 malloc()과 같다 호출 실패 시 NULL 반환 및 errno를 ENOMEM으로 변경 동적 메모리 해제 스택을 거슬러 올라오면서 자동으로 거둬들이는 자동할당과 달리 동적 할당은 수동으로 해제될 때 까지 프로세스 주소공간에 존재 1 2 3 #include \u0026lt;stdlib.h\u0026gt; void free(void *ptr); free()를 호출하면 ptr이 가리키는 메모리 해제(ptr은 반드시 malloc(), calloc(), realloc()에서 반환된 값일 것) 할당된 메모리 블록의 일부만 해제는 불가능 메모리 누수나 댕글링 포인터 접근 주의 정렬 데이터 정렬은 메모리에 데이터를 나열하는 방식(프로세서, 메모리 서브시스템 및 구성요소들은 특정 바이트로 정렬되길 요구) 정렬에 대한 규측은 하드웨어에서 유래하므로 시스템마다 다름(엄격한것, 널널한것) 정렬된 메모리 할당하기 POSIX는 malloc(), calloc(), realloc()에서 반환된 메모리가 어떤 C타입을 사용하든 적절하게 정렬되어야 함 리눅스는 32비트 시스템에서 8byte, 64bit 시스템은 16byte 직접 블록 입출력이나 소프트웨어 하드웨어 간 통신을 위한 버퍼를 적절히 정렬해가 위해 페이지 같은 데 큰단위 정렬된 동적메모리 요구 1 2 3 4 5 6 7 /* 둘 중 아무거나 하나만 정의해도 상관없다 */ #define _XOPEN_SOURCE 600 #define _GNU_SOURCE #include \u0026lt;stdlib.h\u0026gt; int posix_memalign (void **memptr, size_t alignment, size_t size); posix_memalign() 호출이 성공하면 동적 메모리를 size만큼 할당하고 alignment의 배수인 메모리 주세오 맞춰 정렬 alignment는 2의 거듭제곱이며 void포인터 크기의 배수 일 것, 할당된 메모리 주소는 memptr에 저장 성공 시 0반환, 실패시 EINVAL/ENOMEM 중 반환(errno사용 안함) 할당된 메모리는 free()를 이용해 해제해야 함 posix_memalign()정의 전에는 BSD에는 valloc(), memalign()함수를 이용 다른 정렬 고려 사항 비표준 데이터 타입과 복잡한 데이터 타입의 정렬의 네가지 유용한 규칙 구조체의 정렬 요구사항은 가장 큰 멤베의 타입을 따름 구조체는 각 멤베가 그 타입의 요구사항에 맞게 적절히 정렬 될 수 있도록 패딩 필요(gcc 옵션 -Wpadded 사용 시 컴파일러가 패딩을 채워 넣을 때마다 경고를 띄움) 유니언의 정렬 요구사항은 유니언에 속한 가장 큰 타입을 따른다 배열의 정렬 요구사항은 기본 타입을 따른다(배열에는 단일 타입을 넘어서는 요구사항이 없다) C++ 엄격한 앨리어싱 : 객체가 실제 그 객체의 타입, 타입 한정자, 부호, 구조체, 유니언 맴버 또는 char포인터를 통해서만 접근해야함 데이터 세그먼트 관리하기 유닉스 시스템은 전통적으로 데이터 세그먼트를 직접 관리할 수 있는 인터페이스를 제공(malloc() 함수가 사용하기 쉬워서 잘 안씀) 자신만의 힙 기반 할당 메커니즘을 구현하고자 하는 사람들은 아래를 이용 1 2 3 4 #include \u0026lt;unistd.h\u0026gt; int brk(void* end); void* sbrk(intptr_t increment); 이름의 유래 : 메모리에서 힙과 스택을 나누는 경계선을 break 또는 break point라고 한다 brk()를 호출하면 데이터 세그먼트의 마지막 브레이크 포인터를 end로 지정한 주소로 설정 sbrk()를 호출하면 데이터 세그먼트를 increment만큼 늘린다(음수, 양수 가능) 익명 메모리 메핑 glib은 버디 메모리 할당 기법 이둉(메모리 할당은 데이터 세그먼트와 메모리 매핑 이용) malloc()을 구현하는 전통적인 방법은 데이터 세그먼트를 2의 배수만큼의 구역으로 나눈 다음 요청하는 크기에 가장 가까운 크기의 구역 반환 인접 구역이 비어 있을 경우 하나로 합쳐 더 큰 구역을 확보하거나 힙의 윗 부분이 완전히 비어있다면 brk()를 이용해 브레이크 포인트를 낮춰 힙을 줄임 버디 메모리할당 기법은 속도와 명료함이 장점이지만 2종류의 메모리 파편화가 발생 내부 파편화 : 메모리 할당 요청을 충족시키기 위해 요청보다 더 많은 메모리를 반환 할 때 외부 파편화 : 요청을 만족 시킬 메모리가 남아 있지만 인접하지 않은 두 구역으로 떨어져 있을 때 상기 이유로 대규모 메모리 할당에는 glib은 힙을 사용하지 않고 익명 메모리 매핑사용(파일 기반 매핑과 유사하나 파일과 연관되지는 않음) 익명 메모리 맵핑을 단일 할당만을 위한 새로운 힙(힙 영역 밖에 위치) 장점 : 파편화 X, 크기조정/권한설정/힌트 사용 가능, 힙관리 필요 없음 단점 : 크기는 페이지 크기의 정수배로 메모리 낭비 될 수 있음, 매핑을 생성하는 부하가 힙보다 크다 익명 메모리 맵핑 생성하기 익명 메모리 메핑을 사용하거나 독자적인 메모리 할당 시스템을 작성 시 수동으로 익명 메모리 매핑 생성 mmap()으로 메모리 매핑 생성, munmap()으로 매핑 해제 MAP_ANONYMOUS 플래그로 익명으로 매핑 생성, MAP_PRIVATE 플래그로 공유되지 않도록 커널이 COW로 애플리케이션의 익명 페이지를 0으로 채운 페이지로 매핑하므로 0을 채울 필요가 없다. /dev/zero 매핑하기 BSD 간은 다른 유닉스 시스템에는 MAP_ANOYMOUS 플래그가 없어 /dev/zero파일을 매핑하는 방식 사용 고급 메모리 할당 1 2 3 #include \u0026lt;malloc.h\u0026gt; int mallopt(int param, int value); mallopt()를 다른 메모리 할당 인터페이스를 사용 전 호출하여 할당 연산을 인자로 제약 mallopt()를 호출하면 param으로 지정된 메모리 관리와 관련된 인자를 value로 설정 호출이 성공하면 0이 아닌 값을 반환, 실패시 0을 반환(errno는 설정하지 않음) param의 목록은 다음과 같다 M_CHECK_ACTION : MALLOC_CHECK_ 환경변수 값 M_MMAP_MAX : 최대 매핑 계수 설정 M_MMAP_THRESHOLD : 힙 대신 익명 맵핑으로 처리할 할당요청의 임계 크기 M_MXFAST : 패스트 빈의 최대 크기(힙 내 특수 메모리 영역) M_PERTURB : 메모리 포이즈닝 활성화(미리정의한 값으로 채워서 관리 에러 탐지) M_TOP_PAD : 데이터 세그먼트 크기 조정 시 사용되는 패딩의 크기 M_TRIM_THRESHOLD : glib이 sbrk()를 호출해서 메모리를 커널에 반환하기 전 데이터 세그먼트의 빈 메모리의 최소 크기 malloc_usable_size()와 malloc_tirm()으로 튜닝하기 malloc_usable_size()는 메모리 영역의 실제 할당 크기 반환(요청보다 할당 영역이 클 수 있음) malloc_tirm()은 유지되어야 하는 패딩 바이트를 제외한 가능한 많은 데이터 세그먼트를 줄이고 1반환 두 함수를 쓸일은 적다(이식성도 떨어짐) 메모리 할당 디버깅 프로그램은 MALLOC_CHECK_ 환경 변수를 설정해서 메모리 서브시스템의 고급 디버깅 기능을 활성 가능 환경 변수로 디버깅을 제어하므로 재 컴파일은 필요 없이 실행만 하면 됨 환경 변수를 1로 설정 시 유익한 정보를 담은 메세지가 stderr로 출력, 2로 설정하면 프로그램은 즉시 abort()를 호출해서 종료 1 $ MALLOC_CHECK_=1 ./rudder 통계 수집하기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #include \u0026lt;malloc.h\u0026gt; struct mallinfo{ int arena; /* malloc이 사용하는 데이터 세그먼트 크기 */ int ordblks; /* 비어 있는 메모리 블록 수 */ int smblks; /* 패스트 빈 수 */ int hblks; /* 익명 매핑 수 */ int hblkhd; /* 익명 매핑 크기 */ int usmblks; /* 전체 할당 최대 크기 */ int fssmblks; /* 사용 가능한 패스트 빈의 크기 */ int uordblks; /* 전체 할당 공간의 크기 */ int fordblks; /* 사용 가능한 메모리 블록의 크기 */ int keepcost; /* 정리가 가능한 공간 크기 */ } struct mallinfo mallinfo (void); mallinfo()를 호출하면 mallinfo 구조체에 통계를 담하 변환(포인터가 아니라 값으로 반환) 메모리 관련 통계를 stderr로 출력하는 malloc_stats() 함수도 제공 스택기반 할당 1 2 3 #include \u0026lt;alloca.h\u0026gt; void* alloca(size_t size); alloca()은 호출이 성공시 size 바이트 만큼 할당한 메모리에 대한 포인터 반환 상기 메모리는 스택에 위치하며 실행중인 함수가 반환 될때 자동으로 해제 일부 구현에서는 호출이 실패할 경우 NULL반환, 대부분 실패하지 않거나 실패를 알리지 못함(실패는 스택 오버플로를 일으킴) alloca()는 버그가 많고 일관성 없는 구현으로 나쁜 평판을 얻어서 호환성을 위해서는 사용하지 않는 것이 좋으나 malloc() 보다 월등히 성능이 좋다. 작은 메모리 할당이 필요하고 성능향상이 필요할 경수 사용 스택에서 문자열 복사하기 문자열을 임시로 복사하는 경우 alloca()을 매우 자주 사용(책에 예제 있음) 리눅스 시스템은 스택에 문자열을 복사하는 stddup()함수군을 제공, 그러나 POSIX는 상기 함수를 정의하지 않으므로 이식성이 필요한 경우 주의 가변 길이 배열 C99는 컴파일 시점이 아니라 실행중에 배열의 크기를 결정하는 가변 길이 배열(VLA)를 도임(GNU C는 이미 지원하고 있었음) 가변 길이 배열은 alloca()와 마찬가지로 동적메모리 할당이라는 부하를 없애 줌 alloca()로 얻은 메모리는 함수 주기동안 유지, 가변 길이 배열로 얻은 메모리는 변수가 스코프를 벗어 날때 까지만 유지 1 2 3 4 for(i = 0 ; i \u0026lt; n; ++i) { char foo[i+1]; /* foo 사용 */ } 메모리 할당 메커니즘 선택하기 메모리 조작하기 C언어는 메모리 바이트 조작 함수 제공, 사용자가 제공한 버퍼 크기에 의존하며 에러 반환을 안함(잘못된 영역을 넘기면 세그멘테에션 폴트 발생) 바이트 설정하기 1 2 3 #include \u0026lt;string.h\u0026gt; void* memset(void* s, int c, size_t n); memset()을 호출하면 s에서 시작해서 n 바이트만큼 c로 채운 다음 s 반환 0으로 채울 때 가장 많이 쓰이며 BSD에서 bzero()도 제공하였으나 이제는 안씀 바이트 비교하기 1 2 3 #include \u0026lt;string.h\u0026gt; void* memcmp(const void* s1, const void* s2, size_t n); memcmp()는 s1과 s2의 처음 n 파이트를 비교하고 두영역이 같으면 0, s1\u0026gt;s2 이면 음수, s2\u0026gt;s1이면 양수 BSD에서도 bcmp()함수를 제공하나 현재 사용안함 구조체는 패딩 때문에 memcmp로 비교하는 것은 신뢰할 수 없음(각 멤버끼리 비교) 바이트 옮기기 1 2 3 4 5 6 #include \u0026lt;string.h\u0026gt; void* memmove(void *dst, const void* src, size_t n); void* memcpy(void* dst, const void* src, size_t n); void* memccpy(void* dst, const void* src, int c, size_t n); void* mempcpy(void* dst, const void* src, size_t n); memmove()는 src의 처음 n 바이트를 dst로 복사하고 dst를 반환(BSD에서는 bcopy() 함수를 제공하나 사용하지 않음) memmove()함수는 중첩되는 메모리 영역(dst의 일부가 src안에 존재)을 안전하게 다룸(예를 들면 메모리 바이트를 특정 영역안에서 위나 아래로 이동 가능) 중첩된 메모리 영역을 지원하지 않는 memcpy() 제공(잠재적으로 좀 더 빠르게 동작) memccpy()함수는 src 첫 n 바이트 내에서 c 바이트를 발견하면 복사를 멈춤, dst에서 c의 다음 바이트를 가리키는 포인터 반환, 찾지 못하면 NULL 반환 mempcpy()는 마지막 바이트를 복사한 후 다음 바이트를 가리키는 포인터를 반환 바이트 탐색하기 1 2 3 4 5 6 7 8 #include \u0026lt;string.h\u0026gt; void* memchr(const void* s, int c, size_t n); #define _GNU_SOURCE void* memrchr(const void* s, int c, size_t n); void* memmem(const void* haystack, size_t haystacklen, const void* needle, size_t needlelen); memchr()는 s가 가리키는 메모리의 n바이트 범위에서 unsigned char 문자 c를 탐색, c와 일치하는 첫 바이트를 가리키는 포인터를 반환, 찾지 못한 경우 NULL 반환 memrchr()는 시작지점부터 찾는게 아닌 s가 가리키는 메모리의 뒤에서 부터 n바이트 범위 탐색(GNU확장, C언어의 일부 아님) memmem()는 길이가 haystacklen 바이트인 메모리 블록 haystack 내부에서 길이가 needlelen 바이트인 서브 블록 needle의 첫 번째 위치를 가리키는 포인터를 반환, 찾지못하면 NULL 반환 바이트 섞기 1 2 3 4 #define _GNU_SOURCE #include \u0026lt;string.h\u0026gt; void* memfrob(void *s, size_t n); memfrob()는 s에서 시작하는 메모리의 처음 n바이트를 숫자 42와 XOR 연산 후 s를 반환 반환된 메모리를 memfrob()에 다시 넘기면 아무것도 안한 것과 같으므로 암호화에 사용해서는 안됨(알아보기 어렵게 하는 용도) 메모리 락 걸기 리눅스는 필요할 때 디스크에서 페이지를 읽어오거나 필요없을 때 디스크로 되돌리는 요청식 페이지 구현(메모리 가상화) 보통은 유저가 커널의 페이징에 대해 신경쓰지 않아도 되나 어플리케이션이 페이징 동작에 영향을 미치기를 원하는 동작 2가지 결정성 : 페이지 폴트는 디스크 입출력 연산을 발생시켜 결정성 보장 못하게 함, 필요 페이지가 항상 물리메모리에 머물게해 일관성/결정성 보장 보안 : 중요 내용을 메모리에 저장했는데 이런 내용이 암호화 되지 않은 디스크로 페이징 될 수 있음(메모리의 암호화 되지 않은 개인 키가 스왑파일로 저장) 일부 주소 공간 락 걸기 1 2 3 #include \u0026lt;sys/mman.h\u0026gt; int mlock(const void *addr, size_t len); mlock()은 호출이 성공 시 메모리에서 [addr, addr+len) 영역을 포함하는 모든 물리 페이지를 잠금 호출이 성공하면 0을 반환, 실패 시 -1 반환 및 errno를 설정(EINVAL, ENOMEM, EPERM) POSIX 표준은 addr이 반드시 페이지 크기로 정렬되어야 한다고 정의, 리눅스는 강제하지 않으나 필요하다면 addr을 가장 근접한 페이지로 자름 전체 주소 공간 락 걸기 1 2 3 #include \u0026lt;sys/mman.h\u0026gt; int mlockall (int flags); mlockall()을 호출하면 현재 프로세스 주소 공간에 있는 모든 페이지를 물리 메모리에 잠금 flag는 다음 값을 OR 연산(대부분 두값을 OR연산 해서 대입) MCL_CURRENT : 현재 맵핑된 페이지(스택, 데이터 세그먼트, 맵핑된 파일 등)를 프로세스 주소 공간에 잠금 MCL_FUTURE : 향후 주소공간에 맵핑되는 모든 페이지 역시 메모리에 잠금 호출이 성공하면 0을 반환, 실패하면 -1 반환 및 errno를 설정(EINVAL, ENOMEM, EPERM) 메모리 락 해제하기 1 2 3 4 #include \u0026lt;sys/mman.h\u0026gt; int mulock(const void* addr, size_t len); int mulockall(void); mlock()은 addr에서 시작해서 len 바이트만큼 확장된 페이지의 락을 해제(mlock에 대응) 두 함수 모두 호출이 성공하면 0을 반환, 실패 시 -1 반환 및 errno를 설정(EINVAL, ENOMEM, EPERM) 락 제약 리눅스는 프로세스에서 얼마나 많은 페이지를 락 걸수 있는지 제한을 둠(메모리 락은 시스템 전반적인 성능에 영향) CAP_IPC_LOCK 기능을 가진 프로세스는 페이지 수에 제약 없이 락 걸 수 있음 상기 기능이 없는 프로세스는 RLIMIT_MEMLOCK 바이트만 락 걸 수 있음(기본은 32KB, 6장 내용 참조) 페이지가 물리 메모리에 존재하는지 확인하기 1 2 3 4 #include \u0026lt;uinstd.h\u0026gt; #include \u0026lt;sys/mman.h\u0026gt; int mincore (void* start, size_t length, unsigned char *vec); mincore()은 시스템 콜 호출하는 시점에 물리 메모리에 맵핑된 페이지를 기술하는 벡터 제공(디버깅과 진단목적 사용) 호출이 성공하면 0 반환, 실패 시 -1 반환 및 errno 설정(EAGAIN, EFAULT, EINVAL, ENOMEM) 이 시스템 콜은 MAP_SHARED로 생성한 파일 기반 매핑에 대해서만 제대로 동작(사용시 큰 걸림돌) 게으른 할당 리눅스는 게으른 할당 전략 사용 프로세스가 커널에 추가 메모리 요청(데이터 세그먼트 늘림, 메모리 매핑) 시 메모리 할당 약속(이 때는 저장장치 제공이 아닌 약속만) 프로세스가 새로 할당된 메모리에 쓸 때 커널은 사용자에게 제공한 메모리를 물리 메모리 할당으로 변환(페이지 단위로 수행) 다음 3가지 장점 메모리 할당을 지연 시켜 실제 할당이 필요한 최후의 순간까지 대부분의 작업을 미룰 수 있음 요구에 따라 페이지 단위로 요청을 처리하므로 실제 사용하는 물리 메모리만 물리 저장소 소비 할당을 약속한 메모리의 총량이 물리 메모리/스왑 공간 까지 넘어 설 수 있음(이를 오버 커밋이라 함) 오버커밋과 OOM 오버커밋은 더 크고 많은 어플리케이션을 실행할 수 있도록 함 오버커밋으로 인해 요청한 메모리를 충족 시킬 수 있는 메모리가 부족한 경우 OOM(out of memory) 발생 OOM이 발생하면 커널은 OOM 킬러를 실행 시켜 가장 덜 중요한 프로세스를 종료 시킴 OOM 조건은 드물게 발생하므로 초기에 오버커밋을 허용하면 활용도가 극대화 되며 OOM에 의한 프로세스 종료는 용납되지 않는 경우가 있음 /proc/sys/vm/overcommit_memry 파일 또는 sysctl 파라미터 vm.overcommit_memory를 통해 오버커밋 비활성화 가능 0(defualt) : 합당한 오버커밋 허용 1 : 모든 오버커밋 허용 2 : 오버커밋 비활성화(책에 상세 내용 있음) ","date":"2023-10-15T00:00:00Z","permalink":"https://muonkmu.github.io/p/linux_system_programming-09-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B4%80%EB%A6%AC/","title":"[Linux_System_programming] 09 메모리 관리"},{"content":" 커널은 다음 세가지 방법으로 경과 시간을 측정\n실제 시간 : 현실에서 사용하는 진짜 시간과 날짜(wall_time), 사용자 상호작용/이벤티 타임스탬프에 사용 프로세스 시간 : 프로세스를 실행하는데 소비한 시간, 프로세스는 프로파일링과 통계 산출 목적으로 특정 알고리즘을 처리하는데 걸린 시간을 측정 모노토닉 시간 : 반드시 일정하게 증가, 운영체제는 시스템 가동시간(uptime 사용), 두 샘플링 사이의 시간 차이를 계산할 때 유용 시간 측정 방법은 다음 두 형식중 하나로 표현\n상대시간 : 현재 시각과 같이 특정 기준점에 대한 상대값으로 표현, 모노토닉 시간은 상대 시간을 계산하는데 유용 절대시간 : 특정 기준점 없이 절대값으로 표현(2000년 3월 25일 정오같은 표현, 절대 시간을 계산하는데 적합) 유닉스 시스템은 UTC로 정의된 기원 부터 경과된 절대 시간을 초로 표현(epoch time)(UTC = GMT, ZULU)\n운영체제는 커널이 관리하는 소프트웨어 시계를 사용해서 시간 경과를 추적\nJiffie counter : 커널은 타이머를 이용해서 타이머 동작주기마다 틱이나 지피라는 한단위 만큼 늘림 HZ : 시스템 타이머의 빈도(아키텍쳐마다 다름)(100을 사용하다 최근 250을 사용, 4ms마타 시스템 타이머 동작) CLOCK_PER_SEC : POSIX 인터페이스에서 고정 주파수에 맞춰 시간 측정값을 제공하는데 이 때의 고정 주파수 sysconf(_SC_CLK_TCK)로 HZ값을 알 수 있음\n커널은 시작하면서 하드웨어 시계에서 현재 시간을 받아 운영체제 시각을 초기화, 시스템 종료시 현재 시각을 하드웨어 시계에 다시 기록\nhwclock 명령으로 하드웨어 시각과 운영체제 시각을 동기화 가능\n시간에 대한 자료 구조 유닉스 시스템이 발전함에 따라 시간 관리를 위한 독자적인 인터페이스를 구현했으며 단순한 시간 개념을 여러 자료 구조로 표현 전통적인 표현볍 가장 단순한 자료구조는 time_t이며 \u0026lt;time.h\u0026gt;에 정의 (time_t 내부 구조를 숨기기 위함) 대부분의 유닉스 시스템에서 long임 (typedef long time_t) 마이크로 초 정밀도 1 2 3 4 5 6 #include \u0026lt;sys/time.h\u0026gt; struct timeval{ time_t tv_sec; // 초 suseconds_t tv_usec // 마이크로초 } timeval 구조체는 time_t를 확장하여 마이크로초 정밀도를 추가 나노 초 정밀도 1 2 3 4 5 6 #include \u0026lt;sys/time.h\u0026gt; struct timespec { time_t tv_sec; // 초 long tv_nsec; // 나노 초 } timespec 구조체가 나노초까지 해상도를 제공 timespec 구조체 등장 이후 대다수 시간 관련 인터페이스는 timespec을 사용하도록 변경(그라나 중요함수는 여전히 timeval 사용) 시간 세분하기 C표준은 좀 더 사람에게 가까운 형태로 세분화된 시간을 표현하는 tm 구조체를 제공 구조체에 대한 설명은 책에 있음 프로세스 시간을 위한 타입 clock_t는 tick을 표현, int로 표현, 인터페이스에 따라 clock_t 틱은 시스템의 실제 타이머 주파수(HZ) 나 CLOCK_PER_SEC를 나타냄 POSIX 시계 POSIX 시계는 5가지 타입이 있음(clockid_t 타입) CLOCK_REALTIME : 시스템 전역에서 사용하는 실제시간 시계, 설정하려면 특수 권한 필요 CLOCK_MONOTONIC : 시스템 시동과 같이 불특정 시작 시점 부터 단조롭게 증가하는 시계, 어떤 프로세스도 설정하지 못함 CLOCK_MONOTONIC_RAW : CLOCK_MONOTONIC과 유사하나 시간이 뒤틀렸을 때 조정되지 않음(리눅스 전용) CLOCK_PROCESS_CPUTIME_ID : 프로세서 수준에서 지원되는 각 프로세스에서 사용 가능한 고해상되 시계(x86의 경우 TSC 레지스터 사용) CLOCK_THREAD_CPUTIME_ID : CLOCK_PROCESS_CPUTIME_ID와 유사하나 스레드 마다 유일한 시계 시계해상도 1 2 3 #include \u0026lt;time.h\u0026gt; int clock_getres(clock_t clock_id, struct timespec *res); clock_getres() 함수 호출이 성공하면 res가 NULL이 아닐 경우 clock_id로 지정한 시계의 해상도를 res에 저장 성공시 0반환, 실패하면 -1을 반환하고 errno를 EFAULT, EINVAL 중 하나로 설정 현재시간 얻기 1 2 3 #include \u0026lt;time.h\u0026gt; time_t time(time_t *t); time() 함수를 호출하면 epoch time 이후 경과한 현재 시각을 초 단위로 표현하여 반환 에러 발생시 -1을 반환, errno를 설정(유일한 값은 EFAULT로 t가 유효한 포인터가 아님) 더 나은 인터페이스 1 2 3 #include \u0026lt;sys/time.h\u0026gt; int gettimeofday(struct timeval* tv, struct timezone* tz); gettimeofday()호출이 성공시 tv에 현재시각을 기록하고 0을 반환, tz는 사용하지 않으므로 NULL로 설정(커널이 시간대를 관리하지 않음, glibc도 timezone 구조체의 tz_dsttime 필드를 미사용) 호출 실패 시 -1 반환, errno 설정(EFAULT : tv,tz 가 유효하지 않은 포인터) 고급 인터페이스 1 2 3 #include \u0026lt;time.h\u0026gt; int clock_gettime (clockid_t clock_id, struct timespec *ts); clock_gettime()은 지정한 시계의 시간을 얻기 위한 함수, 나노 정밀도를 허용 호출이 성공하면 0을 반환, clock_id로 지정한 시계의 현재 시간을 ts에 저장 실패 시 -1을 반환, errno를 설정(EFAULT, EINVAL) 프로세스 시간 얻기 1 2 3 4 5 6 7 8 9 10 #include \u0026lt;sys/time.h\u0026gt; struct tms { clock_t tms_utime; /* 소비한 사용자 시간 */ clock_t tms_stime; /* 소비한 시스템 시간 */ clock_t tms_cutime; /* 자식 프로세스가 소비한 사용자 시간 */ clock_t tms_cstime; /* 자식 프로세스가 소비한 시스템 시간 */ } clock_t times(struct tms *buf); times() 시스템 콜은 실행 중인 프로세스와 자식 프로세스의 프로세스 시간을 틱 단위로 가져옴(호출 성공 시 buf 채움) 사용자 시간 : 사용자 영역에서 코드를 수행한 시간 시스템 시간 : 커널 영역에서 코드를 수행한 시간 자식 프로세스가 끝나고 부모가 wait() 또는 관련 함수를 호출하고 나서야 각 자식 프로세스에 대한 보고 시간이 구조체의 해당 필드에 포함 호출 실패 시 -1 반환, errno를 설정(EFAULT : buf가 유효하지 않은 포인터) 현재 날짜와 시각 설정하기 1 2 3 4 #define _SVID_SOURCE #include \u0026lt;time.h\u0026gt; int stime(time_t *t); stime() 호출 성공 시 시스템 시간을 t 값으로 설정하고 0을 반환 (time()함수 대응) CAP_SYS_TIME 기능이 필요(일반적으로 root사용자) 호출이 실패 시 -1을 반환, errno를 설정(EFAULT, EPERM) 정확하게 시각 설정하기 1 2 3 #include \u0026lt;sys/time.h\u0026gt; int settimeofday(const struct timeval *tv, const struct timezone *tz); settimeofday() 호출이 성공시 시스템 시간을 tz값으로 설정 후 0반환, tz에는 NULL이 권장 됨(gettimeofday() 대응) 호출이 실패 시 -1을 반환, errno를 설정(EFAULT. EINVAL, EPERM) 시각 설정을 위한 고급 인터페이스 1 2 3 #include \u0026lt;time.h\u0026gt; int clock_settime(clockid_t clock_id, const struct timespec *ts); clokc_settime() 호출 성공 시 clock_id로 지정한 시계를 ts 시간으로 설정, 성공시 0반환 호출이 실패 시 -1 반환, errno를 설정(EFAULT, EINVAL, EPERM) 대부분의 시스템에서 유일하게 설정가능한 시계는 CLOCK_REALTIME, 나노 초 해상도를 제공한다는 장점이 있음 시간 다루기 1 2 3 4 #include \u0026lt;time.h\u0026gt; char* asctime(const struct tm *tm); char* asctime_r(const struct tm *tm, char *buf); asctime()함수는 tm 구조체를 ASCII 문자열로 변환, 정적으로 할당된 문자열을 가리키는 포인터 반환 asctime()은 스레드 세이프 하지 않으므로 asctime_r을 사용(문자열을 buf에 저장, 최소 26글자) 에러 발생의 경우 NULL을 반환 1 2 3 #include \u0026lt;time.h\u0026gt; time_t mktime(struct tm *tm); mktime()은 tm 구조체를 time_t로 변환, tm에 따라 tzset()을 호출해 시간대를 설정 에러가 발생하면 -1을 반환 1 2 3 4 #include \u0026lt;time.h\u0026gt; char* ctime(const time *timep); char* ctime_r(const time_t *timep, char *buf); ctime()은 time_t를 ASCII 표현으로 변환, 실패하면 NULL을 반환 줄바꿈 문자를 반환 문자에 붙임 ctime()은 스레드 세이프 하지 않으므로 ctime_r을 사용(문자열을 buf에 저장, 최소 26글자) 1 2 3 4 #include \u0026lt;time.h\u0026gt; struct tm* gmtime(const time_t *timep); struct tm* gmtime_r(const time_t *timep, struct tm *result); gmtime()은 time_t를 tm구조체로 변환하여 UTC 시간대로 표현, 실패하면 NULL을 반환 gmtime()은 스레드 세이프 하지 않으므로 gmtime_r을 사용 1 2 3 4 #include \u0026lt;time.h\u0026gt; struct tm* localtime(const time_t* timep); struct tm* localtime_r(const time_t* timep, struct tm* result); localtime(), localtime_r은 gmtime(), gmtime_r()과 비슷하게 동작, 그러나 time_t를 사용자 시간대에 맞춰 표현 localtime()도 tzset()을 호출하여 시간대를 초기화 함(localtime_r()의 동작은 표준에 명세되어 있지 않음) 1 2 3 #include \u0026lt;time.h\u0026gt; double difftime(time_t time1, time_t time0); difftime()은 두 값사이에 경과한 초를 double타입으로 변환하여 반환 오버플로우가 감지를 고려하지 않은 점만 제외하고 (double)(time1 - time0)와 같다 리눅스에서 time_t는 정수타입이므로 double 타입으로 변환할 필요는 없지만 이식성을 고려하여 difftime() 사용을 권장 시스템 시계 조율 실제 시간을 갑작스럽게 조정하면 동작 과정에서 절대 시각에 의존하는 어플리케이션은 혼란에 빠짐(ex. Makefile 증분 빌드) 이를 위해 유닉스는 adjtime() 함수를 통해 제공된 시간 차이에 따라 현재 시각을 점진적으로 조정(adjtime()을 사용하여 주기적으로 시간 보정) 1 2 3 4 #define _BSD_SOURCE #include \u0026lt;sys/time.h\u0026gt; int adjtime(const struct timeval *delta, struct timeval *olddelta); adjtime() 함수 호출이 성공하면 커널이 delta로 지정한 시간에 맞추어 천천히 조정 작업을 시작하도록 하고 0을 반환 delta가 양수면 시간을 빠르게 가게, 음수면 느리게 가도록(갑작스런 시간변화는 안생김) delta가 NULL이 아니라면 이전에 등록된 조정값을 처리하는 작업은 멈춤, olddelta가 NULL이 아닐 시 이전에 등록되었으나 아직 적용되지 않은 갑이 가져옴 delta가 NULL이고 olddelta가 NULL이 아니면 현재 조정중인 값을 가져올 수 있음 adjtime()에 적용할 조정값은 크기가 작아야 함(NTP가 이상적 활용 사례) 에러 발생 시 -1을 반환, errno를 설정(EFAULT, EINVAL, EPERM) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #include \u0026lt;sys/timex.h\u0026gt; struct timex { int mode; /* 모드 셀렉터 */ long offset; /* 시간 오프셋(usec) */ long freq; /* 주파수 오프셋 (스케일된 ppm) */ long maxerror; /* 최대 에러 (usec) */ long esterror; /* 예상 에러(usec) */ int status; /* 시계 상태 */ long constant; /* PPL 시간 상수 */ long precision; /* 시계 정밀도 (usec) */ long tolerance; /* 시계 주파수 허용 오차 (ppm) */ struct timeval time; /* 현재 시각 */ long tick; /* 틱 경과 시각(usec) */ }; int adjtimex(struct timex *adj); adjtimex()를 호출하면 커널 시간관련 파라미터를 읽어 adj가 가리키는 timex 구조체에 기록(adjtime()보다 훨씬 강력하고 복잡, 리눅스 전용) timex 구조체의 mode 필드에 따라 선택적으로 특정 파라메터를 추가적으로 설정, mode 필드는 플래그 비트단위 OR(책에 설명 있음) mode가 0이면 아무런 값도 설정하지 않음(설정값을 가져옴), CAP_SYS_TIME 기능이 있는 사용자(root)만 0이 아닌 값을 설정 가능 호출이 성공 시 현재 시계 상태 반환(TIME_OK/INS/DEL 등), 실패시 -1 반한 및 errno설정(EFAULT, EINVAL, EPERM) (책에 설명) 자세한 설명은 RFC130을 참조 잠들기와 대기 1 2 3 #include \u0026lt;unistd.h\u0026gt; unsigned int sleep(usigned int seconds) sleep()은 seconds로 지정한 초 동안 프로세스를 잠재움 sleep()은 호출이 성공하면 0을 반환, 그렇지 않다면 잠들지 않은 초를 반환(시그널이 잠들기를 방해한 상황) 마이크로 초 해상도로 잠들기 1 2 3 4 5 6 7 8 9 10 /* BSD 버전 */ #include \u0026lt;unistd.h\u0026gt; void usleep(unsigned long usec); /* SUSv2 버전 */ #define _XOPEN_SOURCE 500 #include \u0026lt;unistd.h\u0026gt; int usleep(usecond_t usec); usleep() 호출이 성공하면 프로세스는 usec 동안 잠든다 BSD는 반환 값이 없으며, SUS버전은 성공시 0을 실패 시 -1 반환 및 errno설정 나노 초 해상도로 잠들기 1 2 3 4 #define _POSIX_C_SOURCE 199309 #include \u0026lt;time.h\u0026gt; int nanosleep(const struct timespec *req, struct timespec *rem); nanosleep() 함수는 호출이 성공하면 req로 명시한 시간 동안 프로세스가 잠들며 0을 반환 에러가 발생하면 -1을 반환 및 errno를 적절한 값으로 설정 (EFAULT, EINVAL, EINTR) 시그널이 잠들기를 방해하면 -1을 반환하고 errno를 EINTR로 설정, rem이 NULL이 아니라면 남은 시간을 rem에 저장(두번째 인자를 이용하면 인터럽트가 걸리더라도 계속 잠들게 프로그래밍 가능, 책에 예제) nanosleep()은 sleep()/usleep()보다 장점, 왠만하면 이 함수를 사용하자 높은 해상도 제공, POSIX.1b에서 표준화, 시그널로 구현되어 있지 않음 고급 잠들기 기법 1 2 3 #include \u0026lt;time.h\u0026gt; int clock_nanosleep (clockid_t clock_id, int flag, const struct timespec *req, struct timespec *rem); clock_nanosleep()은 nanosleep()과 유사하게 동작하는 고급인터페이스 clock_id는 측정 시계를 명시, CLOCK_PROCESS_CPUTIME_ID를 제외하고 유효(잠들면 프로세스 시간도 잠듬) 어떤 시간까지 잠들어야 한다면 CLOCK_REALTIME, 상대 시간 동안 잠들어야 한다면 CLOCK_MONOTONIC이 제격 flag는 TIMER_ABSTIME이면 req로 지정된 값은 절대값으로 취급(잠재적 경쟁상태 해결, 책에 자세한 설명), 0이면 상대값 이식성을 고려한 잠들기 2장에서 설명했던 select는 이식성이 높은 마이크로 초 단위로 잠들기 방법을 제공(책에 잘 설명되어 있다.) 시간 초과 스케줄러 동작(커널이 정시에 깨웠지만 스케줄러가 다른 작업 선택 수행)이나 타이머 초과(타이머 정밀도가 요청 받은 시간 간격보다 떨어지는 경우) 때문에 요청보다 초과해서 잠들 수 있음 POSIX에서 제공하는 고해상도 시계와 더 높은 HZ 값을 사용하여 시간 초과 가능성을 최소로 줄임 잠들기 대안 되도록이면 잠들기를 피하자, 1초 미만으로 잠든다면 크게 문제되지 않음 이벤트를 기다리며 busy wait를 위해 잠들기로 도배한 코드는 나쁘다. 파일 디스크립터를 븍록해서 커널이 잠들기를 처리하고 프로세스를 깨우도록 하자 타이머 간단한 알람 1 2 3 #include \u0026lt;uinstd.h\u0026gt; unsigned int alarm(unsigned int seconds); alarm() 함수를 호출하면 실제 시간에서 seconds 초가 경과한 후에 호출한 프로세스에 SIGALRM 시그널을 전송하도록 예약 앞서 걸어둔 시그널이 대기 중인 상황이라면 호출은 그 알람을 취소하고 새로 요청한 알람으로 대체 후 이전 알람에서 남아 있는 초 숫자 반환 second가 0이라면 이전 알람은 취소되나 새로운 알람은 안걸어 둠 SIGALRM 시그널 처리를 위한 시그널 핸들러 등록 필요 인터벌 타이머 1 2 3 4 5 6 7 8 9 #include \u0026lt;sys/time.h\u0026gt; struct itiemerval { struct timeval it_interval; /* 다음 값 */ struct timeval it_value; /* 현재 값 */ } int gettimer(int which, struct itimerval *value); int settimer(int which, const struct itimerval *value, struct itimervla *ovalue); 인터벌타이머는 alarm()처럼 동작하나 옵션으로 자동 재장전 기능 제공하며 명확하게 구분된 세가지 모드 중 하나로 동작 ITIMER_REAL : 실제 시간을 측정, 지정된 실제 시간이 경과하면 프로세스에 시그널(SIGALRM) 전송 ITIMER_VIRTUAL : 프로세스 사용자 영역 코드가 수행되는 동안에만 타이머가 흐름, SIGVTALRM 시그널 전송 ITIMER_PROF : 프로세스가 실행 중이거나 커널이(시스템 콜 완료 등) 프로세스를 대신해서 실행 중인 경우 타이머 흘러감(SIGPROF 시그널 전송) itimerval 구조체는 타이머 만료 시간과 만료 이후에 타이머를 다시 설정하기 위한 경과 시간을 사용자가 명시하도록 함 settimer()는 which가 가리키는 타이머에 it_value로 지정한 만료시간을 설정 지정 시간이 경과하면 it_interval로 지정한 시간으로 타이머 재설정 ovalue가 NULL이 아니면 which가 가리키는 인터벌 타이머에 설정된 이전 값 반환 성공 하면 0을 반환, 실패 시 -1반환 및 errno를 설정(EFAULT, EINVAL) 고급 타이머 가장 강력한 타이머 인터페이스는 POSIX 시계에서 시작\n타이머 생성하기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #include \u0026lt;signal.h\u0026gt; #include \u0026lt;time.h\u0026gt; struct sigevent{ union sigval sigev_value; int sigev_signo; int sigev_notify; void (*sigev_notify_function)(union sigval); pthread_attr_t *sigev_notify_attributes; } union sigval { int sival_int; void *sival_ptr; } int timer_create (clockid_t clockid, struct sigevent *evp, timer_t *tiemrid); timer_create() 호출이 성공하면 clockid로 지정한 POSIX 시계와 관련된 새로운 타이머를 생헝하며 timerid에 새롭고 유일한 타이머 식별자를 저장한 다음 0을 반환 호출이 실패하면 -1을 반환 및 errno 설정(EAGAIN, EINVAL, ENOTSUP) evp 인자가 NULL이 아니라면 타이머가 만료될 때 발생하는 비동기식 통지를 지정, 프로세스는 sigev_notify로 타이머 만료시 동작을 명세 SIGEV_NONE : NULL 통지, 타이머 만료 시 아무일도 일어나지 않음 SIGEV_SIGNAL : 타이머가 만료될 때 커널은 프로세스에 sigev_signo로 지정한 시그널을 보냄 SIGEV_THREAD : 타이머가 만료될 때 커널은 새로운 스레드를 띄워서 sigev_notify_function 함수에 sigev_value를 인자로 넘겨서 실행, sigev_notify_attributes가 스레드 속성 evp 가 NULL이면 SIGEV_SIGNAL, SIGALRM, 타이머 id로 설정되서 타이머 만료를 알림 타이머 설정하기 1 2 3 #include \u0026lt;time.h\u0026gt; int timer_settime(tiemr_t timerid, int flag, const struct itimerspec *value, struct itiemrspec *ovalue); timer_settime() 호출 시 만료시간이 value이고 timerid로 지정한 타이머가 설정 flag는 TIMER_ABSTIME이면 req로 지정된 값은 절대값으로 취급(고급 잠들기 기법 참조) 타이머 만료 정보 얻기 1 2 3 #include \u0026lt;time.h\u0026gt; int timer_gettime(timer_t timerid, struct itimerspec *value); timer_gettime() 호출이 성공하면 timerid로 지정한 타이머의 만료 시간을 value에 저장 호출 성공 시 0반환, 실패 시 -1 반환 및 errno설정(EFAULT, EINVAL) 타이머 초과 횟수 얻기 1 2 3 #include \u0026lt;time.h\u0026gt; int timer_getoverrun (timer_t timerid); 호출이 성공하면 타이머의 초기 만료 시점과 타이머 만료 상태를 프로세스에 통지한 시점 사이에 일어난 추가적인 타이머 만료 횟수를 반환 POSIX는 초과횟수가 DELAYTIMER_MAX 이상일 시 DELAYTIMER_MAX 반환(리눅스는 0부터 다시 시작) 실패하면 -1 반환 및 errno를 설정(EINVAL : timerid가 유효하지 않음) 타이머 삭제하기 1 2 3 #include \u0026lt;time.h\u0026gt; int timer_delete (timer_t timerid); timer_delete() 호출이 성공하면 timerid 관련 타이머를 삭제하고 0을 반환 실패하면 -1 반환 및 errno를 설정(EINVAL : timerid가 유효하지 않음) ","date":"2023-10-07T00:00:00Z","permalink":"https://muonkmu.github.io/p/linux_system_programming-11-%EC%8B%9C%EA%B0%84/","title":"[Linux_System_programming] 11 시간"},{"content":"스레딩은 단일 프로세스 내에서 실행 유닛을 여러개 생성하고 관리하는 작업 스레딩은 데이터 경쟁 상태와 데드락을 통해 프로그래밍 에러를 발생시키는 원인(잘 해결해보자)\n바이너리, 프로세스, 스레드 바이너리 : 저장장치에 기록된 프로그램, 실행되지 않은 프로그램 프로세스 : 실행된 바이너리를 표현하기 위한 운영체제 추상개념(가상화된 메모리, 열린 파일디스크립터, 커널 리소스 등을 포함) 스레드 : 프로세스 내의 실행단위(운영체제의 프로세스 스케줄러에 의해 스케줄링 될 수 있는 최소한의 실행 단위) 최신의 운영체제는 가상메모리와 가상프로세서라는 두가지 추상개념을 제공 가상메모리 : 프로세스와 관련 있으며 개별 프로세서가 메모리 전체를 소유했다고 착각 가상프로세서 : 스레드와 관련 있으며 스레드가 시스템의 모든 프로세서를 소유했다고 착각 하게함 멀티스레딩 멀티스레딩의 장점은 다음과 같음 프로그래밍 추상화 : 작업을 나누가 각각 실행단위(스레드)로 할당하는 것은 자연스러운 패턴(연결별 스레드 패턴, 스레드풀 패턴) 병렬성 : 스레드 사용 시 프로세서가 여러개인 머신에서 효과적으로 병렬성을 구현 가능 응답속도 향상 : 오래 실행되는 작업을 워커 스레드에 맡기고 최소한 하나의 스레드는 사용자 입력에 대응하게 하여 응답속도 향상 가능 입출력 블록 : 위와 관련, 스레드 미사용 시 입출력을 블록하면 전체 스레드를 멈추게 함 컨텍스트 스위칭 : 프로세스 컨텍스트 스위칭 비용보다 스레드 전환 비용이 싸다 메모리 절약 : 메모리를 공유하는 효과적인 방법을 제공 멀티스레딩 비용 멀티스레딩 프로그램을 설계/작성/디버깅은 싱글 스레드에 비해 어렵다. 가상화된 프로세서는 복수인데 반해 가상 메모리는 하나(멀티스레드 프로세스는 동시에 여러가지 일을 할 수 있지만 이는 같은메모리 공유) 시스템 설계 시작부터 반드시 스레딩 모델과 동기화 전략 고려 필요 멀티스레딩 대안 지연 시간 및 입출력 장점 측면에서 다중입출력, 논블럭 입출력과 비동기식 입출력을 조합해 스레드 대신 사용 가능 메모리 절약 측면에서 리눅스는 스레드 보다 더 제한된 방식으로 메모리 공유 도구 제공 스레딩 모델 스레드를 구현할 수 있는 몇가지 방법 중 가장 단순한 모델은 커널에서 스레드에 대한 네이티브 지원 제공 상기 모델은 커널이 제공하는 것과 사용자가 사용하는 것이 1:1 관계를 가지므로 1:1 스레딩(커널 레벨 스레딩)이라 함 리눅스에서의 스레딩은 1:1, 리눅스 커널은 단순히 리소스를 공유하는 프로세스의 형태로 스레드를 구현 스레딩 라이브러리는 clone() 시스템콜을 사용하여 새로운 스레드 생성하고 반환된 프로세스는 사용자 영역의 개념적인 스레드로써 직접 관리 사용자 레벨 스레딩 N:1 스레딩(사용자 레벨 스레딩)은 스레드가 N개인 프로세스 하나는 단일 커널 프로세스로 매핑 커널 레벨 스레딩과 대조적으로 사용자 영역에서 스레드 개념 구현 커널의 지원을 거의 필요로 하지 않거나 커널의 지원 없이 스레드를 관리하는 사용자 역역 스케줄러와 논블록킹 방식으로 입출력 처리 사용자 레벨 스레딩은 커널의 관여 없이 스스로 어떤 스레드를 언제 실행할 지 결정할 수 있으므로 컨텍스트 스위칭 비용이 거의 안듬 그러나 최신 하드웨어에서는 컨텍스트 스위칭 비용이 많이 높지 않기에 효과는 미미 하나의 커널 요소가 N개의 스레드를 떠받치고 있기 때문에 여려 개의 프로세서를 활용 할 수 없고 제대로된 병렬성을 제공할 수 없음 리눅스용 사용자 레벨 스레딩 라이브러리는 대부분 1:1 스레딩도 제공 하이브리드 스레딩 하이브리드 스레딩(N:M 스레딩)은 커널은 네이티브 스레드 개념을 제공하고 사용자 영역에서도 역시 스레드를 구현(병렬성과 저렴한 컨텍스트 스위칭 비용을 위해) 구현이 복잡하고 컨텍스트 스위칭 비용이 비싸지 않으므로 인기가 없다 1:1 모델이 리눅스에서는 가장 인기가 높음 코루틴과 파이버 코루틴과 파이버는 스레드보다 더 작은 실행단위(코루틴은 프로그래밍 언어에서, 파이버는 시스템에서 사용되는 용어) Go언어에서 비슷한 고루틴을 제공하므로 알고 싶으면 이걸 써봐라(책에서는 이런게 있다 정도만 언급) 스레딩 패턴 스레드 사용 어플리케이션 구현 시 가장 먼저 할 일은 어플리케이션의 처리과정과 입출력 모델을 결정 짓는 스레딩 패턴 결정 많은 모델이 있지만 연결별 스레드와 이벤트 드리븐 패턴을 설명 연결별 스레드 연결별 스레드 : 하나의 작업 단위(요청이나 연결)가 스레드 하나에 할당되는 프로그래밍 패턴(작업이 완료될 때까지 실행하는 패턴) 연결(요청)이 스레드를 소요하므로 블록킹이 허용됨(스레드가 블록되면 해당 블록킹을 유발한 연결만 멈춤) 구현의 상세 내용은 스레드의 개수(대부분 구현에서는 생성할 스레드 개수 제한, 스레드 상한 시 요청은 요청 큐에 저장 혹은 거부) 이벤트 드리븐 스레딩 웹서버에서 요청마다 스레드를 생성하면 하드웨어 리소스를 많이 사용함 연결별 스레드 패턴에서 대부분의 부하는 단순히 대기하는 것 뿐이므로 스레드에서 대기하는 부분을 분리 입출력은 비동기식으로 처리하고 다중 입출력으로 서버내 제어 흐름을 관리 요청을 처리하는 과정이 일련의 비동기식 입출력 요청으로 변환되어 관련 콜백과 연결 콜백은 다중 입출력 과정에서 대기하기도 함, 이를 이벤트 루프라고 부름 입출력 요청이 반환 시 이벤트 루프는 해당 콜백을 대기중인 스레드로 넘김 이벤트 드리븐 패턴은 멀티스레드 서버를 설계하는데 선호되는 방식 스레드 사용 시스템 소프트웨어 설계 시 먼저 이벤트 드리븐 패턴으로 비동기식 입출력, 콜백, 이벤트 루프, 프로세스 개수만큼 스레드를 사용하는 작은 스레드 풀을 고려해보자 동시성, 병렬성, 경쟁 상태 스레드는 동시성과 병렬성이는 특징을 지님 동시성 : 둘 이상의 스레드가 특정 시간 안에 함께 실행되는 것 병렬성 : 둘 이상의 스레드가 동시에 실행되는 것(다중 프로세서 필요) 경쟁 상태 스레드는 순차적으로 실행되지 않고 실행이 겹치고도 하므로 각 스레드의 실행 순서를 예측할 수 없다. 경쟁 상태란 공유 리소스(커널 리소스, 메모리, 하드웨어 등)가 동기화되지 않은 둘 이상의 스레드가 접근하여 프로그램의 오동작을 유발하는 상황 경쟁 상태를 피할 수 있는 방법에 대해 알아보자 동기화 경쟁 상태를 예방하려면 크리티컬 섹션 접근을 상호 배제(mutual exclusion)하는 방식으로 접근을 동기화 해야함 원자적(atomic) : 다른 연산에 끼어들 여지가 없을 시 뮤텍스 크리티컬 섹션을 원자적으로 만들기 위한 평범한 기번은 크리티컬 섹션 안에서 상호배제를 구현해서 원자적으로 만들어 주는 락, 이를 뮤텍스라 부름 락을 정의 후 크리티컬 섹션으로 들어가기 전 락을 걸고 나올 때 락 반환 데드락 데드락이란 두 스레드가 서로 상대방이 끝나기를 기다리고 있어서 결국엔 둘 다 끝나지 못하는 상태 두 스레드가 서로 상대 스레드가 가지고 있는 뮤텍스를 해제하기를 기다리고 있을 때 발생 데드락 피하기 뮤텍스는 코드가 아닌 데이터와 연관지어 생각 데이터의 계층 구조를 명확히 해서 뮤텍스 또한 확실한 계층구조를 갖도록 하는 것이 중요 ABBA 또는 연인 데드락의 경우 A뮤텍스는 B뮤텍스 보다 먿저 얻도록 할 것 Pthread 리눅스 커널의 스레딩 지원은 clone() 시스템 콜 같은 원시적 수준 뿐이나 사용자 영역에서 스레딩 라이브러리를 제공 POSIX는 스레딩 라이브러리에 대한 표준을 정의 = pthread 리눅스 스레딩 구현 리눅스에서 표준 스레드의 구현은 glib에서 제공하며 pthread의 두가지 구현을 제공(LinuxThread와 NPTL) LinuxThread는 1:1스레딩을 제공, 시그널을 통한 스레드 간 통신, 오래되서 잘 안씀 NPTL(Native POSIX Thread Library) : 1:1스레딩 제공, 스레드의 확장성을 극적으로 향상(이걸 쓰자) Pthread API Pthread API는 \u0026ldquo;\u0026lt;pthread.h\u0026gt;\u0026rdquo; 파일에 정의 되며 모든 함수는 pthread_로 시작 Pthread함수는 크게 두개의 그룹 : 스레드 관리(스레드 생성,종료,조인,디태치), 동기화(뮤텍스와 조건변수, 배리어 등) Pthread 링크하기 glib에서 pthread를 제공하지면 libpthread 라이브러리는 분리 되어 있으므로 컴파일 시 \u0026ldquo;-pthread\u0026rdquo; 플래그로 링크해 줄 것 스레드 생성하기 1 2 3 4 #include \u0026lt;pthread.h\u0026gt; int pthread_create(pthread_t *thread, const pthread_attr_t *attr, void *(*start_routine)(void *), void* arg); pthread_create() 함수는 새로운 스레드를 생성(main함수는 실행 시점에서는 싱글스레드 임) 호출이 성공 시 새로운 스레드가 생성되고 start_routine 인자로 명시된 함수에 arg로 명시된 인자를 넘겨서 실행을 시작 pthread_t 포인터인 thread가 NULL이 아니라면 여기에 새로 만든 스레드를 나타내기 위해 사용되는 스레드ID를 저장 pthread_attr_t 포인터 attr에는 새로 생성된 스레드의 기본 속성을 변경하기 위한 값, 보통 NULL을 넘기며 기본속성으로 설정 스레드는 부모 스레드의 리소스를 공유(특히 주소공간, 시그널 핸들러, 열린 파일 등) 에러 발생시 0이 아닌 에러코드를 직접 반환(EAGAIN, EINVAL, EPERM) 스레드 ID 1 2 3 #include \u0026lt;pthread.h\u0026gt; ptread_t pthread_self(void); TID는 pthread 라이브러리에서 할당(PID는 커널에서 할당) pthread_self()함수를 이용하여 자신의 TID를 얻을 수 있음 스레드ID 비교하기 1 2 3 #include \u0026lt;pthread.h\u0026gt; int pthread_equal(pthread_t t1, pthread_t t2); Pthread 표준은 pthread_t가 산술타입임을 강제하지 않으므로 ==연산자 동작을 보증할 수 없다 phread_equal() 함수는 두 TID가 동일하면 0이 아닌값을 반환(다르면 0) 스레드 종료하기 스레드의 종료상황(1,2,3은 스레드 하나만 종료 되는 경우, 4,5,6은 모든 스레드 종료) start_routine 함수가 반환한 경우 pthread_exit()함수를 호출 pthread_cancel()을 통해 다른 스레드에서 중지 프로세스의 main() 함수 반환 프로세스가 exit() 호출로 종료 프로세스가 execve() 호출로 새로운 바이너리 실행 시그널을 통해 프로세스나 개별 스레드를 종료할 수 있지만 Pthread는 시그널 처리를 복잡하게 만드는 터라 멀티 스레드 프로그래밍 시 시그널 사용을 최소화 할 것 스스로 종료하기 1 2 3 #include \u0026lt;pthread.h\u0026gt; void pthread_exit(void* retval); start_routine을 끝까지 실행하면 스레드를 스스스로 종료하게 할 수 있음 콜 스택 깊은 곳에서 스레드를 종료해야 한다면 pthread_exit 사용 호출 시 thread 종료, retval은 그 프로세서가 종료되길 기다리는 다른 스레드에 전달할 값 호출은 실패하지 않음 다른 스레드 종료하기 1 2 3 4 5 #include \u0026lt;pthread.h\u0026gt; int pthread_cancel(pthread_t thread); int pthread_setcancelstate(int state, int *oldstate); int pthread_setcanceltype(int type, int *oldtype); pthread_cancel() 호출이 성공 시 thread로 명시한 스레드ID를 가진 스레드에 취소 요청을 보냄 호출 성공 시 0을 반환(성공은 취소 요청을 보내는데 성공이란 의미, 종료는 비동기적으로 일어남) 호출 실패시 스레드가 유효하지 않음을 나타내는 ESRCH를 반환 기본적으로 스레드는 취소 가능이나 취소가 불가능할 경우 취소 요청은 취소 가능 때까지 큐에 대기, 취소 상태는 pthread_setcancelstatae()를 통해 변경 가능 호출 성공 시 호출한 스레드의 취소 상태가 state(PTHREAD_CANCEL_ENABLE/DISABLE) 값으로 설정, 이전 상태는 oldstate에 저장 에러가 발생 시 state 값이 유효하지 않음을 나타내는 EINVAL을 반환 스레드의 취소 타입은 비동기 또는 유예, 취소 유예가 기본값 취소 비동기 : 취소 요청이 들어온 이후에 언제든지 스레드 종료 가능 (스레드가 크리티컬 섹션안에 있을 수 있기 때문에 스레드가 공유리소스 미사용, 재진입 가능 함수 사용시 만 사용) 취소 유예 : pthread 나 C라이브러리 함수 내에서 안전한 특정 시점에만 종료 가능 취소 타입은 pthread_setcanceltype() 함수로 변경 가능 호출 성공 시 취소타입은 type(PTHREAD_CANCEL_ASYNCHRONOUS/DEFERRED)값으로 변경, 이전 값은 oldtype에 기록 에러 발생 시 EINVAL을 반환(type값이 유효하지 않음) 스레드 조인과 디태치 스레드 조인 : 스레드의 종료를 동기화 하는 것(프로세스의 wait()) 스레드 조인 1 2 3 #include \u0026lt;pthread.h\u0026gt; int pthread_join(pthread_t thread, voit **retval); pthread_join()은 호출 성공 시 thread로 명시한 스레드가 종료될 때까지 블록되도록 함 (이미 종료되었다면 즉시 반환) 스레드 종료 시 스레드가 깨어남, retval은 종료된 스레드의 반환 값 하나의 스레드는 여러 스레드를 조인 할 수 있지만(1:N), 하나의 스레드만 다른 스레드에 조인을 시도해야함(N:1) 에러가 발생시 0이 아닌 EDEADLK, EINVAL, ESRCH 중 반환(설명은 책에) 스레드 디태치 1 2 3 #include \u0026lt;pthread.h\u0026gt; int pthread_detach(pthread_t thread); 스레드는 조인 되기 전까지 시스템 리소스를 소모하므로 조인할 생각이 없는 스레드는 디태치 해두어야 함 pthread_detach()는 thread로 명시한 스레드를 디태치 하고 호출 성공시 0반환, 에러 발생 시 ESRCH(인자가 유효하지 않음) 반환 스레딩 예제 책에 예제 있음 Pthread 뮤텍스 뮤텍스 초기화하기 뮤텍스는 pthread_mutex_t 객체로 표현, 동적으로 생성할 수 있으나 대부분 정적으로 생성 1 2 /* mutex라는 이름의 뮤텍스를 선언하고 초기화 */ pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER; 뮤텍스 락 걸기 1 2 3 #include \u0026lt;phtread.h\u0026gt; int pthread_mutex_lock(pthread_mutex_t* mutex); pthread에서 pthread_mutex_lock() 함수를 사용하면 락을 걸 수 있음 호출이 성공하면 mutex로 지정한 뮤텍스의 사용이 가능해질때 까지 호출한 스레드 블록 뮤텍스가 사용가느안 상태가 되면 호출한 스레드가 깨어나고 이 함수는 0을 반환(호출 시점에 뮤텍스가 사용가능하다면 즉시 반환) 에러가 발생하면 0이 아닌 에러 코드 반환(EDEADLK, EINVAL) 뮤텍스 해제하기 1 2 3 #include \u0026lt;phtread.h\u0026gt; int pthread_mutex_unlock(pthread_mutex_t* mutex); pthread_mutex_unlock() 호출이 성공하면 mutex로 지정한 뮤텍스를 해제하고 0을 반환, 블록 되지 않고 즉시 mutex 해제 에러가 발생하면 0이 아닌 에러 코드 반환(EINVAL, EPERM) 뮤텍스 예제 책에 스코프드락 설명과 뮤텍스 예제 있음 ","date":"2023-10-01T00:00:00Z","permalink":"https://muonkmu.github.io/p/linux_system_programming-07-%EC%8A%A4%EB%A0%88%EB%94%A9/","title":"[Linux_System_programming] 07 스레딩"},{"content":"유닉스는 바이너리 이미지를 메모리에 적재하는 과정에서 새로운 프로세스를 생성하는 부분을 분리함 이는 fork(), exec()라는 두 개의 시스템 콜을 필요\n프로세스 스케줄링 프로세서 스케줄러는 커널의 서브시스템으로 다음에 실행할 프로세서를 선택(시간 배분) 멀티 태스킹 운영체제는 선점형, 비선점형으로 나눔 현재(?) 리눅스 스케줄러는 CFS로 이전의 스케줄러(O(1) 등)과는 많이 다르다 타임 슬라이스 스케줄러가 선점하기 전까지 프로세스에 허락된 실행 시간 (스케줄러가 프로세스에 할당) 입출력 위주 프로세스와 CPU 위주 프로세스 CPU위주 프로세스 : 무한 루프와 같이 사용가능한 타임 슬라이스를 끊임없이 계속 사용하는 프로세스(수학연산, 이미지 처리) 입출력 위주 프로세스 : 실행시간 보다 리소스를 사용하기 위해 기다리는 시간이 더 많은 프로세스(파일 네트워크 입출력) CPU 위주 프로세스는 최대한 작업을 빨리 마칠 수 있도록 큰 타임 슬라이스가 적합하며 입출력 위즈 프로세스는 반응속도를 빠르게 하기 위해 작은 타임 슬라이스가 적합 선점형 스케줄링 전통적인 유닉스 프로세스 스케줄링에서 모든 실행 가능한(블록되지 않은) 프로세스는 타임 슬라이스를 할당 받음 타임 슬라이스 모두 소진시 커널은 그 프로세스를 잠시 멈추고 다른 프로세스 실행 CFS 스케줄러 CFS전의 유닉스는 스케줄링에 우선순위와 타임슬라이스라는 변수를 사용, 최신 데스크탑/모바일 장치에서는 조금 부족한 면이 있음 CFS(Completely Fair Scheduler)는 타임슬라이스를 사용하지 않는 공정한 스케줄링 알고리즘 사용 CFS는 N개 프로세스에 각각 1/N 만큼 CPU시간을 할당(한정된 시간, 즉 타겟 레이턴시를 N으로 나눔)하고 nice값에 따라 가중치를 둠 가중치가 높을 수록 더 많은 CPU시간을 할당(기본값은 0 = 가중치 1) 프로세스가 많아서 할당받은 CPU시간이 매우 적을 때를 대비해 핵심 변수인 최소단위를 사용 최소 단위가 효과를 발휘하면 공정성이 무너진다는 뜻이므로 평범한 상황에서는 최소단위가 적용되지 않고 타겟레이턴시 만으로 공정성을 유지 CFS는 CPU위주 프로세스와 입출력 위주 프로세스를 함케 스케줄링하는데서 발생하는 많은 문제를 해결 프로세서 양보하기 1 2 3 #include \u0026lt;sched.h\u0026gt; int sched_yield(void); sched_yield()를 호출하면 현재 실행 중인 프로세스를 잠시 멈춘 다음 스케줄러가 다음에 실행할 새로운 프로세스를 선택하도록 함 다른 실행 가능한 프로세스가 없으면 sched_yield()를 호출한 프로세스의 실행이 즉시 재개 호출 성공시 0반환, 실패 시 -1 반환 후 errno를 적절한 에러 코드로 설정 적당한 사용법 선점형 멀티태스킹에서 커널은 어플리케이션보다 더 효율적으로 스케줄링 할 수 있으므로 sched_yield()는 잘 사용하지 않음 외부 인터럽트를 기다려야 하는 프로그램에서 사용할 수 있지만 유닉스 프로그램은 블록가능한 파일 디스크립터에 의존하는 이벤트 드리븐 해법을 목표로 함 최근까지 사용자 영역에서 스레드 락이라는 상황에서 sched_yield()가 필요했으나 최신 커널에서는 퓨텍스(Fast user-space mutex)를 사용 프로세스 우선순위 프로세스에 할당되는 CPU시간은 nice값(프로세스 우선순위)에 의해 우선 순위가 할당 nice값은 -20~19 까지 가능(default 0), nice값이 적을 수록 우선순위가 높다(타임슬라이스가 커짐) nice() 1 2 3 #include \u0026lt;unistd.h\u0026gt; int nice(int inc); nice() 호출이 성공하면 inc만틈 프로세스의 nice 값을 증가 시킴 CAP_SYS_NICE 기능이 있는 프로세스(사실상 root소유 프로세스)만 inc에 음수를 넘겨 nice값을 감소시킬수 있다 에러가 발생하면 -1, 성공시 nice 값 반환(-1 반환이 가능하므로 errno를 0으로 초기화 후 검사가 필요)(에러코드 EPRM, EINVAL) inc에 0을 넘겨 현재 nice 값도 알 수 있다 getpriority()와 setpriority() 1 2 3 4 5 #include \u0026lt;sys/time.h\u0026gt; #include \u0026lt;sys/resource.h\u0026gt; int getpriority(int which, int who); int setpriority(int which, int who, int prio); 우선순위 설정에 선호되는 방법은 좀 더 많은 제어가 가능한 getpriority(), setpriority() 시스템콜 사용 which는 PRIO_PROCESS, PRIO_PGRP, PRIO_USER 중 하나 일것 who는 프로세스 ID, 프로세스 그룹ID, 사용자ID(0이라면 현재 프로세스/프로세스그룹/사용자 ID로 해석) getpriority() 호출 시 지정한 프로세스 중 가장 높은 우선순위 반환 에러가 발생하면 -1, 성공시 nice 값 반환(-1 반환이 가능하므로 errno를 0으로 초기화 후 검사가 필요) setpriority() 호출 시 모든 프로세스의 우선순위를 prio로 변경 CAP_SYS_NICE 기능이 있는 프로세스만 nice값을 감소시킬수 있다 호출 성공 시 0, 실패 시 -1 반환 입출력 우선순위 기본적으로 입출력 스케줄러는 입출력 우선순위를 결정하기 위해 프로세스의 nice값을 사용 nice값 설정 시 자동적으로 입출력 우선순위도 변경 리눅스 커널은 nice값에 독립적으로 입출력 우선 순위를 명시적으로 설정하고 가져올 수 있는 추가적 시스템 콜을 제공 ioprio_get, ioprio_set 그러나 사용자 영역 인터페이스는 제공하지 않음 프로세스 친화 리눅스는 멀티 프로세서를 지원하며 스케줄러는 프로세스가 어떤 CPU에서 수행할지 결정해야함 프로세스가 다른 CPU로 이전하는 것은 부대 비용이 발생해 스케줄러는 최대한 프로세스를 특정 CPUd에 유지하려 함 부대비용 : CPU 이동시 이동한 CPU캐시에 접근불가, 이전 CPU 캐시의 내용을 무효로 해야 함 CPU 사용률 불균형 해소를 위해 언제 프로세스를 옮길지 결정하는 작업을 로드 밸렁싱이라 함 프로세서 친화도는 프로세스를 꾸준히 같은 CPU에 스케줄링할 가능성을 말하며 느슨한 친화도, 엄격한 친화도가 있다. sched_getaffinity()와 sched_setaffinity() 1 2 3 4 5 6 7 8 9 10 11 12 #define _GNU_SOURCE #include \u0026lt;sched.h\u0026gt; typedef struct cpu_set_t; size_t CPU_SETSIZE; void CPU_SET(unsigned long cpu, cpu_set_t* set); void CPU_CLR(unsigned long cpu, cpu_set_t* set); int CPU_ISSET(unsigned long cpu, cpu_set_t* set); void CPU_ZERO(cpu_set_t* set); int sched_setaffinity(pid_t pid, size_t setsize, const cpu_set_t* set); int sched_getaffinity(pid_t pid, size_t setsize, cpu_set_t* set); sched_getaffinity() 호출 시 프로세스의 pid의 CPU 친화도를 조회하여 cpu_set_t 타입 변수에 저장 pid가 0이면 현재 프로세스의 친화도를 조회 setsize 인자는 cpu_set_t타입의 크기(sizeof(cpu_set_t))를 나타내며 타입의 크기가 바뀔 시 호환성 유지 목적 호출이 성공하면 0을 반환, 실패시 -1을 반환하고 errno를 설정(EFAULT, EINVAL, EPERM, ESRCH) CPU_SETSIZE는 set인자의 크기가 아니라 set으로 표현할 수 있는 프로세스의 개수이며 현재 구현은 각 프로세스를 단일 비트로 표현 책에 예제 있으므로 읽어보자 실시간 시스템 실시간 시스템은 운영 시 지켜야 할 최소한의 응답속도가 보장되어야 함 리눅스 및 최신의 운영체제는 실시간성을 몇단계로 나눠 지원 실시간 시스템의 종류 실시간 시스템은 하드 실시간, 소프트 실시간으로 나뉨 실시간 시스템이 반드시 빠를 필요는 없다, 사실 같은 하드웨어에서 비교 시 실시간 시스템은 실시간 프로세스를 지원하는데 따른 부하로 비 실시간 시스템 보다 느릴 수 있다. 레이턴시, 지터, 최소 응답시간 레이턴시 : 명령이 내려졌을 때 실행을 시작하기까지의 시간(명령이 언제 내려졌는지 정확하게 알아야 해서 측정이 힘듬) 지터 : 연속적인 이벤트 간의 시간 편차 하드 실시간 시스템에서 지터가 0이고 레이턴시가 최소 응답시간과 같도록 설계 소프트 실시간 시스템은 지터에 더 민감 리눅스의 실시간 지원 리눅스는 IEEE표준에 정의된 시스템 콜 함수군을 통해 소프트 실시간을 지원 리눅스 커널의 실시간 지원은 더 짧은 레이턴시와 일관적인 지터를 제공할 수 있도록 계속 개선됨 리눅스 커널에 적용된 임베디드 시스템과 실시간 시스템을 위한 변경은 공식 메인스트림이 아님, 그러나 실시간을 위한 변경은 POSIX 인터페이스를 활용 (다음 설명할 내용은 변경된 시스템에서도 유효) 리눅스 스케줄링 정책과 우선순위 프로세스와 관련된 리눅스 스케줄러의 동작 방식은 스케줄링 클래스라고 불리는 프로세스의 스케줄링 정책에 의존 리눅스는 기본정책(SCHED_OTHER) 외 두가지 실시간 스케줄링 정책(SCHED_FIFO, SCHED_RR)을 제공 모든 프로세스는 nice 값과 무관한 고유의 우선순위를 가지며 리눅스는 항상 가장 높은 우선순위의 프로세스를 실행 FIFO 정책 FIFO 스케줄링 정책은 타임 슬라이스를 필요로 하지 않는 매우 단순한 실시간 스케줄 정책 FIFO 스케줄링을 따르는 프로세스는 더 높은 우선순위의 프로세스가 실행가능한 상태가 되지 않는 한 계속 실행 FIFO 스케줄링 정책은 SCHED_FIFO로 지정 기본적으로 시스템 내에서 가장 높은 우선순위를 가진다면 원하는 만큼 계속 실행이 가능 스케줄링 상세 내역은 책을 참조 라운드 로빈 스케줄링 정책 라운드 로빈 스케줄링 정책은 SCHED_RR 매크로로 지정 스케줄러는 각 라운드 로빈 프로세스에 타임슬라이스를 배분 라운드로빈 프로세스가 타임슬라이스를 다 소진하면 스케줄러는 같은 우선순위 프로세스 목록에서 다음 프로세스를 실행 상기 내용을 제외하고는 FIFO 정책과 동일 표준 스케줄링 정책 SCHED_OTHER 매크로는 비실시간 프로세스의 기본 스케줄링 정책인 표준 스케줄링 정책 모든 일반 프로세스는 고유의 우선순위로 0을 가지므로 FIFO나 RR프로세스는 실행중인 일반 프로세스를 선점 가능 배치 스케줄링 정책 SCHED_BATCH는 일괄 또는 유휴 스케줄링 정책(실시간 정책과 약간 반되되는 동작) 이 정책을 따르는 프로세스는 다른 프로세스가 타임 슬라이스를 모두 소진했더라도 시스템에 실행 가능한 프로세스가 없을 때만 실행 우선 순위가 더 높은 프로세스가 타임슬라이스를 다 소진하면 결국에 우선 순위가 가장 낮은 프로세스도 실행 가능 리눅스 스케줄링 정책 설정하기 1 2 3 4 5 6 7 8 9 10 #include \u0026lt;sched.h\u0026gt; struct sched_param { /* ... */ int sched_priority; /* ... */ }; int sched_getscheduler(pid_t pid); int sched_setscheduler(pid_t pid, int policy, const struct sched_param *sp); sched_getscheduler()함수는 호출이 성공하면 pid로 지정한 프로세스의 스케줄링 정책 반환(SCHED_FIFO 등) pid가 0일시 호출한 프로세스 스케줄링 정책 반환 호출이 실패 시 -1를 반환하고 errno를 적절한 값으로 설정 sched_setscheduler()를 호출하면 pid로 지정한 프로세스의 스케줄링 정책을 설정 스케줄링 정책은 sp인자를 통해 설정, pid가 0이면 호출한 프로세스 스케줄링 정책을 설정 호출 성공 시 0반혼, 실패시 -1 반환 및 errno를 적절한 값으로 설정(EFAULT, EINVAL, EPERM, ESRCH) sched_param에서 유효한 필드는 스케줄링 정책에 따라 다르며 RR/FIFO는 sched_priority만 유효 SCHED_OTHER외 다른 스케줄 정책을 적용하려면 CAP-SYS_NICE가 필요(대부분 root사용자가 실행) 스케줄링 인자 설정하기 1 2 3 4 5 6 7 8 9 10 #include \u0026lt;sched.h\u0026gt; struct sched_param { /* ... */ int sched_priority; /* ... */ }; int sched_getparam(pid_t pid, struct sched_param *sp); int sched_setparam(pid_t pid, const struct sched_param *sp); sched_getparam() 호출 시 sp인자를 통해 pid로 지정한 프로세스의 스케줄링 설정값 반환 pid가 0일시 호출한 프로세스 스케줄링 정책 반환 호출이 실패 시 -1를 반환하고 errno를 적절한 값으로 설정 sched_setparam() 호출 성공시 pid로 지정한 프로세스의 스케줄링 인자는 sp에 지정한 내용으로 설정 호출 성공 시 0 반환, 실패시 -1 반환 및 errno설정(EFAULT, EINVAL, EPERM, ESRCH) 유효한 우선순위 범위 확인 POSIX는 시스템에 어떤 우선순위 값이 존재하는지 보장하지 않음(min/max 값 사이에 적어도 32개의 우선순위가 존재해야 한다는 것만 명시) 프로그램은 보통 자체 우선순위 값을 두고 이를 운영체제에 제공하는 우선순위에 맵핑하는 방법을 사용 따라서 우선순위 min/max값을 확인할 수 있는 2가지 시스템 콜을 제공 1 2 3 4 #include \u0026lt;sched.h\u0026gt; int sched_get_priority_min(int policy); int sched_get_priority_max(int policy); sched_get_priority_min/max() 시스템콜은 policy 인자에서 명시한 스케줄링 정책과 관련된 최소/최대 우선순위 반환 호출 실패시 -1 반환, errno는 EINVAL sched_rr_get_interval() 1 2 3 4 5 6 7 8 #include \u0026lt;sched.h\u0026gt; struct timespec { time_t tv_sec;/* 나노 초 */ long tv_nsec; /* 나노 초 */ }; int sched_rr_get_interval(pid_t pid, struct timespec *tp); sched_rr_get_interval()은 호출이 성공하면 pid 프로세스에 할당된 타임 슬라이스의 길이를 tp 포인터가 가리키고 있는 timespec 구조체에 저장 호출 성공 시 0, 실패 시 -1 반환 및 errno 설정(EFAULT, EINVAL, ESRCH) POSIX 표준에 따르면 이 함수는 SCHED_RR 프로세스에 대해서만 사용해야 하지만 리눅스에서는 어떤 프로세스의 타임 슬라이스 길이도 반환 실시간 프로세스의 주의점 CPU를 계속 사용하는 루프가 인터럽트나 더 높은 우선순위 실시간 프로세스가 없어도 무한히 실행되지 않도록(시스템 hang 방지) 실시간 프로세스는 시스템에서 가장 비싼 비용으로 실행, 시스템의 다른 부분이 CPU시간을 얻지 못하는 일이 없도록 busy waiting 사용 시 주의(ex. 우선순위 낮은 프로세스의 리소스를 실시간 프로세스가 대기 시) 실시간 프로세스를 개발할 때 그 실시간 프로세스보다 우선순위가 높은 터미널을 하나 열어놓을 것(비상 탈출) util-linux 패키지의 chrt 유틸리티는 쉽게 다른 프로세스의 실시간 속성을 가져오거나 설정할 수 있다. 결정론 실시간 컴퓨팅 환경에서 결정론적 : 주어진 입력이 같다면 항상 같은 결과를 같은 시간안에 도출 최신의 컴퓨터는 여러 계층에 걸친 캐시, 멀티 프로세서, 페이징 스와핑, 멀티태스킹으로 인해 결정론적이지 않음 실시간 어플리케이션은 예측할 수 없는 부분과 최악의 지연을 제한하기 위해 선행폴트 데이터와 메모리락, CPU친화도를 이용 선행폴트 데이터와 메모리 락 페이징과 스와핑은 실시간 프로세스를 망가트릴 수 있는 비결정적인 동작 야기 선행폴트를 일으켜 스와핑된 데이터를 메모리에 올린 다음, 주소공간 내 모든 페이지를 실제 물리 메모리에 락을 걸거나 고정 배선 해버림 페이지가 담겨 있는 모든 메모리를 락 걸고 나면 커널은 절대 이 페이지를 디스크로 스왑하지 않음 4장에서는 데이터를 메모리에 선행 폴트하는 인터페이스(readahead)를, 9장에서는 물리 메모리에 있는 데이터를 락거는 방법 설명 CPU 친화도와 실시간 프로세스 멀티태스킹에서 (비록 선점형이라도) 스케줄러는 항상 특정 프로세스를 위해 다른 프로세스를 즉시 스케줄링 할 수 없다. (다른 프로세스가 크리티컬 섹션에서 동작할 경우 등) 멀티태스킹은 페이징에 관련된 예측 불가능성에 따른 비결정성을 유발 만약 멀트 프로세스 시스템이라면 그 중 하나를 실시간 프로세스에만 할당하면 됨 이를 간단하게 구현하는 것은 init프로그램 수정(책에 코드 설명 있음) 리소스 제한 리눅스 커널은 프로세스에 대해 파일개수, 메모리 페이지, 대기중 시그널 같은 커널 리소스 제한을 도입 커널은 이 제한을 초과하는 리소스 소비를 허용하지 않음(허용 수치를 초과하면 해당 호출 실패) 리소스 제한을 설정할 수 있는 두가지 시스템콜을 제공(getrlimit, setrlimit) 소프트제한은 프로세스 생성 시 기본으로 적용되는 한도, 하드제한은 소프트제한이 최대한 늘릴 수 있는 제한 프로세스는 소프트제한값을 0~하드제한값까지 자유롭게 변경가능, CAP_SYS_RESOURCE 기능이 제한된 프로세스(non-root)는 하드제한보다 낮은 값만을 설정 가능 리소스 제한의 특수 값 0: 리소스 사용 금지, -1 : 무한값 제한 리눅스에는 16가지 리소스 제한이 존재 (책에 설명 있음) 기본제한 프로세스에 적용 가능한 기본 제한은 최초의 소프트제한, 최초의 하드제한, 시스템 관리자라는 세가지 변수에 의존(책에 기본값 표 있음) 실제제한은 일반적으로 시스템 관리자가 여러가지 제한을 설정해둔 사용자의 쉘에 의해 결정 관리자는 이값을 낮출 필요가 없음. 제한 설정과 조회 1 2 3 4 5 6 7 8 9 10 #include \u0026lt;sys/time.h\u0026gt; #include \u0026lt;sys/resource.h\u0026gt; struct rlimit { rlim_t rlim_curr; /* 소프트 제한 */ rlim_t rlim_max; /* 하드 제한 */ } int getrlimit(int resource, struct rlimit *rlim); int setrlimit(int resource, const struct rlimit *rlim); getrlimit()는 resource인자가 나타내는 리소스의 하드제한과 소프트제한을 rlim포인터가 기리키고 있는 구조체에 저장 setrlimit()는 resource인자로 지정한 리소스의 제한을 rlim 포인터가 가리키는 값으로 설정 호출이 성공시 0반환, 실패시 -1반환 및 errno를 설정 (EFAULT, EINVAL, EPERM) ","date":"2023-09-17T00:00:00Z","permalink":"https://muonkmu.github.io/p/linux_system_programming-06-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-%EA%B4%80%EB%A6%AC/","title":"[Linux_System_programming] 06 프로세스 관리"},{"content":"Prologue HW 가속기를 설계함에 있어 HW IP 개발 능력 외 리눅스 디바이스 드라이버 및 리눅스 Application 제작 등 다양한 기술이 요구된다. YOLO IP를 만들던 중 기술적 한계를 느껴 우선 간단한 프로젝트부터 시작해보고자 하였다. 결론적으로 IP에 DMA엔진을 통하여 데이터를 전송하는 부분에서 많이 고생하였고 현재 프로젝트를 완료하고 다른 개인 프로젝트를 진행중이다. Concept Realsense에서 YUYV 640*480 이미지 획득 획득된 이미지를 FPGA영역으로 전송하여 하드웨어 영상 가속 처리 Resize (to 428*204) Color space Conversion (to RGB) Edge detection (Sobel filter) 처리된 이미지를 ROS로 Publish Result CPU 사용률 : A53 1core 15% 이미지 처리 시간(hw) : about 1ms ","date":"2023-09-15T00:00:00Z","permalink":"https://muonkmu.github.io/p/prj_rs_acc-01-project-concept-and-result/","title":"[prj_RS_Acc] 01 project concept and result"},{"content":"유닉스는 바이너리 이미지를 메모리에 적재하는 과정에서 새로운 프로세스를 생성하는 부분을 분리함 이는 fork(), exec()라는 두 개의 시스템 콜을 필요\n프로그램, 프로세스, 스레드 바이너리 : 디스크에 저장되어 있는 컴파일된 실행 할 수 있는 코드 프로세스 : 실행 중인 프로그램 (가상화된 메모리 인스턴스, 커널리소스, 보안정보 및 스레드를 지님) 스레드 : 프로세스 내 실행단위(각 스레드는 가상화된 프로세서, 스택, 레지스터, 명령어 포인터를 가짐) 프로세스 ID (PID) PID는 프레세스의 식별자로 해당 프로세스가 살아있는 동안 유일한 값 idle프로세스의 PID는 0이고 init 프로세스의 PID는 1 사용자가 커널에 명시적으로 요청하지 않으면 커널은 다음 순서로 init프로세스를 확인하여 실행, 실패 시 커널 패닉 /sbin/init -\u0026gt; /etc/init -\u0026gt; /bin/init -\u0026gt; /bin/sh PID 할당 보통 PID의 최대값은 32768이며 /proc/sys/kernel/pid_max로 설정 가능 pid값이 pid_max 값에 도달해서 처음부터 다시 할당하기전까지는 앞선 pid 값이 비어 있더라도 재사용 하지 않음 프로세스 계층 새로운 프로세스를 생성(spawn)하는 프로세스를 부모 프로세스, 새롭게 생성된 프로세스를 자식 프로세스 모든 프로세스는 사용자와 그룹이 소유, 소유란 리소스에 대한 접근권한을 제어하기 위해 사용 모든 프로세스는 다른 프로세스와의 관계를 표한하고 있는 프로세스 그룹의 일부, 자식 프로세스는 부모 프로세스의 프로세스 그룹에 속함 pid_t pid는 pid_t 자료형으로 표현되며 \u0026lt;sys/type.h\u0026gt;에 정의, 보통 int자료형에 대한 typedef 임 프로세스ID와 부모 프로세스 ID 얻기 getpid() 시스템콜은 호출한 프로세스의 pid를 반환 getppid() 시스템 콜은 호출한 프로세스의 부모 프로세스 pid를 반환 새로운 프로세스 실행하기 exec 시스템 콜 : 프로그램 바이너리를 메모리에 적재, 프로세스의 주소공간에 있는 이전 내용 대체 후 새로운 프로그램의 실행 시작 fork 시스템 콜 : 부모 프로세스를 거의 그대로 복제하여 새로운 프로세스를 생성 exec 함수들 1 2 3 4 5 6 7 8 #include \u0026lt;unistd.h\u0026gt; int execl(const char* path, const char* arg, ...); int execlp(const char* file, const char* arg, ...); int execle(const char* path, const char* arg, ..., char* const envp[]); int execv(const char* path, char* const envp[]); int execvp(const char* file, char* const envp[]); int execve(const char* filename, char* const argv[], char* const envp[]); execl 시스템콜을 호출 시 현재 프로세스를 path가 가리키는 프로그램으로 대체 \u0026hellip;은 가변인자로 가변인자의 목록은 반드시 NULL로 끝나야 함 일반적으로 반환 값이 없으나 에러가 발생할 경우 -1을 반환하고 errno를 적절한 값으로 설정 호출이 성공시 다음의 프로세스 속성도 변화 대기중인 시그널은 사라짐 프로세스가 받은 시그널은 시그널 핸들러가 더이상 프로세스의 주소 공간에 존재하지 않으므로 디폴트 방식으로 처리 메모리 락이 해제 스레드의 속성 대부분이 기본값으로 변경 프로세스의 통계 대부분이 재설정 메모리에 매핑된 파일을 포함하여 그 프로세스 메모리 주소 공간과 관련된 내용 사라짐 atexit()의 내용 처럼 사용자 영역에만 존재하는 모든 내용이 사라짐 pid, ppid, 우선순위, 소유자/그룹 속성은 유지 열린 파일은 그대로 상속되나 실제로는 exec 호출 전 파일을 모두 close(), 하거나 fcntl()을 이용하여 커널이 모두 닫도록 하는 것을 많이 사용 시스템 콜 함수는 execve이며 나머지는 래퍼 함수 l \u0026amp; V : 인자를 리스트로 제공해야 하는지 배열(벡터)로 제공해야 하는지 p : file 인자값을 사용자의 실행 경로 환경 변수에서 찾음 fork() 시스템콜 1 2 3 4 #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; pid_t fork(void); fork()는 fork()를 실행한 프로세스(부모)와 거의 모든 내용이 동일한 새로운 프로세스(자식)을 생성 자식 프로세스의 pid는 부모와 다르며 ppid는 부모의 pid 자식 프로세스의 리소스 통계는 0으로 초기화 처리되지 않은 시그널은 모두 사라지고 자식 프로세스로 상속되지 않음 부모 프로세스의 파일 락은 자식에게 상속되지 않음 자식 프로세스에서 fork()시스템콜 반환 값은 0, 부모에서는 자식의 pid 실패시 -1을 반환하고 errno를 적절한 값으로 설정, 실패시 자식프로세스는 생성 안됨 copy-on-write 리눅스는 fork()시 부모 프로세스의 주소 공간을 모두 복사하는 것이 아니라 페이지에 대한 COW를 수행 페이지가 변경될 때 까지 복사를 미루고 페이지가 변경되면 복사 후 쓰기 vfork() fork()와 같은 동작을 하지만 자식프로세스는 즉시 exec계열의 함수를 호출하던가 _exit()함수를 호출해야함 주소공간 복사를 하지 않기 위한 시스템 콜이나 COW 도입 후 잘 안쓰임, 쓰지말자 프로세스 종료하기 1 2 3 #include \u0026lt;stdlib.h\u0026gt; void exit(int status); exit() 호출 시 몇 가지 기본적인 종료 단계를 거쳐 커널이 프로세스 종료 반환값 없음(프로세스가 종료되기 때문에) status 인자는 프로세스의 종료 상태를 나타내기 위한 값으로 쉘 같은 다른 프로그램에서 확인 status \u0026amp; 0377 이 부모프로세스로 반환(ex. exit(EXIT_SUCCESS)) C 라이브러리 프로세스 종료 절차 atexit()나 on_exit()에 등록된 함수를 등록 역순 호출 열려있는 모든 표준 입출력 스트림버퍼 비움 tmpfile()함수를 통해 생성한 임시파일 삭제 위의 절차 종료후 _exit() 시스템콜을 호출(남은 종료 절차를 커널이 처리하도록) 리소스 정로(할당된 메모리, 열린파일, 시스템 V 세마포어) 프로세스를 종료하는 다른 방법 프로그램 끝가지 진행(main 함수 반환) 프로그램 정상 종료 시 exit(0), return 0을 반환하여 명시적으로 종료상태를 반환하는 것이 좋음 시그널 SIGTERM, SIGKILL 송신 커널에게 밉보이기(잘못된 연산 및 세그멘테이션 폴트) atexit 1 2 3 #include \u0026lt;stdlib.h\u0026gt; int atexit(void (*func)(void)); 프로세스의 정상 종료 시 호출할 함수를 등록 시그널에 의한 종료 시 등록된 함수는 미호출 exec 함수 호출 시 등록된 함수 목록 삭제 등록된 함수는 등록된 함수의 역순으로 호출 등록 가능한 함수의 개수는 ATEXIT_MAX 임(32개) 성공 시 0을 반환, 에러 발생 시 -1 반환 SIGCHLD 프로세스 종료 시 커널은 SIGCHLD 시그널을 부모 프로세스로 전송 signal(), sigaction() 시스템 콜을 사용해서 이 시그널을 처리하도록 할 수 있음 자식 프로세스 종료 기다리기 자식프로세스가 부모 프로세스보다 먼저 죽으면 자식 프로세스는 좀비 프로세스로 변경 됨 (종료값 회수를 위해) 좀비 프로세스는 커널 자료 구조만 가지고 있는 프로세스 뼈대 좀비가 된 프로세스는 부모가 자산의 상태를 조사하도록 기다리고 부모가 자신의 정보를 회수한 다음에서야 공식 종료 됨 1 2 3 4 #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; pid_t wait (int *status); wait() 호출 시 종료된 프로세스의 pid를 반환, 자식 프로세스 미종료 시 종료 때까지 블록 에러가 발생한 경우 -1을 반환 후 errno 설정(ECHILD, EINTR) status가 NULL이 아닐 경우 추가 정보가 그 포인터에 저장, 이를 해석하기 위한 매크로를 함께 제공 int WIFEXOTED(status) 등 (책 참조) 특정 프로세스 기다리기 1 2 3 4 #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; pid_t waitpid (pid_t pid, int *status, int options); pid인자는 기다리기를 원하는 프로세스를 지정 \u0026lt; -1 : 절대값 gid 일치, -1 : 모든 프로세스, 0 : 호출한 프로세서와 동일한 프로세스 그룹, 0 \u0026gt; : pid 일치 status인자는 wait()와 같음 option인자는 WNOHANG 등을 OR한 값 (책참조) 반환값은 상태가 변경된 프로세스의 pid, 에러 발생시 -1 반환하고 errno를 변경(ECHILD, EINTR, EINVAL) 좀 더 다양한 방법으로 기다리기 1 2 3 #include \u0026lt;sys/wait.h\u0026gt; pid_t waitid (idtype_t idtype, id_t id, siginfo_t *infop, int options); waitid()는 POSIX의 XSI확장에서 정의하고 리눅스에서 제공 wait()보다 다양한 옵션 및 정보(siginfo_t 등)를 제공하나 리눅스가 아닌 시스템의 이식성을 고려하면 단순한 함수를 쓰는게 낫다 BSD 방식으로 기다리기 1 2 3 4 5 6 7 #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/time.h\u0026gt; #include \u0026lt;sys/resource.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; pid_t wait3 (int *status, int options, struct rusage *rusage); pid_t wait4 (pid_t pid, int *status, int options, struct rusage *rusage); BSD에서 제공하는 독자적인 함수 POSIX에서 정의한 함수가 아니므로 리소스 사용 정보가 매우 중요한 경우에만 사용할 것 새로운 프로세스를 띄운 다음 기다리기 1 2 3 4 #define _XOPEN_SOURCE /* WEXITSTATUS 등을 사용할 경우 */ #include \u0026lt;stdlib.h\u0026gt; int system (const char* command); 새로운 프로세스를 생성하고 종료를 기다르는 동작을 하나로 묶은, 즉 동기식 프로세스 생성 인터페이스 command 인자는 /bin/sh -c 뒤에 붙을 명령으로, command가 NULL이면 /bin/sh가 유효한 경우 0이 아닌 값을 반환(반대 시 0반환) 호출 성공 시 wait()와 같이 그 명령의 상태를 반환, 실행한 명령의 종료코드는 WEXITSTATUS로 알 수 있음 명령을 실행하는 동안 SIGCHLD는 블록되고 SIGINT와 SIGQUIT는 무시됨 좀비프로세스 프로세스가 종료될 때 리눅스 커널은 그 프로세스의 자식 프로세스들을 모두 init프로세스(pid 0)으로 입양 init프로세스는 차례대로 주기적으로 자식 프로세스를 기다리며 오랫동안 좀비 상태로 남아있지 않도록 함 사용자와 그룹 실제, 유효, 저장된 사용자 ID와 그룹 ID 프로세스에 연관된 사용자ID는 4종료 실제 사용자ID : 그 프로세스를 최초로 실행한 사용자의 uid 유효 사용자ID : 그 프로세스가 현재 영향을 미치고 있는 사용자id(접근권한은 이 값을 기준으로 점검) 저장된 사용자ID : 프로세스의 최초 유효 사용자ID, 프로세스가 포크되면 자식프로세스는 부모의 저장된 사용자ID를 상속 프로세스 초기에 실제 사용자ID와 유효 사용자ID는 동일, exec호출 도중 유효 사용자ID는 프로그램 파일을 소유한 사용자ID로 변경 실제 사용자ID는 프로그램을 실제로 실행하는 사용자에서 속한 유효 사용자ID이며 저장된 사용자ID는 exec과정에서 suid 바이너리로 변경되기 전까지 유효한 사용자ID 유효 사용자ID가 가장 중요한 값(자격 검증) 실제, 저장된 사용자, 그룹ID 변경하기 1 2 3 4 5 #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int setuid (uid_t uid); int setgid (uid_t gid); setuid()를 호출 시 현재 프로세스의 유효 사용자 ID를 설정한다 프로세스의 현재 유효 사용자ID가 0(root)일 때 실제 사용자와 저장된 사용자ID 역시 설정됨, root는 uid로 어떤 값이든 사용가능 프로세스의 현재 유효 사용자ID가 0이 아닐 때, 실제 사용자와 저장된 사용자ID만 유효 사용자ID로 설정 가능 성공할 경우 0반환, 실패 시 -1 반환 errno를 변경 (EAGAIN, EPERM) 유효 사용자ID나 유효 그룹 ID 변경하기 1 2 3 4 5 #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int seteuid (uid_t euid); int setegid (uid_t egid); seteuid()를 호출 시 유효 사용자ID를 euid로 설정 root 사용자는 euid 값으로 어떤 값이든 사용 가능 비 root 사용자는 euid 값으로 실제 사용자와 저장된 사용자ID만 유효 사용자ID로 설정 가능 성공할 경우 0반환, 실패 시 -1 반환 errno를 변경 (EAGAIN, EPERM) 비 root사용자의 경우 seteuid()와 setuid()는 동일하게 동작 즉 항상 seteuid()를 사용하는 것이 표준에 맞음 프로세스가 root로 실행된다면 setuid()를 사용하는 것이 합리적 BSD 방식으로 사용자, 그룹 ID 변경하기 BSD는 사용자ID와 그룹ID를 설정할 수 있는 독자적인 인터페이스를 제공(setreuid(), setregid()) HP-UX 방식으로 사용자, 그룹 ID 변경하기 HP-UX도 사용자ID와 그룹ID를 설정할 수 있는 독자적인 인터페이스를 제공(setreuid(), setregid()) 바람직한 사용자/그룹 ID 조작법 비 root 프로세스는 seteuid()를 사용해서 유효 사용자 ID를 변경 root 프로세스는 세가지 사용자ID를 모두 바꾸려면 setuid()를 유효 사용자ID만 임시로 바꾸려고 한다면 seteuid()를 사용 세션과 프로세스 그룹 프로세스 그룹이란 작업 제어 목적으로 하나 이상의 프로세스를 모아 놓은 집합 프로세스 그룹의 주된 속성은 그룹 내 모든 프로세스에게 시그널을 보낼 수 있음 프로세스 그룹은 프로세스 그룹ID(pgid)로 구분되며 이는 프로세스 그룹 리더의 pid와 동일 구성원이 하나라도 남아있다면 프로세스 그룹은 사라지지 안흠(리더가 종료되더라도 프로세스 그룹은 남음) 새로운 사용자가 처음으로 시스템 로그인 시 로그인 프로세스는 사용자 로그인 셸 프로세스 하나로 이루어진 새로운 세션을 생성(로그인 셀은 세션리더로 동작) 세션은 하나 이상의 프로세스 그룹이 들어 있는 집합. 세션 리더의 pid는 세션 ID 세션은 로그인한 사용자 활동을 처리 및 제어터미널(tty)과 사용자 사이를 연결(세션은 대부분 쉘과 관련을 맺음) 세션은 제어 터미널을 둘러싼 로그인을 통합하는 기능 세션 시스템 콜 1 2 3 #include \u0026lt;unistd.h\u0026gt; pid_t setsid(void); setsid()는 새로운 세션 내부에 새로운 프로세스 그룹을 생성하며 호출한 프로세스를 그 세션과 프로세스 그룹의 리더로 함 setsid() 호출이 성공 시 새롭게 생성한 세션의 ID를 반환, 실패시 -1 반환하며 errno를 EPERM으로 설정(호출한 프로세스가 이미 프로세스 그룹 리더) 어떤 프로세스가 프로세스 그룹 리더가 되지 않게 하는 가장 손쉬운 방법은 프로세스를 포크하고 부모 프로세스를 종료한다움 자식프로세스에서 setsid()를 호출 getsid()로 현재 세션ID를 얻을 수 있음 프로세스 그룹 시스템 콜 1 2 3 4 #define _XOPEN_SOURCE 500 #include \u0026lt;unistd.h\u0026gt; pid_t setpgid(pid_t pid, pid_t pgid); setpgid()는 pid인자로 지정한 프로세스의 프로세스 그룹ID를 pgid로 설정 pid가 0이면 현재 프로세스의 프로세스 그룹ID를 변경, pgid인자가 0인경우 pid인자로 지정한 프로세스ID를 프로세스 그룹ID로 설정 setpgid() 호출이 성공 시 0을 반환, 에러 발생시 -1을 반환하고 errno를 설정(EACCES, EINVAL, EPERM, ESRCH) pid로 지정한 프로세스가 해당 시스템콜을 호출하는 프로세스이거나 호출하는 프로세의 자식프로세스이며 아직 exec를 호출하지 않았고 부모프로세스와 동일한 세션일 것 pid로 지정한 프로세스가 세션의 리도가 아닐 것 pgid가 이미 있으면 호출하는 프로세스와 동일한 세션에 속해 있을 것 getpgid()로 프로세스의 프로세스 그룹ID를 얻는 것도 가능 사용되지 않는 프로세스 그룹 관련 함수들 setpgrp()/getpgrp() 같은 오래된 BSD 인터페이스도 있음 데몬 데몬은 백그라운드에서 수행되며 제어 터미널이 없는 프로세스 데몬은 일반적으로 부팅 시에 시작되며 root 혹은 다른 특수한 사용자 계정 권한으로 실행 되어 시스템 수준의 작업을 처리 crond, sshd처럼 편의를 위해 데몬의 이름은 d로 끝나는 경우가 많은데 필수적이거나 보편적인 것은 아님 데몬의 필수조건은 반드시 init의 자식 프로세스여야 하며 터미널과 연결되어 있으면 안됨 데몬이 되는 과정은 다음과 같다, 대부분의 유닉스 시스템은 daemon()함수를 제공하여 하기 과정을 자동화 fork()를 호출해서 데몬이 될 새로운 프로세스 생성 부모 프로세스에서 exit()를 호출해 데몬 프로세스의 부모 프로세스 종료(데몬 프로세스가 프로세스 그룹의 리더가 되지 않도록) setsid()를 호출하여 데몬이 새로운 프로세스 그룹과 세션의 리더가 되도록 함(데몬프로세스가 제어터미널에 연관되지 않도록) chdir()을 사용하여 작업디렉토리를 루트 디렉토리로 변경(임의의 디렉토리가 계속 열린 상태로 실행되어 그 디렉토리를 해제하지 못하는 경우 방지) 모든 파일 디스크립터를 닫음 0,1,2 파일 디스트립터(표준 입력, 출력, 에러)를 열고 /dev/null로 다이렉트한다. ","date":"2023-08-15T00:00:00Z","permalink":"https://muonkmu.github.io/p/linux_system_programming-05-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-%EA%B4%80%EB%A6%AC/","title":"[Linux_System_programming] 05 프로세스 관리"},{"content":"Prologue 기본적인 가속환경 기술 습득을 위해 Realsense에서 이미지를 얻고 영상처리된 데이터를 ROS로 publish 하는 toy 프로젝트를 수행할 예정이다.\nRealsense에서 YUYV(640*480) 이미지를 UVC를 통해서 얻음 이미지 가속기로 데이터 처리(color space conv(YUYV to Gray) -\u0026gt; resize 424*240 -\u0026gt; edge detection(sobel)) 처리된 데이터를 ROS로 출력 현 포스트에서는 상기 2항을 하기위한 IP를 만들고 포팅하는 방법을 기술한다.\nReference vitis_hls design flow https://docs.xilinx.com/r/en-US/ug1399-vitis-hls/Using-Vitis-HLS vitis xo kernel import https://docs.xilinx.com/r/en-US/Vitis-Tutorials-Hardware-Acceleration/Using-the-RTL-Kernel-in-a-Vitis-IDE-Project?tocId=rLszN3XoWFzuz7Fyhq0T1A vitst host programming guide https://docs.xilinx.com/r/en-US/Vitis-Tutorials-Hardware-Acceleration/Host-Code-Programming 개발환경 이전 포스트와 동일(02 KV260_Platform_Setup) vitis_HLS에서 Xilinx vision lib을 사용하기 위해서는 xilinx의 환경에 맞게 컴파일된 openCV 라이브러리가 필요하다. 하기 매뉴엘을 참고하여 별도의 폴더에 openCV를 빌드 및 설치하자 https://github.com/Xilinx/Vitis_Libraries/tree/main/vision Test하기 위한 이미지를 하나 받아 만들어 놓자 1 2 3 v4l2-ctl --set-fmt-video=width=640,height=480,pixelformat=YUYV --stream-mmap --stream-count=1 --device /dev/video4 --stream-to=raw.yuyv (or) gst-launch-1.0 v4l2src device=/dev/video4 num-buffers=1 ! video/x-raw, width=640, height=480, format=YUY2 ! filesink location=rs_cap.yuy2 개발절차 1. HW kernel design with vitis_HLS vitis HLS를 이용하여 kerel을 만들고 이를 .xo 파일로 export 한다.\nsource와 Testbench의 cflag/csimflag에는 개발환경에서 설정한 opencv 헤더의 위치가 포함되어야 한다. source와 Testbench의 cflag/csimflag에는 xilinx vision lib를 다운받은 위치가 포함되어야 한다. link flag에는 개발환경에서 설정한 opencv 라이브러리 위치가 포함되어야 한다. solution에서 클럭주기는 3.3으로 하고 flow target는 vitis kernel flow로 한다. 시뮬레이션을 위해 개발환경에서 생성한 이미지 위치를 input argument에 넣어준다. rs_img_accel_ip_xrt.cpp\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 #include \u0026#34;rs_img_accel_ip_xrt.h\u0026#34; void rs_img_accel(ap_uint\u0026lt;AXI_WIDTH_IN\u0026gt;* srcFrame, ap_uint\u0026lt;AXI_WIDTH_OUT\u0026gt;* dstFrame, int inFrameHeight,int inFrameWidth, int outFrameHeight, int outFrameWidth) { static constexpr int __XF_DEPTH_IN = ((FRAME_HEIGHT_IN) * (FRAME_WIDTH_IN) * (XF_PIXELWIDTH(XF_PIX_TYPE_IN , XF_NPPCX_IN ))) / (AXI_WIDTH_IN); static constexpr int __XF_DEPTH_OUT = ((FRAME_HEIGHT_OUT) * (FRAME_WIDTH_OUT) * (XF_PIXELWIDTH(XF_PIX_TYPE_OUT, XF_NPPCX_OUT))) / (AXI_WIDTH_OUT); #pragma HLS INTERFACE m_axi port=srcFrame offset=slave bundle=gmem1 depth=__XF_DEPTH_IN #pragma HLS INTERFACE m_axi port=dstFrame offset=slave bundle=gmem2 depth=__XF_DEPTH_OUT #pragma HLS INTERFACE s_axilite port=inFrameHeight #pragma HLS INTERFACE s_axilite port=inFrameWidth #pragma HLS INTERFACE s_axilite port=outFrameHeight #pragma HLS INTERFACE s_axilite port=outFrameWidth #pragma HLS INTERFACE s_axilite port=return xf::cv::Mat\u0026lt; XF_PIX_TYPE_YUYV, FRAME_HEIGHT_IN, FRAME_WIDTH_IN, XF_NPPCX_YUYV, XF_CV_DEPTH_YUYV \u0026gt; cvImgYuyv(inFrameHeight, inFrameWidth); xf::cv::Mat\u0026lt; XF_PIX_TYPE_BGR, FRAME_HEIGHT_IN, FRAME_WIDTH_IN, XF_NPPCX_BGR, XF_CV_DEPTH_BGR \u0026gt; cvImgBgr(inFrameHeight, inFrameWidth); xf::cv::Mat\u0026lt; XF_PIX_TYPE_GRAY, FRAME_HEIGHT_IN, FRAME_WIDTH_IN, XF_NPPCX_GRAY, XF_CV_DEPTH_GRAY \u0026gt; cvImgGray(inFrameHeight, inFrameWidth); xf::cv::Mat\u0026lt; XF_PIX_TYPE_RESIZE, FRAME_HEIGHT_OUT, FRAME_WIDTH_OUT, XF_NPPCX_RESIZE, XF_CV_DEPTH_RESIZE \u0026gt; cvImgResize(outFrameHeight, outFrameWidth); xf::cv::Mat\u0026lt; XF_PIX_TYPE_SOBEL, FRAME_HEIGHT_OUT, FRAME_WIDTH_OUT, XF_NPPCX_SOBEL, XF_CV_DEPTH_SOBEL_GX \u0026gt; cvImgSobelGx(outFrameHeight, outFrameWidth); xf::cv::Mat\u0026lt; XF_PIX_TYPE_SOBEL, FRAME_HEIGHT_OUT, FRAME_WIDTH_OUT, XF_NPPCX_SOBEL, XF_CV_DEPTH_SOBEL_GY \u0026gt; cvImgSobelGy(outFrameHeight, outFrameWidth); xf::cv::Mat\u0026lt; XF_PIX_TYPE_SOBEL, FRAME_HEIGHT_OUT, FRAME_WIDTH_OUT, XF_NPPCX_SOBEL, XF_CV_DEPTH_SOBEL_GY \u0026gt; cvImgSobel(outFrameHeight, outFrameWidth); #pragma HLS DATAFLOW xf::cv::Array2xfMat\u0026lt;AXI_WIDTH_IN, XF_PIX_TYPE_IN, FRAME_HEIGHT_IN, FRAME_WIDTH_IN, XF_NPPCX_IN, XF_CV_DEPTH_IN\u0026gt;(srcFrame, cvImgYuyv); xf::cv::yuyv2bgr\u0026lt;XF_PIX_TYPE_YUYV, XF_PIX_TYPE_BGR, FRAME_HEIGHT_IN, FRAME_WIDTH_IN, XF_NPPCX_BGR, XF_CV_DEPTH_YUYV, XF_CV_DEPTH_BGR\u0026gt;(cvImgYuyv, cvImgBgr); xf::cv::bgr2gray\u0026lt;XF_PIX_TYPE_BGR, XF_PIX_TYPE_GRAY, FRAME_HEIGHT_IN, FRAME_WIDTH_IN, XF_NPPCX_GRAY, XF_CV_DEPTH_BGR, XF_CV_DEPTH_GRAY\u0026gt;(cvImgBgr, cvImgGray); xf::cv::resize\u0026lt;RESIZE_INTERPOLATION, XF_PIX_TYPE_RESIZE, FRAME_HEIGHT_IN, FRAME_WIDTH_IN, FRAME_HEIGHT_OUT, FRAME_WIDTH_OUT, XF_NPPCX_RESIZE, RESIZE_MAXDOWNSCALE, XF_CV_DEPTH_GRAY,XF_CV_DEPTH_RESIZE\u0026gt;(cvImgGray, cvImgResize); xf::cv::Sobel\u0026lt;XF_BORDER_CONSTANT, SOBEL_FILTER_SIZE, XF_PIX_TYPE_SOBEL, XF_PIX_TYPE_SOBEL, FRAME_HEIGHT_OUT, FRAME_WIDTH_OUT, XF_NPPCX_SOBEL, XF_USE_URAM, XF_CV_DEPTH_RESIZE, XF_CV_DEPTH_SOBEL_GX, XF_CV_DEPTH_SOBEL_GX\u0026gt;(cvImgResize, cvImgSobelGx, cvImgSobelGy); xf::cv::add\u0026lt;XF_CONVERT_POLICY_SATURATE, XF_PIX_TYPE_SOBEL, FRAME_HEIGHT_OUT, FRAME_WIDTH_OUT, XF_NPPCX_SOBEL\u0026gt;(cvImgSobelGx, cvImgSobelGy, cvImgSobel); xf::cv::xfMat2Array\u0026lt;AXI_WIDTH_OUT, XF_PIX_TYPE_OUT, FRAME_HEIGHT_OUT, FRAME_WIDTH_OUT, XF_NPPCX_OUT, XF_CV_DEPTH_OUT\u0026gt;(cvImgSobel, dstFrame); } rs_img_accel_ip_xrt.h\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #ifndef _RS_IMG_ACCEL_IP_XRT_H_ #define _RS_IMG_ACCEL_IP_XRT_H_ #include \u0026#34;hls_stream.h\u0026#34; #include \u0026#34;ap_int.h\u0026#34; #include \u0026#34;common/xf_common.hpp\u0026#34; #include \u0026#34;common/xf_utility.hpp\u0026#34; #include \u0026#34;common/xf_infra.hpp\u0026#34; #include \u0026#34;imgproc/xf_resize.hpp\u0026#34; #include \u0026#34;imgproc/xf_cvt_color.hpp\u0026#34; #include \u0026#34;imgproc/xf_cvt_color_1.hpp\u0026#34; #include \u0026#34;imgproc/xf_sobel.hpp\u0026#34; #include \u0026#34;core/xf_arithm.hpp\u0026#34; #include \u0026#34;rs_img_accel_ip_xrt_config.h\u0026#34; /* In/Out stream data width calculation */ #define _DATA_WIDTH_(_T, _N) (XF_PIXELWIDTH(_T, _N) * XF_NPIXPERCYCLE(_N)) #define _BYTE_ALIGN_(_N) ((((_N) + 7) / 8) * 8) #define DATA_WIDTH_IN _DATA_WIDTH_(XF_PIX_TYPE_IN, XF_NPPCX_IN) #define DATA_WIDTH_OUT _DATA_WIDTH_(XF_PIX_TYPE_OUT, XF_NPPCX_OUT) #define AXI_WIDTH_IN _BYTE_ALIGN_(DATA_WIDTH_IN) #define AXI_WIDTH_OUT _BYTE_ALIGN_(DATA_WIDTH_OUT) void rs_img_accel(ap_uint\u0026lt;AXI_WIDTH_IN\u0026gt;* srcFrame, ap_uint\u0026lt;AXI_WIDTH_OUT\u0026gt;* dstFrame, int inFrameHeight,int inFrameWidth, int outFrameHeight, int outFrameWidth); #endif //_RS_IMG_ACCEL_IP_XRT_H_ rs_img_accel_ip_xrt_config.h\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #ifndef _RS_IMG_ACCEL_CONFIG_H_ #define _RS_IMG_ACCEL_CONFIG_H_ #define FRAME_WIDTH_IN (640) #define FRAME_HEIGHT_IN (480) #define FRAME_WIDTH_OUT (424) #define FRAME_HEIGHT_OUT (240) /* pixel type value*/ #define XF_PIX_TYPE_YUYV (XF_16UC1) //16 #define XF_PIX_TYPE_BGR (XF_8UC3) //8 #define XF_PIX_TYPE_GRAY (XF_8UC1) //8 #define XF_PIX_TYPE_RESIZE (XF_PIX_TYPE_GRAY) #define XF_PIX_TYPE_SOBEL (XF_8UC1) #define XF_PIX_TYPE_IN (XF_PIX_TYPE_YUYV) #define XF_PIX_TYPE_OUT (XF_PIX_TYPE_SOBEL) /* Pixel Per Cycle */ #define XF_NPPCX_YUYV (XF_NPPC1) #define XF_NPPCX_BGR (XF_NPPC1) #define XF_NPPCX_GRAY (XF_NPPC1) #define XF_NPPCX_RESIZE (XF_NPPC1) #define XF_NPPCX_SOBEL (XF_NPPC1) #define XF_NPPCX_IN (XF_NPPCX_YUYV) #define XF_NPPCX_OUT (XF_NPPCX_SOBEL) /* CV depth config */ #define XF_CV_DEPTH_YUYV 2 #define XF_CV_DEPTH_BGR 2 #define XF_CV_DEPTH_GRAY 2 #define XF_CV_DEPTH_RESIZE 2 #define XF_CV_DEPTH_SOBEL_GX 2 #define XF_CV_DEPTH_SOBEL_GY 2 #define XF_CV_DEPTH_IN (XF_CV_DEPTH_YUYV) #define XF_CV_DEPTH_OUT (XF_CV_DEPTH_SOBEL_GY) /* Resize config Parameters */ #define RESIZE_MAXDOWNSCALE 2 #define RESIZE_INTERPOLATION 1 /* Sobel config Parameters */ #define SOBEL_FILTER_SIZE 3 #define XF_USE_URAM false #endif //_RS_IMG_ACCEL_CONFIG_H_ rs_img_accel_ip_xrt_tb.cpp\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 #define SW_TEST_ONLY 0 #include \u0026#34;stdio.h\u0026#34; #if SW_TEST_ONLY #include \u0026#34;opencv2/opencv.hpp\u0026#34; #include \u0026#34;opencv2/imgproc/imgproc.hpp\u0026#34; #include \u0026#34;opencv2/highgui/highgui.hpp\u0026#34; #include \u0026#34;opencv2/imgcodecs/imgcodecs.hpp\u0026#34; #include \u0026#34;opencv2/video/video.hpp\u0026#34; #include \u0026#34;rs_img_accel_config.h\u0026#34; #else #include \u0026#34;common/xf_headers.hpp\u0026#34; #include \u0026#34;common/xf_axi.hpp\u0026#34; #include \u0026#34;rs_img_accel_ip_xrt.h\u0026#34; #endif #define CV_SIM_PARAM_SOBEL_DEPTH CV_8U #define CV_SIM_PARAM_SOBEL_SCALE 1 #define CV_SIM_PARAM_SOBEL_DELTA 0 #define RESULT_PARAM_ERR_DIFF_TRESHOLD 5 #define RESULT_PARAM_ERR_PERCENT 3.0f // #define CANNY_LOW_TRESHOLD 30 // #define CANNY_HIGH_TRESHOLD 64 int main(int argc, char** argv) { #if SW_TEST_ONLY cv::Mat inImg, outSwLibImg; #else cv::Mat inImg, outHwAccImg, outSwLibImg, errDiffImg; #endif char inImgBuff[FRAME_HEIGHT_IN*FRAME_WIDTH_IN*2]; /* argument check */ if (argc != 2) { fprintf(stderr, \u0026#34;Invalid Number of Arguments!\\nUsage: \u0026lt;executable\u0026gt; \u0026lt;image\u0026gt;\\n\u0026#34;); return -1; } /* RAW file(YUYV) open and Input generation*/ FILE* srcFileStream = fopen(argv[1], \u0026#34;rb\u0026#34;); fread(inImgBuff, sizeof(char), FRAME_HEIGHT_IN*FRAME_WIDTH_IN*2, srcFileStream); fclose(srcFileStream); inImg = cv::Mat(FRAME_HEIGHT_IN, FRAME_WIDTH_IN, CV_8UC2, inImgBuff); /* Run OpenCV image converting, make Golden result */ cv::Mat tmpSwLibBgrImg, tmpSwLibGrayImg, tmpSwLibResizeImg, tmpSwLibSobelGxImg, tmpSwLibSobelGyImg; cv::cvtColor(inImg, tmpSwLibBgrImg, cv::COLOR_YUV2BGR_YUYV, 0); cv::cvtColor(tmpSwLibBgrImg, tmpSwLibGrayImg, cv::COLOR_BGR2GRAY, 0); cv::resize(tmpSwLibGrayImg, tmpSwLibResizeImg, cv::Size(FRAME_WIDTH_OUT, FRAME_HEIGHT_OUT), 0, 0, cv::INTER_LINEAR); cv::Sobel(tmpSwLibResizeImg, tmpSwLibSobelGxImg, CV_SIM_PARAM_SOBEL_DEPTH, 1, 0, SOBEL_FILTER_SIZE, CV_SIM_PARAM_SOBEL_SCALE, CV_SIM_PARAM_SOBEL_DELTA, cv::BORDER_CONSTANT); cv::Sobel(tmpSwLibResizeImg, tmpSwLibSobelGyImg, CV_SIM_PARAM_SOBEL_DEPTH, 0, 1, SOBEL_FILTER_SIZE, CV_SIM_PARAM_SOBEL_SCALE, CV_SIM_PARAM_SOBEL_DELTA, cv::BORDER_CONSTANT); outSwLibImg = tmpSwLibSobelGxImg + tmpSwLibSobelGyImg; #if SW_TEST_ONLY cv::imshow(\u0026#34;outSwLibImg\u0026#34;, outSwLibImg); cv::waitKey(0); #endif cv::imwrite(\u0026#34;outSwLibImg.png\u0026#34;, outSwLibImg); #if !(SW_TEST_ONLY) /* Run HW Accelerator */ outHwAccImg = cv::Mat(FRAME_HEIGHT_OUT, FRAME_WIDTH_OUT, CV_8UC1); // Launch the kernel rs_img_accel((ap_uint\u0026lt;AXI_WIDTH_IN\u0026gt;*)inImg.data, (ap_uint\u0026lt;AXI_WIDTH_OUT\u0026gt;*)outHwAccImg.data, FRAME_HEIGHT_IN, FRAME_WIDTH_IN, FRAME_HEIGHT_OUT, FRAME_WIDTH_OUT); cv::imwrite(\u0026#34;outHlsIp.png\u0026#34;, outHwAccImg); /* Compare HW ans SW Result */ float errPer; cv::absdiff(outSwLibImg, outHwAccImg, errDiffImg); xf::cv::analyzeDiff(errDiffImg, RESULT_PARAM_ERR_DIFF_TRESHOLD, errPer); cv::imwrite(\u0026#34;errDiffImg.png\u0026#34;, errDiffImg); if (errPer \u0026gt; RESULT_PARAM_ERR_PERCENT) { fprintf(stderr, \u0026#34;ERROR: Test Failed.\\n \u0026#34;); return -1; } else std::cout \u0026lt;\u0026lt; \u0026#34;Test Passed \u0026#34; \u0026lt;\u0026lt; std::endl; #endif return 0; } 2. HW kernel import and test SW design with vitis 이전 포스트에서 만들어 놓은 vitis 개발 환경에 새로운 커널을 import하고 테스트용 HOST sw를 개발한다.\nvitis 환경에서 기존에 생성한 platform에 신규 application 프로젝트를 생성한다 이전에 생성한 플랫폼과 sysroot 활용 프로젝트는 Empty Application(XRT Native API\u0026rsquo;s) 하위 kernels 프로젝트에tj Hardware Functions에 Top Function을 포팅한 커널로 설정하자 Host program은 raw이미지를 한장 받아서 가속 처리 된 이미지 파일을 생성한다. 헤더 파일\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #pragma once #define CL_HPP_CL_1_2_DEFAULT_BUILD #define CL_HPP_TARGET_OPENCL_VERSION 120 #define CL_HPP_MINIMUM_OPENCL_VERSION 120 #define CL_HPP_ENABLE_PROGRAM_CONSTRUCTION_FROM_ARRAY_COMPATIBILITY 1 #include \u0026lt;CL/cl2.hpp\u0026gt; //Customized buffer allocation for 4K boundary alignment template \u0026lt;typename T\u0026gt; struct aligned_allocator { using value_type = T; T* allocate(std::size_t num) { void* ptr = nullptr; if (posix_memalign(\u0026amp;ptr,4096,num*sizeof(T))) throw std::bad_alloc(); return reinterpret_cast\u0026lt;T*\u0026gt;(ptr); } void deallocate(T* p, std::size_t num) { free(p); } }; 소스파일\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 #include \u0026#34;rs_img_accel_app.h\u0026#34; #include \u0026#34;opencv2/opencv.hpp\u0026#34; #include \u0026#34;opencv2/imgproc/imgproc.hpp\u0026#34; #include \u0026#34;opencv2/highgui/highgui.hpp\u0026#34; #include \u0026#34;opencv2/imgcodecs/imgcodecs.hpp\u0026#34; #include \u0026#34;opencv2/video/video.hpp\u0026#34; #include \u0026lt;fstream\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #define OCL_CHECK(error, call) \\ call; \\ if (error != CL_SUCCESS) { \\ printf(\u0026#34;%s:%d Error calling \u0026#34; #call \u0026#34;, error code is: %d\\n\u0026#34;, __FILE__, __LINE__, error); \\ exit(EXIT_FAILURE); \\ } #define FRAME_WIDTH_IN (640) #define FRAME_HEIGHT_IN (480) #define FRAME_WIDTH_OUT (424) #define FRAME_HEIGHT_OUT (240) int main(int argc, char* argv[]) { // TARGET_DEVICE macro needs to be passed from gcc command line if (argc != 3) { std::cout \u0026lt;\u0026lt; \u0026#34;Usage: \u0026#34; \u0026lt;\u0026lt; argv[0] \u0026lt;\u0026lt; \u0026#34; \u0026lt;xclbin\u0026gt;\u0026#34; \u0026lt;\u0026lt; \u0026#34; \u0026lt;yuyv image\u0026gt;\u0026#34;\u0026lt;\u0026lt; std::endl; return EXIT_FAILURE; } size_t img_in_byte_size = FRAME_HEIGHT_IN*FRAME_WIDTH_IN*2*sizeof(unsigned char); size_t img_out_byte_size = FRAME_HEIGHT_OUT*FRAME_WIDTH_OUT*sizeof(unsigned char); std::string xclbinFilename = argv[1]; std::string imgFilename = argv[2]; //prepare input img CV_MAT char inImgBuff[img_in_byte_size]; FILE* srcFileStream = fopen(argv[2], \u0026#34;rb\u0026#34;); fread(inImgBuff, sizeof(char), img_in_byte_size, srcFileStream); fclose(srcFileStream); cv::Mat inImg = cv::Mat(FRAME_HEIGHT_IN, FRAME_WIDTH_IN, CV_8UC2, inImgBuff); cv::Mat outHwAccImg = cv::Mat(FRAME_HEIGHT_OUT, FRAME_WIDTH_OUT, CV_8UC1); std::vector\u0026lt;cl::Device\u0026gt; devices; cl_int err; cl::Context context; cl::CommandQueue q; cl::Kernel krnl; cl::Program program; std::vector\u0026lt;cl::Platform\u0026gt; platforms; bool found_device = false; // traversing all Platforms To find Xilinx Platform and targeted // Device in Xilinx Platform cl::Platform::get(\u0026amp;platforms); for (size_t i = 0; (i \u0026lt; platforms.size()) \u0026amp; (found_device == false); i++) { cl::Platform platform = platforms[i]; std::string platformName = platform.getInfo\u0026lt;CL_PLATFORM_NAME\u0026gt;(); if (platformName == \u0026#34;Xilinx\u0026#34;) { devices.clear(); platform.getDevices(CL_DEVICE_TYPE_ACCELERATOR, \u0026amp;devices); if (devices.size()) { found_device = true; break; } } } if (found_device == false) { std::cout \u0026lt;\u0026lt; \u0026#34;Error: Unable to find Target Device \u0026#34; \u0026lt;\u0026lt; std::endl; return EXIT_FAILURE; } std::cout \u0026lt;\u0026lt; \u0026#34;INFO: Reading \u0026#34; \u0026lt;\u0026lt; xclbinFilename \u0026lt;\u0026lt; std::endl; FILE* fp; if ((fp = fopen(xclbinFilename.c_str(), \u0026#34;r\u0026#34;)) == nullptr) { printf(\u0026#34;ERROR: %s xclbin not available please build\\n\u0026#34;, xclbinFilename.c_str()); exit(EXIT_FAILURE); } // Load xclbin std::cout \u0026lt;\u0026lt; \u0026#34;Loading: \u0026#39;\u0026#34; \u0026lt;\u0026lt; xclbinFilename \u0026lt;\u0026lt; \u0026#34;\u0026#39;\\n\u0026#34;; std::ifstream bin_file(xclbinFilename, std::ifstream::binary); bin_file.seekg(0, bin_file.end); unsigned nb = bin_file.tellg(); bin_file.seekg(0, bin_file.beg); char* buf = new char[nb]; bin_file.read(buf, nb); // Creating Program from Binary File cl::Program::Binaries bins; bins.push_back({buf, nb}); bool valid_device = false; for (unsigned int i = 0; i \u0026lt; devices.size(); i++) { auto device = devices[i]; // Creating Context and Command Queue for selected Device OCL_CHECK(err, context = cl::Context(device, nullptr, nullptr, nullptr, \u0026amp;err)); OCL_CHECK(err, q = cl::CommandQueue(context, device, CL_QUEUE_PROFILING_ENABLE, \u0026amp;err)); std::cout \u0026lt;\u0026lt; \u0026#34;Trying to program device[\u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;]: \u0026#34; \u0026lt;\u0026lt; device.getInfo\u0026lt;CL_DEVICE_NAME\u0026gt;() \u0026lt;\u0026lt; std::endl; cl::Program program(context, {device}, bins, nullptr, \u0026amp;err); if (err != CL_SUCCESS) { std::cout \u0026lt;\u0026lt; \u0026#34;Failed to program device[\u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;] with xclbin file!\\n\u0026#34;; } else { std::cout \u0026lt;\u0026lt; \u0026#34;Device[\u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;]: program successful!\\n\u0026#34;; OCL_CHECK(err, krnl = cl::Kernel(program, \u0026#34;rs_img_accel\u0026#34;, \u0026amp;err)); valid_device = true; break; // we break because we found a valid device } } if (!valid_device) { std::cout \u0026lt;\u0026lt; \u0026#34;Failed to program any device found, exit!\\n\u0026#34;; exit(EXIT_FAILURE); } OCL_CHECK(err, cl::Buffer imageToDevice(context, CL_MEM_READ_ONLY, img_in_byte_size, NULL, \u0026amp;err)); OCL_CHECK(err, cl::Buffer imageFromDevice(context, CL_MEM_WRITE_ONLY, img_out_byte_size, NULL, \u0026amp;err)); OCL_CHECK(err, err = krnl.setArg(0, imageToDevice)); OCL_CHECK(err, err = krnl.setArg(1, imageFromDevice)); OCL_CHECK(err, err = krnl.setArg(2, FRAME_HEIGHT_IN)); OCL_CHECK(err, err = krnl.setArg(3, FRAME_WIDTH_IN)); OCL_CHECK(err, err = krnl.setArg(4, FRAME_HEIGHT_OUT)); OCL_CHECK(err, err = krnl.setArg(5, FRAME_WIDTH_OUT)); /* Copy input vectors to memory */ OCL_CHECK(err, q.enqueueWriteBuffer(imageToDevice, // buffer on the FPGA CL_TRUE, // blocking call 0, // buffer offset in bytes img_in_byte_size, // Size in bytes inImg.data)); // Pointer to the data to copy // Profiling Objects cl_ulong start = 0; cl_ulong end = 0; double diff_prof = 0.0f; cl::Event event_sp; // Execute the kernel: OCL_CHECK(err, err = q.enqueueTask(krnl, NULL, \u0026amp;event_sp)); clWaitForEvents(1, (const cl_event*)\u0026amp;event_sp); event_sp.getProfilingInfo(CL_PROFILING_COMMAND_START, \u0026amp;start); event_sp.getProfilingInfo(CL_PROFILING_COMMAND_END, \u0026amp;end); diff_prof = end - start; std::cout \u0026lt;\u0026lt; (diff_prof / 1000000) \u0026lt;\u0026lt; \u0026#34;ms\u0026#34; \u0026lt;\u0026lt; std::endl; // Copying Device result data to Host memory OCL_CHECK(err, q.enqueueReadBuffer(imageFromDevice, // This buffers data will be read CL_TRUE, // blocking call 0, // offset img_out_byte_size, outHwAccImg.data)); // Data will be stored here q.finish(); cv::imwrite(\u0026#34;outHlsIp.png\u0026#34;, outHwAccImg); } ","date":"2023-08-13T00:00:00Z","permalink":"https://muonkmu.github.io/p/prj_rs_acc-03-image-accelerator-porting/","title":"[prj_RS_Acc] 03 Image Accelerator Porting"},{"content":"Prologue 우선 가격의 문제로 보드는 KV260 보드를 사용하기로 하였고 Depth camera인 realsense를 사용할 예정이다. 우선 가속기를 설계하기 전 이미지를 받고 이를 처리할 수 있는 IP를 만들어 테스트를 프로젝트를 시도하였다. 처음엔 설계의 유연성을 가질 수 있도록(xilinx 개발환경에서 dependency를 줄이는) 시도한 방법은 다음과 같으나 실패하였다. vitis_HLS + xilinx vision library를 이용 IP 만들기 이때 IP는 AXIvideo2xfmat을 이용해서 VDMA를 통하여 데이터를 이동하도록 구성(axi stream video) vivado에서 만들어진 HLS IP와 VDMA를 이용하여 PL 부분을 구성 petalinux를 통해 BOOT.bin과 rootfs를 만들고 KV260 펌업 및 SD카드 부팅 DMA proxy driver(dma engine wrapper)를 구성하여 데이터 전송 https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/1027702787/Linux+DMA+From+User+Space+2.0 우선 제작한 IP를 붙이지 않고 VDMA loopback test를 xilinx에서 제공한 코드를 이용해서 구현해도 테스트에 실패한다. https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/18842337/Linux+Soft+DMA+Driver 현재 Xilinx 포럼에 물어봐도 회신이 없으므로 결국 Xilinx에서 최근에 권장하는 방법인 Vitis acceleration flow를 이용하자. https://support.xilinx.com/s/question/0D54U00007B8X2QSAV Reference vitis acceleration flow based on customize kira som https://docs.xilinx.com/r/en-US/Vitis-Tutorials-Vitis-Platform-Creation/Custom-Kria-SOM-Platform-Creation-Example https://www.hackster.io/whitney-knitter/vitis-acceleration-flow-on-kv260-extensible-vitis-platform-c3f947 https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/18841645/Solution+Zynq+PL+Programming+With+FPGA+Manager 개발환경 xilnx IDE는 22.1의 경우 Vitis HLS가 오동작하는 경우가 많았고 22.2의 경우 공식문서에 지원하지 않는다고 나와 있다.(실제로 해봐도 부팅이 안됨) xilnx IDE는 23.1을 선택하였다. 빌드 시 무언가 조금 불안하다.\nHost : ubuntu20.04(x64), vitis 2023.1, petalinux 2023.1 upgrade 1 target : kv260 개발절차 1. Vivado Hardware Design 커널을 구동할 기본 하드웨어를 구성한다.상기 공식 레퍼런스 매뉴얼 대로 진행하면 큰 문제 없다. 매뉴얼과 다른 부분은 하기와 같다.\n클럭에 300Mhz를 추가후 이를 Default로 설정(가속 IP를 300Mhz로 구동 예정이다.) vivao에서 기본적으로 checkpoint기능을 이용한 incremental synthesis 기능을 사용하는데 증분 합성 시 (합성이 2번이상 수행된 프로젝트에서)는 vitis의 V++ 컴파일러가 이를 정상적으로 인식하지 못한다. 상기 기능을 꺼주자 Settings(on Flow Navigator) -\u0026gt; Project Settings -\u0026gt; Synthesis -\u0026gt; Incremental synthesis -\u0026gt; Disable incremental synthesis 2. Petalinux 커널을 구동할 기본 하드웨어를 구성한다.상기 공식 레퍼런스 매뉴얼에서 정상적인 사용을 위해서는 몇가지를 수정해야 한다. 절차는 하기와 같다.\nxilinx에서 제공하는 KV260으로 프로젝트 생성 후 vivado에서 생성한 xsa파일 임포트 프로젝트 설정(petalinx-config)에서 다음을 수정 FPGA manger를 enable (나중에 Acceleration 패키지 그룹을 설치하면 자동으로 활성화 되기도 함) device tree overlay 기능 활성화 (DTG setting -\u0026gt; Devicetree overlay),(안하면 나중에 가속 기능 사용 못함) rootfs를 SD카드에서 사용하기위해 EXT4로 변경(Image Packaging Configuration -\u0026gt; Root filesystem type -\u0026gt; EXT4) SD카드 디바이스노드 변경 (Image Packaging Configuration -\u0026gt; Device node of SD device -\u0026gt; /dev/mmcblk1p2) kv260에서 sd카드는 mmcblk1이고 eMMC가 mmcblk0이다. (필요시) rootfs의 용량이 2GB를 넘어가면 cpio 압축 안된다. rootfs 포맷에서 cpio관련 내용을 삭제하자 (Image Packaging Configuration -\u0026gt; Root Filesystem formats) rootfs에서 ROS와 커널소스를 추가할 수 있도록 다음을 설정 {petalinux_project_root}/project_spec/meta-user/conf/user-rootfsconfg 파일에 CONFIG_kernel-devsrc, CONFIG_packagegroup-petalinux-ros, CONFIG_packagegroup-petalinux-ros-dev 추가 rootfs 설정에서 다음을 추가(petalinxu-config -c rootfs) packagegrup-petalinux-vitis-acceleration-essentail 과 dev 패키지 xrt, zocl, dnf, resize-part \u0026amp; resize2fs, opencl-headers, x11 packagegroup, gdb, valgrind 등이 설치됨 sysroot에서 사용하기 위해 dev 패키지도 같이 설치 그 외 필요한 패키지를 설치 필자의 경우 vim, gsstreamer, openamp, opencv, python-module, self-host, v4lutils, x11, qt, qt-extended qt 관련 패키지들 컴파일이 오래걸리므로 꼭 필요한 경우에만 설치 kernel 소스와 ROS를 user package에서 추가 FPGA manager를 위한 커널 모듈 확인(defualt로 enable 되어 있다) Device Drivers -\u0026gt; FPGA Configuration Framework -\u0026gt; FPGAM Region / FPGA Bridge Framework Device Drivers \u0026ndash;\u0026gt; Device Tree and Open Firmware support -\u0026gt; Device Tree Overlays / Device Tree Overlays ConfigFS interface 빌드 및 sysroot 생성 (petalinux-build, petalinux-build \u0026ndash;sdk) 부트이미지 생성 (BOOT.bin) 혹시 fpga pl을 포함하고 싶다면 \u0026ndash;fpga 옵션 추가 1 petalinux-package --boot --u-boot --force kv260 board firmware update 보드 fwup 버튼 누른 후 부팅(전원 인가) 후 192.168.0.111 접속, 관리페이지에서 펌업 가능 이미지 영역이 2개이므로 B영역에 펌업을 했다면 부팅 영역을 B영역으로 설정하자. SD 카드 준비 방법은 2가지이다. 이미지 파일을 만들어서 자동으로 굽거나 수동으로 SD카드에 파티션을 나눈 뒤 boot파티션 과 rootfs파티션에 해당 파일을 넣는 것(첫번째만 설명) 1 petalinux-package --wic --images-dir images/linux/ --bootfiles \u0026#34;boot.scr,Image,system.dtb,system-zynqmp-sck-kv-g-revB.dtb\u0026#34; --disk-name \u0026#34;mmcblk1\u0026#34; 생성된 wic 파일을 툴을 이용하여 굽자 부팅 확인 및 파티션 확장 리눅스 부팅이 정상적으로 되는지 확인 SD이미지를 통해 SD카드를 준비했다면 root파티션이 4GB로 고정된다. 다음을 통해 확장하자. sudo parted /dev/mmcblk1 resizepart 2, sudo resize2fs /dev/mmcblk1p2, reboot 3. device tree overlay 컴파일 매뉴얼 대로 따라하자\n4. vitis 플랫폼 생성 매뉴얼 대로 따라하자\n5. (option) vadd를 통한 테스트 vitis acceleration flow가 구동되는지 확인하기 위해 수행해보자(매뉴얼 참조)\n6. (option) librealsense 설치 필자는 향후 Depth image 처리를 위해서 realsense 카메라를 사용할 예정이므로 Target에 librealsense를 설치하자\nhttps://github.com/IntelRealSense/librealsense/blob/master/doc/installation.md librealsense source clone udev rule 적용 빌드 및 설치 (빌드 시 petalinux kernel의 uvc를 사용하면 커널 폴트가 일어난다. 리얼센스에서 제공하는 usb backend를 사용하자) 1 2 3 4 5 6 git clone https://github.com/IntelRealSense/librealsense.git \u0026amp;\u0026amp; cd librealsense ./scripts/setup_udev_rules.sh echo \u0026#39;hid_sensor_custom\u0026#39; | sudo tee -a /etc/modules mkdir build \u0026amp;\u0026amp; cd build cmake ../ -DCMAKE_BUILD_TYPE=Release -DFORCE_RSUSB_BACKEND=true -DBUILD_EXAMPLES=false -DBUILD_GRAPHICAL_EXAMPLES=false sudo make uninstall \u0026amp;\u0026amp; make clean \u0026amp;\u0026amp; make \u0026amp;\u0026amp; sudo make install ","date":"2023-08-08T00:00:00Z","permalink":"https://muonkmu.github.io/p/prj_rs_acc-02-kv260_platform_setup/","title":"[prj_RS_Acc] 02 KV260_Platform_Setup"},{"content":"벡터 입출력 벡터 입출력은 한번의 시스템콜을 사용해서 여러개의 버퍼 벡터에 쓰거나 여러 개의 버퍼 벡터로 읽어 들일때 사용 2장의 표준 읽기 쓰기는 선형 입출력이라 하며 이에 비해 다음의 장점이 있다. 여러 필드에 걸쳐 데이터가 분리되어 있다면 직관적인 방법으로 조작 가능 여러번의 선형입출력 대체 가능하므로 효율적 시스템콜의 횟수를 줄이고 선형입출력 구현에 비해 최적화 되어 효율적 선형 입출력과 대조적으로 벡터 입출력 연산은 원자성을 보장 readv()와 writev() 1 2 3 4 5 6 7 8 9 #include \u0026lt;sys/uio.h\u0026gt; struct iovec{ void* iov_base; //버퍼의 시작포인터 size_t iov_len; //버퍼의 크기(바이트) }; size_t readv(int fd, const struct iovec* iov, int count); size_t writev(int fd, const struct iovec* iov, int count); readv()는 파일 디스크립터 fd에서 데이터를 읽어서 count 개수만큼 iov버퍼에 저장 writev()는 count 개수의 iov버퍼 데이터를 파일 디스크립터 fd에 씀 iovec구조체는 세그멘트라고 하는 독립적으로 분리된 버퍼, 세그멘테이션 집합을 벡터라 부름 readv(), writev()는 성공 시 읽거나 쓴 바이트 개수 반환, 에러 발생 시 -1을 반환 후 errno를 설정 epoll 커널 2.6에서 poll과 select의 한계를 극복하기 위한 epoll 도입 poll과 select가 실행시 마다 전체 파일 디스크립터를 요구하는 문제, epoll은 실제 검사할 파일디스크립터를 등록 부분을 분리 epoll 인스턴스 생성 1 2 3 4 #include \u0026lt;sys/epoll.h\u0026gt; int epoll_create1(int flag) // epoll_create는 구식 방법임 epoll 인스턴스 생성 및 그 인스턴스와 연관된 파일 디스크립터를 반환, 에러 발생시 -1 반환 후 errno를 설정 flag는 EPOLL_CLOSEXEC만 유효, 새 프로세스가 실행될 때 이 파일을 자동적으로 닫아 줌 epoll_create1에서 반환되는 파일 디스크립터는 사용 완료후 close()로 닫아 줄 것 epoll 제어 1 2 3 4 5 6 7 8 9 10 11 12 13 #include \u0026lt;sys/epoll.h\u0026gt; struct epoll_event { __u32 event; union { void* ptr; int fd; __u32 u32; __u64 u64; } data; } int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); epoll context에 파일 디스크립터를 추가하거나 삭제할때 사용, 호출이 성공하면 0을 반환, 실패시 -1을 반환 후 errno를 설정 epfd : 호출이 성공시 epoll 인스턴스는 epfd 와 연결 op : 파일드스크립터 fd가 가리키는 파일에 대한 작업을 명시 EPOLL_CTL_ADD 감시 추가, EPOLL_CTL_DEL 감시 삭제, EPOLL_CTL_MOD 감시 이벤트 갱신 epoll_event 구조체 event 필드는 감시할 이벤트 목록으로 OR연산으로 묶을 수 있음 EPOLLIN 지연되지 않고 읽기 가능 감시, EPOLLOUT 지연되지 않고 파일 쓰기 가능 감시, EPOLLPRI 읽어야할 OOB데이터 존재 여부 감시 EPOLLERR 에러 상황 감시(default), EPOLLHUP 행업 감시(default) EPOLLET 감시 시 에지 트리거 사용, EPOLLONESHOT 한번 만 감시(다시 활성화 하려면 EPOLL_CTL_MOD로 다시 설정 필요) epoll_evnet 구조체 data 필드는 사용자를 위한 필드로 이벤트가 발생해서 사용자에게 반환될때 함께 반환, 보통 data.fd를 fd로 채워씀 epoll 이벤트 대기 1 2 3 #include \u0026lt;sys/epoll.h\u0026gt; int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); epoll_wait()를 호출하면 timeout 밀리 초 동안 epfd와 연관된 파일의 이벤트 대기, 호출이 성공 시 발생 이벤트 갯수를 실패시 -1을 반환 후 errno를 기록 호출 성공 events에 해당 이벤트를 기록, 최대 maxevent만큼의 이벤트 기록 timeout시 반환값음 0 에지 트리거와 레벨트리거 트리거 종류, 당신이 생가하는 그거 메모리에 파일 매핑 mmap() 1 2 3 #include \u0026lt;sys/mman.h\u0026gt; void* mmap(void* addr, size_t len, int prot, int flags, int fd, off_t offset); mmap()을 호출하면 파일디스크립터 fd가 가리키는 파일의 offset위치에서 len 바이트만큼 메모리에 매핑하도록 요청 addr이 포함되면 메모리에서 해당주소를 선호한다고 커널에 알림(단지 제안일 뿐) mmap()은 성공시 맵핑의 실제 시작주소를 반환, 실패 시 MAP_FAILED 반환 후 errno를 설정 prot인자는 맵핑에 원하는 메모리 보호 정책을 명시, OR 연산으로 묶어서 설정 가능 PROT_NONE 접근불가(거의 미사용), PROT_READ 읽기 가능, PROT_WRITE 쓰기가능, PROT_EXEC 실행가능 flags 인자에는 맵핑의 유형 및 동작 요소 MAP_FIXED : addr인자를 제안이 아닌 요구사항으로 취급, 커널이 해당주소를 확보 못할 시 호출 실패 MAP_PRIVATE : 맵핑 미공유, 파일은 copy-on-write로 매핑, 매핑된 내용에 변경이 발생하더라도 실제파일이나 다른 프로세스에 미영향 MAP_SHARED 같은 파일을 맵핑한 모든 프로세스와 맵핑 공유, 변경이 일어나면 실제 파일에서도 동일한 내용 기록 mmap()시스템콜은 페이지를 다루며 addr과 offset은 페이지 크기로 정렬되어야 함(정렬이 되지않은 값이 전달되면 정렬된 값으로 할당) 페이지의 크기를 구하는 방법은 sysconf(_SC_PAGESIZE), getpagesize() 등이 있음(이식성을 고려 시 sysconf를 사용) mmap()관련 시그널은 SIGBUS(프로세스가 유효하지 않은 맵핑영역 접근), SIGSEGV(읽기전용 매핑영역에 쓰려고 할 때)가 있음 munmap() 1 2 3 #include \u0026lt;sys/mman.h\u0026gt; int munmap(void* addr, size_t len); munmap()은 addr에서 len 바이트만큼 이어지는 프로세스 주소 공간 내 존재하는 페이지를 포함하는 맵핑을 해제 성공시 0을 반환, 실패 시 -1을 반환하고 errno를 적절한 값으로 설정 mmap 장점 read/write 시스템콜을 사용할 때 발생하는 불필요한 복사 방지 잠재적인 페이지 폴트를 제외하면 매핑된 파일을 읽고 쓰는데 다른 시스템 콜이나 컨택스트 스위칭이 발생하지 않음 여러 프로세스가 같은 객체를 메모리에 매핑한다면 데이터는 모든 프로세스 사이에서 공유 lseek()같은 시스템콜을 사용하지 않고 매핑영역 탐색 가능 mmap 단점 메모리 매핑은 페이지 크기의 정수배만 가능하므로 작은 파일이 많다면 공간이 낭비됨 32비트 주소공간에서 다양한 크기의 수많은 매핑 사용시 주소공간이 파편화 됨(64비트 주소공간에서는 상관없음) 메모리 매핑과 관련된 자료구조를 커널 내부에서 생성, 유지하는 오버헤드가 있음 매핑 크기 변경 1 2 3 4 #define _GNU_SOURSE #include \u0026lt;sys/mman.h\u0026gt; void* mremap(void* addr, size_t old_size, size_t new_size, unsigned long flags); mremap()은 [addr, addr+old_size)에 매핑된 영역을 new_size만큼의 크기로 변경 flags인자는 0이거나 MREMAP_MAYMOVE(크기변경 수행하는데 필요시 맵핑의 위치가 변경되도 됨)가 될 수 있다. 성공 시 크기가 조정된 메모리 맵핑의 시작주소 반환, 실패시 MAP_FAILED 반환 후 errno를 설정 매핑의 보호모드 변경 1 2 3 #include \u0026lt;sys/mman.h\u0026gt; int mprotect(const void* addr, size_t len, int prot); mremap()은 [addr, addr+len)내에 포함된 메모리의 페이지 보호모드를 prot로 변경(추가 아님) 성공시 0을 반환, 실패 시 -1을 반환하고 errno를 설정 파일과 매핑의 동기화 1 2 3 #include \u0026lt;sys/mman.h\u0026gt; int msync(void* addr, size_t len, int flags); msync()는 addr에서 len 바이트 만큼 매핑된 파일이나 파일의 일부를 디스크로 동기화 msync()를 호출하지 않으면 매핑이 해제되기 전까지는 맵핑된 메모리에 쓰여진 내용이 디스크로 반영됨을 보장할 수 없다. flags는 동기화 방식 제어 MS_SYNC 동기화 호출, MS_ASYNC 비동기식 동기화, MS_INVALIDATE 맵핑의 캐시 복사본을 모두 무효화 성공시 0을 반환, 실패 시 -1을 반환하고 errno를 설정 맵핑의 사용처 알려주기 1 2 3 #include \u0026lt;sys/mman.h\u0026gt; int madvise(void* addr, size_t len, int advice); madvice() 시스템콜은 주어진 힌트에 따라 맵핑의 동작 방식을 최적화(캐시 및 미리읽기 방식) 가능 advice 인자는 커널에 알려줄 힌트 기술 MADV_NOMAL : 일반적 영역, 적당양 미리 읽기 MADV_RANDOM : 랜덤하게 접근하는 영역, 미리읽기 미사용(최소한의 데이터만 가져옴) MADV_SEQUENTIAL : 순차적으로 접근하는 영역, 공격적인 미리 일기 수행 MADV_WILLNEDD : 곧 접근하는 영역, 미리읽기 활성화 후 주어진 페이지를 메모리로 읽음 MADV_DONTNEED : 당분간 접근하지 않는 영역, 페이지와 곤련된 자원 해제 후 동기화 되지 않은 페이지 버림 성공시 0을 반환, 실패 시 -1을 반환하고 errno를 설정 일반 파일 입출력에 대한 힌트 posix_fadvise() 시스템콜 1 2 3 #include \u0026lt;fcntl.h\u0026gt; int posix_fadvise(int fd, off_t offset, off_t len, int advice); fd의 [offset, offset+len] 범위에 대한 흰트를 제공 주로 len을 0으로 넘겨서 파일 전체에 대한 힌트를 제공하는 방식으로 사용 힌트에 대해 커널이 등으하는 방식은 구현에 따라 커널버전에 따라 다름 POSIX_FADV_NORMAL : 힌트없음, 적당한 미리읽기 POSIX_FADV_RANDOM : 랜덤 접근, 미리읽기를 하지 않고 매번 일기마다 최소한의 데이터만 읽음 POSIX_FADV_SEQUENTIAL : 순차접근, 미리읽기 윈도우 크기를 두배로 늘림 POSIX_FADV_WILLNEED : 곧 접근함, 미리읽기 활성화하고 주어진 페이지를 메모리로 읽음 POSIX_FADV_NOREUSE : 곧 한번만 접근, POSIX_FADV_WILLNEED와 동일하게 동작 POSIX_FADV_DONTNEED : 당분간 안 접근함, 범위내의 캐싱 중인 데이터를 페이지 캐시에서 제거 성공 시 0 반환, 실패 시 -1 반환 후 errno 값을 설정 readahead() 시스템콜 1 2 3 4 #define _GNU_SOURCE #include \u0026lt;fcntl.h\u0026gt; ssize_t readahead(int fd, off64_t offset, size_t count); 리눅스 전용으로 POSIX_FADV_WILLNEED와 동일한 방식을 제공하기 위해 사용 fd가 가리키는 파일의 [offset, offset+counter) 영역의 페이지를 캐시 생성 부담없이 힌트를 사용하자 책에서는 힌트는 도움이 많이 된다고 기술 POSIX_FADV_WILLNEED를 이용하여 읽으려는 파일을 미리 캐시에 넣어 어플리케이션에서 블로킹을 방지하거나 비디오 스트림 기록 같은 경우 POSIX_FADV_DONTNEED로 캐시에서 제거 할 수 있음 동기화, 동기식, 비동기식 연산 동기식, 비동기식 용어는 입출력 연산이 반환하기 전 어떤 이벤트(ex. 데이터 저장)을 기다리는지 여부 동기화, 비동기화 용어는 정학한 어떤 이벤트(ex. 데이터를 디스크에 기록)이 발생해야 함을 나타냄 책에 표를 읽어보자 비동기식 입출력 aio라이브러리는 비동기식 입출력을 요청하고 작업이 완료되면 알림을 받는 함수를 제공 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #include \u0026lt;aio.h\u0026gt; struct aiocb{ int aio_fildes; // 파일 디스크립터 int aio_lio_opcode; // 수행할 작업 int aio_reqrio; // 요청 우선순위 오프셋 volatile void* aio_buf; // 버퍼에 대한 포인터 size_t aio_nbytes; // 연산의 크기 struct sigevent aio_sigevent; // 시그널의 번호와 값 /* 내부적으로 사용하는 프라이빗 멤버 */ } int aio_read(struct aiocb* aiocbp); int aio_write(struct aiocb* aiocbp); int aio_error(const struct aiocb* aiocbp); int aio_return(struct aiocb* aiocbp); int aio_cancel(int fd, struct aiocb* aiocbp); int aio_fsync(int op, struct aiocb* aiocbp)l int aio_suspend(const struct aiocb* const cblist[], int n, const struct timespec* timeout); 입출력 스케줄러와 성능 디스크 주소 지정 방식 하드디스크는 CHS(cylinder, Head, Sector) 주소 지정방식 사용하며 요즘은 이 값대신 유일한 블록 번호를 CHS에 맵핑 입출력 스케줄러 동작 방식 입출력 스케줄러는 병합과 정렬이라는 두가지 기본 동작 수행(하드디스크의 입출력 성능 극대화를 위함) 병합은 둘 이상의 인접한 입출력 요청을 단일 요청으로 합침 대기 중인 입출력 요청을 블록 순서의 오름 차순으로 정렬 읽기 개선 입출력 스케줄러에는 다음과 같은 방식이 있다. 데드라인 입출력 스케줄러 예측 입출력 스케줄러 CFQ 입출력 스케줄러 Noop 입출력 스케줄러 입출력 스케줄러 선택과 설정 기본 입출력 스케줄러는 부팅 시 커널 명령행 인자인 iosched를 통해서 선택 가능 `/sys/block/[device]/queue/scheduler 값을 변경해서 입출력 스케줄러 선택가능 입출력 성능 최적화 자잘한 연산을 묶어 입출력 연산을 최소화 하거나 입출력을 블록 크기에 정렬되도록 수행하거나 3장의 사용자 버퍼링, 벡터 입출력, 2장의 위치를 지정한 입출력, 비동기식 입출력 등을 고려할 수 있다. 사용자 영역 어플리케이션에서 커널과 유사한 방식을 사용하여 더 나은 성능을 얻을 수 있도록 노력해야함 사용자 영역에서 입출력 스케줄링 하기 경로로 정렬하기 inode로 정렬하기 물리 블록으로 정렬하기 ","date":"2023-06-14T00:00:00Z","permalink":"https://muonkmu.github.io/p/linux_system_programming-04-%EA%B3%A0%EA%B8%89%ED%8C%8C%EC%9D%BC-%EC%9E%85%EC%B6%9C%EB%A0%A5/","title":"[Linux_System_programming] 04 고급파일 입출력"},{"content":"사용자 버퍼 입출력 커널은 내부적으로 지연된 쓰기 연산, 미리읽기, 연속된 입출력 요청을 모아서 처리하는 방식으로 버퍼링을 구현 그러나 일반 파일에 대해 잦은 입출력을 처리해야만 한다면 성능 개선을 위해 사용자 영역에서 애플리케이션, 라이브러리에 의한 버퍼링 처리 필요 블록크기 실제로 블록크기는 보통 512, 1024, 2048, 4096, 8192로 정해짐 stat명령으로 블록크기을 알아낼 수 있지만 이런 경우는 드물며 블록크기의 정수배가 아닌 특이한 값으로 입출력 연산을 하여 정렬되지 않은 입출력을 피해야 한다. 그러나 어플리케이션에서 데이터를 블록 단위가 아닌 행, 필드, 단일문자를 다루므로 사용자 버퍼를 사용하여 하나의 큰 버퍼에 저장되어 있다가 블록크기에 맞춰 파일 시스템으로 전달 사용자가 직접 버퍼링을 구현할 수도 있지만 C의 표준 입출력 라이브러리나 C++의 iostream사용 표준 입출력 이 책에서는 최신 리눅스 시스템에 포함된 glib에서 구현된 인터페이스와 동작방식 설명 C의 표준입출력은 파일 디스크립터를 직접 다루지 않고 파일포인터라는 독자적 식별자를 사용하며 파일포인터는 C라이브러리 내부에서 파일 디스크립터로 매핑 표준 입출력에서 열린 파일은 스트림이라고 부르기도 한다. 파일 열기, 닫기 1 2 3 4 5 #include \u0026lt;stdio.h\u0026gt; FILE* fopen(const char* path, const char* mode); FILE* fdopen(int fd, const char* mode); int fclose(FILE* stream); fopen은 path를 mode에 따라 원하는 용도로 새로운 스트림 생성, 모드는 하기와 같음 r : 읽기 목적으로 파일을 염, 스트림은 파일 시작 지점 r+ : 읽기/쓰기, 스트림은 파일 시작지점 w : 쓰기, 파일 존재시 길이 0으로 만들고 존재하지 않으면 새로 만듬, 스트림은 파일 시작지점 w+ : 읽기/쓰기, 나머지는 w와 같음 a : 덧붙이기 쓰기, 파일 존재 시 길이 0으로 만들고 존재하지 않으면 새로 만듬, 스트림은 파일 끝지점 a+ : 덧붙이기 읽기/쓰기, 나머지는 a와 같음 fdopen은 파일 디스크립터를 통해 스트림을 만들며, 원래 파일 디스크립터 열 때의 모드와 호환성 유지 필요, 스트림 닫을 시 파일 디스크립터도 닫힘 fclose는 스트림을 닫음, fcloseall은 현재 프로세스와 관련된 모든 스트림 닫음 스트림에서 읽기 한번에 한문자씩 읽기 1 2 3 4 #include \u0026lt;stdio.h\u0026gt; int fgetc(FILE* stream); int ungetc(int c, FILE* stream); fgetc()는 stream에서 다음 문자를 읽고 unsigned char 타입을 int 타입으로 변환 반환 파일 끝이나 에러를 알려주기 위해 int로 반환, ungetc()는 c값을 찔러 넣음, 성공 시 c를 반환, 성공후 fgetc로 읽으면 c값이 읽힘, 읽어보고 되돌려 넣기 위해 사용 한 줄씩 읽기 1 2 3 #include \u0026lt;stdio.h\u0026gt; char* fgets(char* str, int size, FILE* stream); fgets()는 size보다 하나 적은 내용을 읽어서 결과를 str에 저장, 마지막 바이트 읽고 난 후 버퍼 마지막에 null문자(\\0) 저장 EOF나 개행문자를 만나면 읽기 중단 바이너리 데이터 읽기 1 2 3 #include \u0026lt;stdio.h\u0026gt; size_t fread(void* buf, size_t size, size_t nr, FILE* stream); fread()는 stream에서 크기가 size 바이트인 엘리멘트를 nr개 읽어서 buf에 저장, 읽어 들인 엘리멘트 갯수 반환(바이트 아님) nr보다 적은 값을 반환하여 실패나 EOF를 알림, ferror()/feof()를 사용해야만 어느 에러에 해당되는지 알 수 있다. 스트림에 쓰기 1 2 3 4 5 #include \u0026lt;stdio.h\u0026gt; int fputc(int c, FILE* stream); int fputs(const char* str, FILE* stream); size_t fwrite(void* buf, size_t size, size_t nr, FILE* stream); fputc()는 c로 지정한 바이트를 stream에 쓰고 성공시 c를 반환, 실패시 EOF반환 후 errno를 설정 fputs()는 str이 가르키는 NULL로 끝나는 문자열 전부를 stream에 기록, 성공 시 음수가 아닌값 반환 실패 시 EOF 반환 fwrite()는 buf가 가리키는 데이터에서 size크기의 엘리먼트 nr개를 stream에 쓴다, 성공 시 엘리먼트 개수 반환 스트림 탐색 1 2 3 4 5 6 7 #include \u0026lt;stdio.h\u0026gt; int fseek(FILE* stream, long offset, int whence); int fsetpos(FILE* stream fpos_t* pos); void rewind(FILE* stream); long ftell(FILE* stream); int fgetpos(FILE* stream fpos_t* pos); fseek()는 stream에서 파일의 위치를 조작, 성공시 0을 반환, 실패 시 -1을 반환 errno를 적절한 값으로 설정 whence가 SEEK_SET이면 파일의 위치를 offset 값으로 설정 whence가 SEEK_CUR이면 파일의 위치를 현재 위치에서 offset 만큼 더한 값으로 설정 whence가 SEEK_END이면 파일의 위치를 파일 끝에서 offset 만큼 더한 값으로 설정 fsetpos()는 stream 위치를 pos로 설정, SEEK_SET를 사용한 fseek()와 동일하게 동작, 이기종 호환성 위해 존재, 리눅스 계열에서는 사용 불필요 frewind()는 stream을 시작위치로 되돌림, 반환 값이 없으므로 호출전 errno를 초기화 한후 호출 후 errno를 확인해야 함 ftell()를 현재 스트림을 빈환, fseek()는 갱신된 위치를 반환하지 않으므로 이 함수를 이용 fgetpos()는 현재 스트림을 pos에 반환, 이기종 호환성을 위해 존재 스트림 비우기 1 2 3 #include \u0026lt;stdio.h\u0026gt; int fflush(FILE* stream); fflush()는 stream에 있는 쓰지 않은 데이터를 커널로 비움, stream이 NULL이면 프로세스의 모든 입력 스트림을 비움 성공시 0을 반환 실패시 EOF를 반환하고 errno를 적절한 값으로 설정 스트림(유저영역 존재)을 비우는 것이므로 물리매체에 기록하는 것을 보장하기 위해서는 fsync()를 호출해야 한다. 에러와 EOF 1 2 3 4 5 #include \u0026lt;stdio.h\u0026gt; int ferror(FILE* stream); int feof(FILE* stream); void clearerr(FILE* stream); fread()와 같은 몇몇 표준 입출력 인터페이스는 에러와 EOF를 구분하는 방벙을 제공하지 않는 등 에러를 알려주는 기능이 약함, 이를 위해 스트림 상태 확인 함수 제공 ferror()는 stream에 에러 지시자가 설정되어 있을 경우 0이 아닌 값을 반환, 반대의 경우 0을 반환 feof()는 stream에 EOF 지시자가 설정되어 있을 경우 0이 아닌 값을 반환, 반대의 경우 0을 반환 clearerr()는 stream에서 에러/EOF 지시자 초기화, 항상 성공하므로 검사먼저 하고 이 함수를 호출 할 것 파일 디스크립터 얻어오기 int fileno(FILE* stream)는 stream과 관련된 파일 디스크립터를 반환 표준 입출력 함수와 시스템콜을 섞어서 사용하는 방식은 권장하지 않음 버퍼링제어하기 1 2 3 #include \u0026lt;stdio.h\u0026gt; int setvbuf(FILE* stream, char* buf, int mode, size_t size); 표준 입출력 함수 setvbuf()은 유형의 사용자 버퍼링을 구현하고 버퍼의 유형과 크기를 다룰 수 있는 인터페이스 제공 mode는 버퍼미사용, 행버퍼, 블록버퍼 3가지 설정, buf는 stream을 위한 버퍼로 사용(NULL이라면 glib이 메모리 할당) 스레드 세이프 표준 입출력 함수는 내부적으로 락 및 락 카운터를 가지고 있어 스레드 세이프 보장 더 넓은 수준의 원자성을 구현하기 위해 표준입출력은 스트림에 관련된 락을 개별적으로 조작하는 함수 제공 수동으로 파일락 걸기 1 2 3 4 5 #include \u0026lt;stdio.h\u0026gt; void flockfile(FILE* stream); void funlockfile(FILE* stream); int ftrylockfile(FILE* stream); flockfile()은 stream의 락이 해제될 때까지 기다린 후 락 카운터를 올리고 락을 얻은 다음, 스레드가 스트림을 소유하도록 만든 후 반환 funlockfile()은 stream과 연관된 락카운터를 하나 줄임, 락카운터가 0이 되면 스레드는 stream의 소유권을 포기후 다른 스레드가 락을 얻을 수 있게 함 ftrylockfile()은 flockfile의 논블럭 버전 락을 사용하지 않는 스트림 연산 락을 수행하지 않는 버전의 표준 입출력 함수 제공(책에 함수 리스트 참조, 이름에 _ulocked 라고 뒤에 붙음) 락 오버헤드를 없애서 성능향상을 할 수 있음, 락이 필요하다면 개발자가 수동으로 락을 얻고 해제해야함 표준입출력 비평 표준 입출력의 가장 큰 문제는 이중복사 문제(read()시 커널에서 표준 입출력 버퍼로 복사 후 fgetc()를 통해서 어플리케이션 버퍼로 이동) 이중복사를 해결할 수 있는 고도로 최적화된 사용자 버퍼링 라이브러리가 존재 ","date":"2023-05-25T00:00:00Z","permalink":"https://muonkmu.github.io/p/linux_system_programming-03-buffer-i/o/","title":"[Linux_System_programming] 03 Buffer I/O"},{"content":"PIPELINE PIPELINE directive를 사용하여 loop와 function들을 순차적이 아닌 병렬적으로 처리 가능 loop pipeline에서 면적과 퍼포먼스 최적화를 위해 inner-most-loop에 파이프라인 설정 (교재 그림 참조) PIPELINE directive 이하의 루프는 unroll 되므로 바깥쪽에 위치하면 리소스를 많이 사용하게 됨 Top-level loop는 Buble이 생길 수 있다. function pipeline는 계속해서 돌지만 loop pipeline은 반복 횟수가 있음 loop pipeline은 시작전에 한사이클의 초기화 동작(start/stop/done signal 동작 등) 필요 이로 인해 buble이 생기는데 rewind option으로 제거 가능 Pipeline은 input이 없으면 (input data valid low) 파이프라인을 멈춤, 그러나 FLUSH option 사용 시 입력 행동은 멈추지만 이미 들어온 입력의 처리는 멈추지 않고 계속됨 다음 상황은 pipelining을 막음 loop에서 variable bound(반복횟수가 가변일 때) 코드 안의 feedback (if else 문 같은 조건 분기) resource contention(in/outport에서 경쟁조건, 포트 동시 읽기 쓰기) DATAFLOW DATAFLOW directive는 task level의 pipelining 지원, function과 loop 병렬 수행되게 함 sample base의 데이터 packet/array에서 동작 DATAFLOW는 서브모듈 사이에 채널(ping-pong buffer 또는 FIFO)를 구현 data rate 유지를 위함 array는 memory element(default ping-pong buffer)를 사용하고, scalar는 handshake가 있는 레지스터 사용 하기 상황은 Dataflow optimization을 제한한다.(예제는 교재 참조) single producer-consumer violations bypassing task feedback between tasks conditional execution of task loop with multiple exit condition DATAFLOW와 PIPELINE의 차이는 다음과 같다 DATAFLOW는 \u0026ldquo;coarse grain\u0026rdquo; 개념(function/loop), PIPELINE은 \u0026ldquo;fine grain\u0026rdquo;(operator +,\u0026raquo;) DATAFLOW는 top level의 loop/function에서 쓰이며 PIPELINE은 function/loop 내부에서 사용가능 PIPELINE는 loop를 unroll 함 Optimizing Structure HLS에서 array는 default로 dual-port RAM으로 합성(FIF도 가능), single/dualport-RAM/FIFO/LUT 등 다양한 옵션을 제공(UG902에 설명 있음) dual-port RAM이라도 read/write가 한 사이클에 끝나지 않기에 성능의 병목구간으로 작용, 이의 해결을 위해 Partitioning과 Bottleneck을 지원 Partitioning Array Partitioning은 물리적으로 메모리 인스턴스를 추가하여 동시에 접근할 수 있는 데이터의 개수를 늘리는 방법 ARRAY_PARTITON directive를 사용하며 factor 옵션으로 memory element의 갯수를 결정할 수 있음, 하기 3가지 방법 Block : 연속된 element를 가진 동일 사이즈의 블럭으로 분리 cyclic : interleaving element를 가진 동일 사이즈의 블럭으로 분리 complete : array를 개별 레지스터 element로(LUT?) 합성(factor 옵션 미지원) dimension 옵션은 array의 물리적 dimension을 결정 int my_array[10][6][4]는 width가 32-bit, depth가 240개인 메모리 인스턴스 한 개를 생성 한 번에 4 픽셀을 접근해야 한다면 dimension 3에 대해서 분할하면 됨. 그러면 4개의 메모리 인스턴스가 생성 Solution setting에서 config_array_partioning을 이용하면 다음 파라메터를 설정할 수 있음(공부 필요) auto_partitioning_threshold auto_promotion_threshold include_extern_global, scalarize_all Reshaping 메모리 인스턴스의 개수는 그대로 둔채로 메모리 워드의 너비를 늘리는 방식 block, cyclic, complete 방식이 있음 Data pack and Data dependencies struct 타입은 기본적으로 포트 구성이 구조체 내부의 element에 따라 구성됨, 구조체 내부에 배열이 있으면 배열은 하나의 포트로 구성 됨 DATA_PACK directive를 사용하면 single wide port로 구성할 수 있다. BRAM은 한 사이클에 읽기 쓰기가 동시에 안되므로(dual-port RAM도 마찬가지) 루프안에 내부 디펜던시가 있으면 Dependency가 있으면 pipeline이 정상적으로 안될 수 있다. DEPENDENCY directive를 이용해서 dependecy에 대한 추가 정보를 줄 수 있다.(dependency가 루프 내부에만 있는지 외부랑 연결되어 있는지) ","date":"2023-04-26T00:00:00Z","permalink":"https://muonkmu.github.io/p/hls-xilinx-hls-chap_02-performance-optimization/","title":"[HLS] Xilinx HLS Chap_02 Performance optimization"},{"content":"Introduction HLS는 High Level Synthesis의 약자로 C를 RTL 언어로 컴파일해주는 상위 언어이다. 기본적으로 C언어에 사용자가 기입한 Directive에 따라 HW로 컴파일됨 (ex, unroll, pipeline, dataflow) HLS는 현재 표준이 없기에 많은 업체에서 자신의 툴체인을 제공한다. 그러나 Xilinx는 자신의 툴체인을 무료로 배포한다. 장점은 빠른 개발이 가능하다는 것이다. Vivado HLS Tool Flow HLS Phases scheduling 각 Clock Cycle 마다 어떤 Operation이 실행될지를 결정 지정된 clock frequency와 user directive를 이용 Binding 스케줄링된 Operation이 어떤 HW resource를 사용할지 결정 즉, dsp를 쓸지 addsub 등을 사용할 지 결정 Control Logic Extraction RTL 내 FSM 로직을 만들기 위해 control logic을 추출 Terminology Latency : 입력에서 부터 출력까지 걸린 클럭수 Throughput : input과 input 간의 클럭수(다시 인풋을 읽어가는 시간) Initiation Interval(II) : 파이프라인에서 Input 간의 시간(Throughput과 같지만 파이프라인에서만 쓰임) C to RTL Conversion Top level function의 arguments는 I/O port로 합성됨 C function들은 RTL hierarchy를 가진 블럭으로 합성됨(대충 계층이 유지된다는 뜻인듯) C언어의 Loop는 기본이 rolled로 합성(Directive로 변경) C언어의 array는 Block RAM으로 합성 시스템콜, Dynamic Memory, pointer casting, recursive function, STL은 지원하지 않음 Library Support Arbitray Pricision Data type Lib(ex uint5_t 이런거)(ap_cint.h,ap_fixed.h) HLS MATH / Stream / Video library (hls_math.h, hls_stream.h, hls_video.h) fir, fft, shift register, linear algebra, dsp 라이브러리 Validation 검증은 하기 2단계로 나뉨 Pre-synthesis : C 코드의 에러를 검증(C simulation) post-synthesis : RTL을 검증(C/RTL co-simulation) C function의 Test bench는 Top level function을 포함한 main function을 이용하며 성공시 0, 실패시 1을 출력하게 함 Command line TCL 콘솔을 이용해서 명령을 내릴 수 있다. vivado_hls 등등등(필요하면 교재를 보자) Introduction to HLS ultrafast Design xilinx에서 추천하는 디자인 방법론은 하기와 같다. 대략적으로 로직짜고 인터페이스 정하고, pipeline 등을 이용해 성능최적화, 레이턴시 최적화, 면적 최적화 순으로 진행 I/O Interface Interface는 block과 port 레벨 두가지 타입으로 분류 Block-level interface protocol : RTL 블럭을 컨트롤 다른 port-level I/O protocol과 구분되며 ap_start, ap_ready, ap_idle, ap_done을 나타냄, Directive는 port=return Port-level interface protocol : 데이터 포트를 위한 시그날 그룹, C의 Top-level function의 argument에 타입에 따라 결정됨, Directive는 port=\u0026lt;포트이름\u0026gt; Function Argument는 데이터 포트로 합성되며 타입별 특성은 하기와 같음 Function return : ap_return으로 매핑 pointer는 읽기를 위한 입력 포트, 출력을 위한 쓰기 포트가 포트가 분리되어 합성 array 는 pointer 처럼 합성 지원되는 Interface type은 하기와 같음 Block Level I/O HLS 모듈의 behavior를 컨트롤 ap_ctrl_none/hs/chain 3가지 type의 block level-I/O가 있으며 s_axi_lite도 지원 ap_ctrl_hs(hand-shake) : ap_start, ap_ready, ap_idle, ap_done이 나타남(타이밍 차트는 교재에 있음) ap_ctrl_chain : ap_ctrl_hs에서 ap_continue가 추가됨, ap_continue는 체인연결에서 ap_ready와 연결되어 이전 블럭을 stall시킴 ap_none : 신호 행성이 안됨, II=1인 컨트롤이 필요없는 블럭에 사용 s_axi_lite로 만들 수 있으며 control signal, interupt control 등이 생성(map은 교재에 있음) solution setting에서 clock period, clock uncertainty을 설정할 수 있다. reset 컨트롤을 config_rtl에서 none, control, state, all등으로 설정할 수 있다. array등을 리셋할 때 이를 주의해야한다.(교재 참조) Port-level I/O 프로토콜의 type은 C의 포트 타입에 따라 결정됨 : pass-by-value scalar, Pass-by-reference pointers, pass-by-reference arrays. AXI Interface protocol 지원 axis(AXI4-Stream I/O), s_axilite(AXI4-Lite slave), m_axi(AXI4 Master) No I/O protocol ap_none : 데이터 read/write 시 이를 나타내는 컨트롤 포트가 없음 ap_stable : ap_none과 같지만 레지스터의 컨트롤 세팅입력과 같이 리셋이후(?) normal operation에서 값이 변하지 않는 입력등을 위해 사용 Wire handshake Protocol ap_hs : valid와 acknowledge신호를 가진 two-way handshake ap_ack, ak_vld : valid와 acknowledge 중 하나만 가짐 ap_ovld : in-out port를 위한 것으로 in_port는 ap_none, out_port는 ap_vld로 설정 Memory Interface(RAM,FIFO) ap_memroy, bram : array argument 구현을 위해 사용되며 random access를 위한 memory element(RAM/ROM)과의 통신포트(data, addr, ce, we 포트), 기본적으로 dual-port로 구성되며, ap_memory는 discreate port로 합성 bram은 그룹 포트로 합성 ap_fifo : array가 시퀀셜하게 접근 시 사용가능 Bus Protocol ap_bus : 특정한 버스 스탠다드를 따르지 않는 bus bridge를 위한 인터페이스 input은 ap_none, output은 ap_vld, inout은 ap_ovld가 default임 Port-level I/O Protocol AXI4 Interface AXI4 Lite Array제외한 다양한 포트가 그룹핑이 가능(same bundle name) reset은 자동적으로 active low로 설정 grouped port에서 input은 ap_none, output은 ap_vld, return은 ap_ctrl_hs가 defualt. 별도의 클럭과 리셋을 세팅할 수 있음(별도 클럭은 ap_clk에 sync되고 느려야함, 별도 클럭 도메인을 위한 FIFO 등이 없음) AXI4 Master 다수 포트가 그룹될 수 있다 C/RTL cosimulation을 위해 Depth옵션이 필요하다(pointer에서 array는 아님) Individual/Burst transfer 모드를 위해 array나 pointer arguement 사용시 사용 가능 memcpy 등을 사용하여 데이터를 옮길 수 있으며 이때 offset을 지정할 수 있다. burst Access가 지원 가능 AXI4 Stream input 또는 array/pointer output argument에 사용 가능 side channel 및 register option 옵션 설정 가능 (공부가 더 필요\u0026hellip;) Memory Interface RAM port(ap_memory) array를 위한 인터페이스로 Single/dual port 인터페이스 제공 RESOURCE directive를 사용하여 off-chip RAM 인터페이스처럼 사용 FIFO port(ap_fifo) array, pointer, reference에 사용되며 input/output이 분리됨 Block RAM Interface(BRAM) ap_memory와 비슷하며 grouped single port interface ap_memory가 off-chip sram과 연결이 쉽고 BRAM 인터페이스가 없지만 ap_bram은 BRAM 인터페이스가 쉬움 BUS protocol ap_bus 비산업표준의 generic bus, pointer와 pass-by-reference를 위한 인터페이스 complex pointer arithmetic을 쓸 경우 ap_bus를 사용(포인터 연산은 ap_bus만 지원) standard 모드는 개별 read/write 동작, memcpy를 통한 burst transfer 지원 ","date":"2023-04-25T00:00:00Z","permalink":"https://muonkmu.github.io/p/hls-xilinx-hls-chap_01-intro-and-port-i/o/","title":"[HLS] Xilinx HLS Chap_01 Intro and Port I/O"},{"content":"파일 열기 1 2 3 4 5 6 #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/stat.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; int open(const char *name, int flags); int open(const char *name, int flags, mode_t mode) 파일을 열고 파일 디스크립터에 매핑 flags 인자는 O_RDONLY, O_WRONLY, O_RDWR중 하나를 포함해야 함, 그외 O_APPEND, O_NONBLOCK 등이 있음 (책참조) mode에서는 새로운 파일의 권한(ex 0644, S_IRWXU 등이 있음, 책참조) O_WRONLY|O_CREAT|O_TRUNC 조합은 너무 일반적이라 creat()시스템 콜을 제공 에러 발생 시 -1 리턴 후 errno를 적절한 에러값으로 설정 파일 읽기 1 2 3 #include \u0026lt;unistd.h\u0026gt; ssize_t read(int fd, void *buf, size_t len) fd가 참조하는 파일의 현재 오프셋에서 len byte바이트 만큼 buf에 읽기 현재 읽은 바이트수 반환, 에러시 -1 반환, 0 반환 시 EOF(읽을 데이터 없음)을 나타냄(책에 리턴의 다양한 상황에 대해 나와 있음) 논블럭 읽기 모드에서 읽을 데이터가 없다면 -1반환, errno를 EAGAIN으로 설정(논블럭 모드시 반드시 점검할 것) 최대 읽기 값은 SSIZE_MAX(LONG_MAX, 0x7ffffff) SIZE_MAX는 size_t의 최대값으로 부호가 없는 값, ssize_t는 부호 있는 값을 나타냄 파일 쓰기 1 2 3 #include \u0026lt;unistd.h\u0026gt; ssize_t write(int fd, const void *buf, size_t count) count 바이트 만큼 fd가 참조하는 현재 파일 시작지점이 buf인 내용 기록 현재 쓴 바이트수 반환, 에러시 -1 반환 후 errno를 적당한 값으로 변경 O_APPEND 모드는 파일 오프셋이 항상 파일 끝에 위치하도록 함(멀티 프로세스 관점에서 보면 파일 오프셋을 원자적으로 갱신) 논블럭 쓰기 모드에서 블럭되면 -1반환, errno를 EAGAIN으로 설정 write 명령 시스템콜은 물리적 영역에 바로 쓰는 것이 아닌 버퍼에 복사해 놓음, 이후 이 버퍼를 수집해서 정렬 후 디스크에 씀 동기식 입출력을 지원하기 위해 fsync()와 fdatasync(), sync() 시스템 콜 지원 fsync()는 버퍼의 모든 변경점을 디스크에 씀(메타데이터, 즉 파일생성시간 등 포함) fdatasync()는 메터데이터를 제외한 데이터만 기록 sync()는 인자/반환값이 없이 버퍼의 모든 내용을 기록하도록 요구, 범용성이 높음 open() 호출 시 O_SYNC 플래그 사용하면 모든 파일 입출력 동기화(O_DSYNC, O_RSYNC도 있다, 역할은 찾아볼 것) 직접입출력 open() 시 O_DIRECT 플래그를 사용하면, 캐시/버퍼링/입출력관리 값은 복잡한 계층을 우회하여 직접 입출력 관리, 효과가 미미하다. 파일 닫기 1 2 3 #include \u0026lt;unistd.h\u0026gt; int close(int fd) fd에 연관된 파일과의 매핑해제, 프로세스에서 파일 떼어냄 파일을 닫더라도 버퍼의 내용을 디스크에 강제로 쓰지 않는다. 파일을 닫을 때 커널 내부에서 그 파일을 표현하는 자료구조 해제, 메모리에서 inode 복사본 제거 close의 반환값을 검사하는 것이 좋다. EBADF(fd가 유효하지 않음) 및 EIO 가 중요 파일 탐색 1 2 3 4 #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; off_t lseek(int fd, off_t pos, int origin) 파일의 특정위치로 이동하는 함수, origin 인자 : SEEK_CUR, SEEK_END, SEEK_SET 호출 성공 시 새로운 파일 오프셋을 반환하며 에러가 발생하면 -1을 반환, errno를 설정 lseek(fd, 0, SEEK_CUR)을 이용하면 현재 오프셋을 알아낼 수 있다. 파일의 끝을 넘어 위치를 지정하는 것도 가능, 이의 경우 데이터를 읽으면 EOF 반환, 쓰기 요청시 새로운 공간 생성 후 0으로 채움 이렇게 0으로 채운 공간을 구멍이라고 하는데 이는 물리적 공간을 차지하지 않으며 이를 다루는 과정에서 물리적 입출력 작업이 필요하지 않다. 지정한 위치 읽고 쓰기 1 2 3 4 #include \u0026lt;unistd.h\u0026gt; ssize_t pread(int fd, void *buf, size_t count, off_t pos); ssize_t pwrite(int fd, const void *buf, size_t count, off_t pos); 읽고 쓸 파일 오프셋을 지정하여 read(), write() 수행함, 그러하 하기 차이점 존재 호출 완료 후 파일포인터를 갱신하지 않음 lseek사용 시 발생할 수 있는 경쟁 상태를 회피 가능(멀티 스레드에서 각 스레드가 동시에 offset을 업데이트 하려는 경우) 파일 잘라내기 1 2 3 4 5 #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int ftruncate(int fd, off_t len); int truncate(const char *path, off_t len) 파일을 len 크기로 잘라내는 시스템 콜 호출 성공 0 반환, 에러가 발생하면 -1을 반환, errno를 설정 파일의 크기보다 큰 값으로 잘라내기 가능, 확장된 바이트는 모두 0으로 채워 짐 다중입출력 여러 개의 파일 디스크립터를 동시에 블록하고 그중 하나라도 블록되지 않고 읽고 쓸 준비가 되면 알려주는 기능 논블록 입출력을 사용하면 프로세스가 계속 기다리면서 임의의 순서대로 입출력을 요청해야 함(비효율적임) 3가지 다중입출력 방식 제공 : select, poll, epoll select() 1 2 3 4 5 6 7 8 #include \u0026lt;sys/select.h\u0026gt; int select(int n, fd_set* readfds, fd_set* writefds, fd_set* exceptfds, struct timeval* timeout); FD_CLR(int fd, fd_set* set); FD_ISSET(int fd, fd_set* set); FD_SET(int fd, fd_set* set); FD_ZERO(fd_set* set); 동기화된 다중 입출력 메커니즘 제공, 파일 디스크립터가 입출력을 수행할 준비가 되거나 옵션으로 정해진 시간이 경과할 때 까지만 블럭 argument n : 파일디스크립터 집합에서 가장 큰 파일 디스크립터 숫자 readfds : 데이터 읽기가 가능한지 파악하기 위한 파일 디스크립터 집합 writefds : 데이터 쓰기가 가능한지 파악하기 위한 파일 디스크립터 집합 exceptfds : 예외 발생 또는 소켓에서 대역을 넘어서는 데이터가 존재하는지 파악하기 위한 파일 디스크립터 집합 timeout : NULL이 아니면 timeout 시간 설정, 함수 반환시 남은 시간을 넣어서 반환 됨 timeout을 제외한 나머지 fds집합을 NULL로 넘겨서 다양한 시스템에서 동작하는 쉬운 잠들기 구현 가능 pselect() : 파일 디스크립터와 시그널을 기다리는 사이에 발생할 수 있는 경쟁상태 해결을 위해 sigmask인자 추가 poll() 1 2 3 4 5 6 7 8 9 10 #include \u0026lt;sys/poll.h\u0026gt; struct pollfd { int fd; short events; short revents; }; int poll(struct pollfd* fds, nfds_t nfds, int timeout); systemv에서 제공하는 다중입출력, poll의 단점을 보완(그러나 이식성의 이유로 select가 많이 사용) argument fds : 감시하고자 하는 단일 파일 디스크립터를 명시, events 필드는 감시할 이벤트 비트마스크, revents는 반환 이벤트 nfds : 감시할 파일 디스크립터 갯수 timeout : timeout 밀리 초단위, 음수 사용시 무한 대기 select VS poll poll이 파일 디스크립터 숫자가 큰경우 점 더 효율정 select를 사용하면 파일 디스크립터 집합을 반환하는 시점에 재구성되므로 잇단 호출과정에서 매번 초기화 필요 select가 상대적으로 이식성이 높다 커널 부분 가상파일 시스템 추상화된 파일 시스템에 대한 설명(책 참조) 페이지 캐시 디스크 파일 시스템에서는 캐시를 가지고 있음 페이지 쓰기 저장 커널은 버퍼를 통해 쓰기 작업을 지연(플러셔 스레드가 별도로 있음) ","date":"2023-04-22T00:00:00Z","permalink":"https://muonkmu.github.io/p/linux_system_programming-02-file-i/o/","title":"[Linux_System_programming] 02 File I/O"},{"content":"Petalinux로 사용하다 보니 여러가지 면에서 불편한다. apt패키지 매너저도 사용하고 싶고 향후 여러 라이브러리의 호한성을 맞추기 위해 rootFS를 ubuntu로 바꾸어 보자\n기본 지식 xilinx는 zynq의 운영체제로 리눅스를 자신의 입맛에 맞게 개조한 petalinux를 제공한다. 좀 더 정확하게 말하면 petalinux란 자신들의 zynq를 위한 리눅스 시스템을 빌드 할 수 있는 YOCTO wrapper 이다. 상기 시스템에서 제공하는 커널 및 rootFS를 그냥 사용해도 되지만 librealsense 등을 포팅하기 힘들고 패키지 매니저 사용들이 불편함으로 ubuntu base를 포팅하자 ubuntu desktop을 사용하여 GUI 환경을 바로 꾸밀 수 도 있지만 desktop 환경은 LXDE/KDE를 직접 설치해도 되므로 우선 CLI 환경만 구성한다. 펌웨어, uboot, 커널, 디바이스 트리는 petalinux에서 제공하는 것을 사용하고 rootFS 만 Ubuntu base로 사용한다. Porting 절차 커널 및 모듈 빌드 xilinx에서 Ubuntu 사용을 위해 권장하는 petalinux의 커널 세팅은 하기와 같다. https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/148668419/Zynq+UltraScale+MPSoC+Ubuntu+VCU+Gstreamer+-+Building+and+Running+Ubuntu+Desktop+from+Sources 근데 PCI 는 왜 disable 하라는지 몰라서 그냥 enable 시켜 놓았다. (아마 제공하는 예제에서는 AP 부분 DP dual lane을 사용하느라 PCIe를 안써서 그런것 같다.) 향후 사용성을 위해 mali driver도 같이 넣기로 하였다. Following are some of the mandatory configurations needed for successful booting of Ubuntu Desktop. Disable initramfs in kernel configuration GUI at ‘General setup -\u0026gt; Initial RAM file system and RAM disk (initramfs/initrd) support’ Following settings are required to enable Input device, multimedia and USB related settings Device Drivers-\u0026gt;Input device support-\u0026gt;Event interface Device Drivers-\u0026gt;Input device support-\u0026gt;Keyboards Device Drivers-\u0026gt;Input device support-\u0026gt;Mouse interface Device Drivers-\u0026gt;Multimedia support-\u0026gt;Media USB Adapters-\u0026gt;USB Video Class (UVC) Device Drivers-\u0026gt;Multimedia support-\u0026gt;Cameras/video grabbers support Device Drivers-\u0026gt;Multimedia support-\u0026gt;V4L platform devices Device Drivers-\u0026gt;USB support and enable all required classes Device Drivers-\u0026gt;HID support-\u0026gt;Generic HID driver Device Drivers-\u0026gt;HID support-\u0026gt;USB HID support-\u0026gt;USB HID transport layer Disabling the PMBUS PMIC so that power demo can use them without any issues Device Drivers-\u0026gt;Hardware Monitoring support-\u0026gt;PMBus support-\u0026gt;Maxim MAX20751 Enable the PHY settings Device Drivers-\u0026gt;PHY Subsystem Device Drivers-\u0026gt;PHY Subsystem-\u0026gt;Xilinx ZynqMP PHY driver Disable the PCI settings Bus Support-\u0026gt;PCI support’ This needs to be disabled for this version Enable the sound related settings: Device Drivers-\u0026gt;Sound card support Device Drivers-\u0026gt;Sound card support-\u0026gt;Advanced Linux Sound Architecture’ enabling ALSA support Kernel hacking \u0026gt; Tracers \u0026gt; Kernel Function Tracer Enable VCU driver Device Drivers-\u0026gt;SOC (System On Chip) Specific Drivers-\u0026gt;Xilinx SoC Drivers-\u0026gt;[ m ] Xilinx VCU logicoreIP Initsupport mali 및 vcu 커널모듈을 만들기 위해 petalinux rootfs 세팅을 다음과 같이 설정한다. Filesystem Packages-\u0026gt;libs-\u0026gt;libmali-xlnx-\u0026gt;[ * ] libmali-xlnx Packages-\u0026gt;libs-\u0026gt;libmali-xlnx-\u0026gt;[ * ] libmali-xlnx-dev Filesystem Packages-\u0026gt;misc-\u0026gt;hdmi-module-\u0026gt;[ * ] kernel-module-hdmi 혹시 mali backend를 x11에서 fb나 wayland로 하고 싶다면 \u0026lt;plnx-proj-root\u0026gt;/project-spec/meta-user/conf/petalinuxbsp.conf에 MALI_BACKEND_DEFAULT = \u0026quot;wayland\u0026quot;를 추가하자 (https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/18841928/Xilinx+Arm+Mali-400+Driver#XilinxArmMali-400Driver-X11backend%3A) petalinux를 빌드한다. 빌드 중에 cpio에서 에러가 나는 경우가 있는데 이는 파일용량이 2GB를 넘어가서이다. 해결방법은 아래와 같다. https://docs.xilinx.com/r/2021.2-English/ug1144-petalinux-tools-reference-guide/do_image_cpio-Function-Failed rootfs 포팅 하기 사이트에 우분투 base의 rootfs를 받을 수 있다. 그러나 이것을 사용하면 사용자 입맛에 맞게 이미지를 만들 수 있으나 설정에 시간이 너무 오래 걸린다. https://cdimage.ubuntu.com/ubuntu-base/releases/focal/release/ 그래서 eewiki에서 제공하는 이미지를 사용하기로 하였다. 이미지를 다운 받아 rootfs 영역에 카피한다.(압축 풀 때 파일 권한이 변경되지 않도록 tar의 p옵션 필수) https://nuclearrambo.com/wordpress/running-ubuntu-20-04-on-zynq-soc/ mali와 vcu 커널 모듈을 rootfs복사한다.(옵션) sudo cp -rfv build/tmp/sysroots-components/xilinx_zcu104/kernel-module-vcu/lib/modules/5.15.19-xilinx-v2022.1/extra/* ${target_root}/root/modules/ sudo cp -rfv build/tmp/sysroots-components/xilinx_zcu104/kernel-module-mali/lib/modules/5.15.19-xilinx-v2022.1/extra/* ${target_root}/root/modules/ sudo cp -rfv build/tmp/sysroots-components/zynqmp/vcu-firmware/lib/firmware/* ${target_root}/lib/firmware/ zynq - ubuntu 기본설정 zynq 보드를 부팅시키고 로그인한다. 네트워크 매니저를 세팅한다. 현재 네트워크 매니저 세팅파일이 없으므로 만들어 준다. 호스트가 ubuntu라면 동일 파일이 있으니 복사해도 된다. 그후 리부팅 1 2 3 4 5 6 7 8 sudo cat \u0026gt; /etc/netplan/01-network-manager-all.yaml \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; network: version: 2 renderer: NetworkManager EOF sudo netplan generate sudo netplan apply sudo reboot 무슨 문제인지 부팅 후에도 dhcp에서 DNS를 받아오지 못한다(원인 파악 중) 다음 방법으로 해결해보자 임시 : echo \u0026ldquo;nameserver ${네임서버주소}\u0026rdquo; | sudo tee /etc/resolv.conf \u0026gt; /dev/null 영구 : /etc/systemd/resolved.conf 파일에 DNS 입력 후 재부팅 업데이트 및 시간대 설정 1 2 sudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade \u0026amp;\u0026amp; sudo apt-get dist-upgrade timedatectl set-timezone Asia/Seoul 참고 자료 https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/148668419/Zynq+UltraScale+MPSoC+Ubuntu+VCU+Gstreamer+-+Building+and+Running+Ubuntu+Desktop+from+Sources https://xilinx.github.io/Embedded-Design-Tutorials/docs/2022.2/build/html/docs/Design_Tutorials/MPSoC_Graphic_Subsystem/README.html https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/18841732/Ubuntu+on+Zynq https://nuclearrambo.com/wordpress/running-ubuntu-20-04-on-zynq-soc/ ","date":"2023-04-19T00:00:00Z","permalink":"https://muonkmu.github.io/p/yolo_acc_prj-06-ubuntu-setting/","title":"[YOLO_Acc_prj] 06 Ubuntu setting"},{"content":"매뉴얼 대로 했는데 커널 패닉이 일어난다. 로그를 보니 rootfs 디바이스가 로드 되지 않는 듯 하다. 일반적인 환경이 아니라 vmware상에서 개발하다보니 생긴 문제이다. 즉 커널 모듈이 vmware의 SCSI 드라이버를 로드하지 못하여 생기는 문제인 것 같다. 하기 두가지 방법으로 해결이 가능하다.\ninitramfs환경으로 부팅 : grub.cfg와 fstab에 파티션 이름이 아닌 UUID를 넣고 initramfs로 부팅하는 방법인데 부팅은 되나 이제껏 구성했던 환경은 아니다. kernel에 필요한 모듈을 넣어 다시 컴파일 하는 방법 필자는 삽질에 삽질을 계속하여 2번째 방법으로 해결하였다.\nKernel rebuild Host 환경 재로그인 필자는 grub을 설치한 후 백업을 하지 않았다. grub 명령어로 host의 우부투를 부팅하자. (부팅 시 grub 부팅 선택 화면에서 c키를 누르면 됨)\n1 2 3 4 set root=(hd0,1) linux /boot/vmlinuz root=/dev/sda1 initrd /boot/initrd.img boot 이후 매뉴얼의 7.3, 7.4절에서 기술한 내용으로 LFS chroot 환경으로 진입한다.\nkernel rebuild 매뉴얼의 10.3절에서 다음의 커널 모듈을 포함하여 커널을 리빌드한다. 사실 몇가지만 필요할텐데 정확하게 필요한게 먼지 몰라서 다 포함 시켰다.\nVMware Balloon Driver VMware VMCI Driver Maintain a devtmpfs filesystem AMD PCnet32 PCI support SCSI device support, SCSI low-level drivers BusLogic SCSI support Fusion MPT ScsiHost drivers for SPI Fusion MPT ScsiHost drivers for FC Fusion MPT ScsiHost drivers for SAS File Systems, Ext4 Journaling file system support 커널이 리빌드 되면 커널 모듈을 인스톨하고 빌드된 커널을 /boot에 복사하자. 이후 리부팅 후 성공\nreference https://blog.andreev.it/2013/09/linux-from-scratch-as-a-virtual-machine-on-vmware-workstation/ https://www.linuxquestions.org/questions/linux-from-scratch-13/my-lfs-does-not-boot-878722/ ","date":"2023-03-28T00:00:00Z","permalink":"https://muonkmu.github.io/p/lfs_prj-05-final-debugging/","title":"[LFS_prj] 05 Final debugging"},{"content":"ZCU104 - See3CAM_CU30를 이용하여 이미지 포맷 변환 및 리사이즈 Pipeline을 구성하였다. Xilinx에서 제공하는 M2M 기본 예제를 이용하였으며 삽질의 연속으로 2주 정도 걸린 것 같다. 결론적으로 Xilinx에서 제공하는 VPSS의 경우 다양한 기능을 지원하여 LUT의 사용량이 많고 M2M 드라이버의 경우 지원이 종료되었다. 향후에는 HLS로 직접 구현해야 할 것으로 생각된다.\n기본 지식 Xilinx IP list Video Processing SubSystem(VPSS) : Color Space Conversion(CSC)와 SCaler(SC) 기능을 지원한다. 기본적으로 Xilinx_Video_Driver와 호환된다. Framebuffer Read/Write : VDMA의 후속(?)으로 이미지 전용 DMA이다. Gstreamer와 호환이 되어 편리하게 이용이 가능하다. Xilinx Linux Driver Mem 2 Mem Composite Video Framework : Framebuffer Read/Write를 이용하여 비디오 파이프라인을 기술해주는 Bridge Driver이다. 현재 지원 종료되었으므로 테스트로만 이용한다.(지원종료된 이유를 확인 중) HW Design 레퍼런스에 나와 있는데로 FB_R -\u0026gt; CSC -\u0026gt; SC -\u0026gt; FB_W 파이프 라인으로 구성한다. 필자가 실수하여 삽질한 부분은 하기와 같다.\n파이프라인의 동작 클럭은 300Mhz로 하였다. 내부 클럭을 이용하므로 clock wizard ip의 입력 패드를 No buffer로 설정 FB_R과 FB_W의 리셋은 드라이버에 의해 구동되므로 PS EMIO로 제어 가능하게 구성한다.(pl reset으로 구동하지 말것) 생각보다 CSC/SC가 리소스를 많이 잡아 먹어서 나중에 HLS로 변경해야 할 듯 하다. SW Design 파이프라인 SW component의 구성을 보면 하기와 같다(필자의 생각이므로 틀릴 수 있음) Deviece Tree Petalinux는 빌드 시 HW Description 파일(.xsa)을 분석하여 PL Side의 Device Tree를 자동으로 생성해준다. \u0026lt;plnx-proj-root\u0026gt;/component/plnx_workspace/device-tree/device-tree/pl.dtsi에서 생성된 Device-tree를 확인할 수 있다 (petalinux-config에서 자동으로 디바이스트리를 생성하도록 설정되어 있어야한다). 우리는 vidoe-M2M 드라이버를 사용해야 하므로 디바이스 트리를 수정해야 하는데 이는 \u0026lt;plnx-proj-root\u0026gt;/project-spec/meta-user/recipes-bsp/device-tree/files/system-user.dtsi에 기술 시 적용된다. 하기 내역을 수정한다.\nFrameBufferRead/Write IP에 의해 브리지 노드가 생성되는데 이 노드를 Disable하고 Properity 및 node를 삭제한다. video_m2m 노드를 생성하고 DMA기술하고 및 포트 연결을 기술한다. CSC/SC 노드의 포트의 연결을 수정하여 video_m2m과 연결될 수 있도록 한다. CSC/SC 노드의 compatible 속성을 xlnx,v-vpss-csc 와 xlnx,v-vpss-sc로 고정한다. 이 v-vpss-*가 비디오 드라이버와 연결되는 드라이버이다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 /include/ \u0026#34;system-conf.dtsi\u0026#34; / { aliases { /delete-property/ vcap_v_proc_ss_1; /delete-property/ v_pl_dispv_proc_ss_0; }; }; #include \u0026lt;dt-bindings/media/xilinx-vip.h\u0026gt; \u0026amp;amba_pl { /delete-node/ vcap_v_proc_ss_1; /delete-node/ drm-pl-disp-drvv_proc_ss_0; vcap_v_proc_ss_1 { status = \u0026#34;disabled\u0026#34;; }; v_pl_dispv_proc_ss_0 { status = \u0026#34;disabled\u0026#34;; }; video_m2m { compatible = \u0026#34;xlnx,mem2mem\u0026#34;; dmas = \u0026lt;\u0026amp;v_frmbuf_rd_0 0\u0026gt;, \u0026lt;\u0026amp;v_frmbuf_wr_0 0\u0026gt;; dma-names = \u0026#34;tx\u0026#34;, \u0026#34;rx\u0026#34;; ports { #address-cells = \u0026lt;1\u0026gt;; #size-cells = \u0026lt;0\u0026gt;; port@0 { reg = \u0026lt;0\u0026gt;; direction = \u0026#34;input\u0026#34;; video_m2m_0_wb_in: endpoint { remote-endpoint = \u0026lt;\u0026amp;sca_outv_proc_ss_1\u0026gt;; }; }; port@1 { reg = \u0026lt;1\u0026gt;; direction = \u0026#34;output\u0026#34;; video_m2m_0_rb_out: endpoint { remote-endpoint = \u0026lt;\u0026amp;encoderv_proc_ss_0\u0026gt;; }; }; }; }; }; \u0026amp;v_proc_ss_0 { compatible = \u0026#34;xlnx,v-vpss-csc\u0026#34;; csc_portsv_proc_ss_0: ports { csc_port0v_proc_ss_0: port@0 { encoderv_proc_ss_0: endpoint { remote-endpoint = \u0026lt;\u0026amp;video_m2m_0_rb_out\u0026gt;; }; }; }; }; \u0026amp;v_proc_ss_1 { compatible = \u0026#34;xlnx,v-vpss-scaler-2.2\u0026#34;; scaler_portsv_proc_ss_1: ports { scaler_port1v_proc_ss_1: port@1 { sca_outv_proc_ss_1: endpoint { remote-endpoint = \u0026lt;\u0026amp;video_m2m_0_wb_in\u0026gt;; }; }; }; }; Test 보드가 정상적으로 부팅이 된다면 /dev/video#, /dev/media#, /dev/v4l-subdev#이 등록되었을 것이다. 상기 디바이스가 보이지 않는다면 dmesg 등을 이용해 커널 로그 및 부팅 로그를 확인하여 문제를 분석하자.\nTest는 media-ctl로 CSC/SC를 설정하고 Gstreamer로 스트리밍 파이프라인을 구성하여 결과를 파일로 저장하였다. 카메라 출력은 UYUV/1280x720이며 CSC/SC에서 이를 RGB/640x480으로 변환한다.\n1 2 3 4 5 6 7 media-ctl -d /dev/media0 -V \u0026#34;\\\u0026#34;a0100000.v_proc_ss\\\u0026#34;:0 [fmt:UYVY8_1X16/1280x720 field:none colorspace:rec709]\u0026#34; media-ctl -d /dev/media0 -V \u0026#34;\\\u0026#34;a0100000.v_proc_ss\\\u0026#34;:1 [fmt:RBG888_1X24/1280x720 field:none colorspace:srgb]\u0026#34; media-ctl -d /dev/media0 -V \u0026#34;\\\u0026#34;a0040000.v_proc_ss\\\u0026#34;:0 [fmt:RBG888_1X24/1280x720 field:none colorspace:srgb]\u0026#34; media-ctl -d /dev/media0 -V \u0026#34;\\\u0026#34;a0040000.v_proc_ss\\\u0026#34;:1 [fmt:RBG888_1X24/640x480 field:none colorspace:srgb]\u0026#34; gst-launch-1.0 v4l2src device=/dev/video1 num-buffers=1 ! video/x-raw, width=1280, height=720, format=UYVY ! v4l2convert capture-io-mode=4 output-io-mode=4 ! video/x-raw, width=640, height=480, format=RGB ! filesink location=test.rgb gst-launch-1.0에서 파이프라인 입출력 설정이 한번에 되지 않는 거 같은데 테스트 스크립트임으로 넘어가자..\n상기 저장된 파일은 RGB raw 데이터 파일인데 이를 확인하기 위해 간단한 파이썬 코드를 이용한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import os import cv2 import sys import numpy as np HEIGHT = 480 WIDTH = 640 CHANNEL = 3 if __name__ == \u0026#39;__main__\u0026#39;: #argment number check if (len(sys.argv) \u0026lt; 2) : print(\u0026#34;Arguments is missing\\n\u0026#34;) exit(1) rawRgbFileName = sys.argv[1] rawRgbFd = open(rawRgbFileName, \u0026#39;rb\u0026#39;) cvImage = np.fromfile(rawRgbFd, dtype=np.uint8, count=HEIGHT*WIDTH*CHANNEL).reshape(HEIGHT,WIDTH,CHANNEL) image_rgb = cv2.cvtColor(cvImage, cv2.COLOR_RGB2BGR) rawRgbFd.close # cv2.imshow(\u0026#39;\u0026#39;, cvImage) cv2.imshow(\u0026#39;\u0026#39;, image_rgb) cv2.waitKey() cv2.destroyAllWindows() 참고 자료 M2M reference : https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/80707675/Mem+2+Mem+VPSS-CSC+VPSS-SC+device Petalinux Devie Tree Tip : https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/18842482/Device+Tree+Tips ","date":"2023-03-22T00:00:00Z","permalink":"https://muonkmu.github.io/p/yolo_acc_prj-05-image-format-conversion-and-resize/","title":"[YOLO_Acc_prj] 05 Image format conversion and resize"},{"content":"chroot 환경에서 LFS 시스템 구축 과정을 다룬다.\nInstalling Basic System Software Package install 하기 패키지를 설치한다\nMan-pages-6.03 Iana-Etc-20230202 Glibc-2.37 Zlib-1.2.13 Bzip2-1.0.8 Xz-5.4.1 Zstd-1.5.4 File-5.44 Readline-8.2 M4-1.4.19 Bc-6.2.4 Flex-2.6.4 Tcl-8.6.13 Expect-5.45.4 DejaGNU-1.6.3 Binutils-2.40 GMP-6.2.1 MPFR-4.2.0 MPC-1.3.1 Attr-2.5.1 Acl-2.3.1 Libcap-2.67 Shadow-4.13 GCC-12.2.0 Pkg-config-0.29.2 Ncurses-6.4 Sed-4.9 Psmisc-23.6 Gettext-0.21.1 Bison-3.8.2 Grep-3.8 Bash-5.2.15 Libtool-2.4.7 GDBM-1.23 Gperf-3.1 Expat-2.5.0 Inetutils-2.4 Less-608 Perl-5.36.0 XML::Parser-2.46 Intltool-0.51.0 Autoconf-2.71 Automake-1.16.5 OpenSSL-3.0.8 Kmod-30 Libelf from Elfutils-0.188 Libffi-3.4.4 Python-3.11.2 Wheel-0.38.4 Ninja-1.11.1 Meson-1.0.0 Coreutils-9.1 Check-0.15.2 Diffutils-3.9 Gawk-5.2.1 Findutils-4.9.0 Groff-1.22.4 GRUB-2.06 Gzip-1.12 IPRoute2-6.1.0 Kbd-2.5.1 Libpipeline-1.5.7 Make-4.4 Patch-2.7.6 Tar-1.34 Texinfo-7.0.2 Vim-9.0.1273 MarkupSafe-2.1.2 Jinja2-3.1.2 Systemd-252 D-Bus-1.14.6 Man-DB-2.11.2 Procps-ng-4.0.2 Util-linux-2.38.1 E2fsprogs-1.47.0 Cleaning up 패키지 설치를 위한 컴파일 시 디버그 옵션(-g)가 들어가 있으르모 디버그 심볼이 포함되어 있다. 이를 제거한다면 용량을 줄일 수 있다. 그러나 시스템 소프트웨어에서 디버깅을 하거나 valgrind 또는 GDB를 이용한 회기 테스트 시 디버그 심볼이 필요하다. 매뉴얼을 참조하여 디버깅 심볼을 스트리핑하여 백업하자 /tmp 폴더의 임시파일, libtool archive 파일, 이전 장에서 사용한 컴파일러 등은 필요없으니 매뉴얼을 참조하여 삭제 System Configuration General Network Configuration systemd-networkd에 의해 기본적인 네트워크 구성이 된다. 네트워크 디바이스의 이름을 변경하고 싶거나 정적 IP/DHCP를 구성하고 싶을시 /etc/systemd/network 하위에 관련 설정 파일을 생성 /etc/resolv.conf 파일을 이용해 인터넷 연결 시 필요한 DNS를 설정할 수 있다. /etc/hosts에 호스트 이름을 설정(요즘은 네임서버가 거의 있어서 필요업지만 /etc/hosts를 참조하여 네임서버를 ip로 변환) Managing Devices systemd를 설치했으므로 udev 데몬이 활성화 된다.(udev는 부팅시 sysfs, 내부적으로는 devtmpfs에 검색된 디바이스를 로딩한다.) devtmpfs 인스턴스가 /dev에 마운트 될때, 디바이스 노드가 이름,퍼미션,소유권을 변경하면서 생성, 그 뒤커널은 uevent를 udev데몬(udevd)에 날린다. /etc/udev/rules.d 등 에 기술된 rule을 기반으로 udevd는 디바이스 노드에대한 새로운 심볼릭링크를 생성 만약 중복된 디바이스의 이름을 변경하고 싶다면(ex, usb 카메라와 같은) /etc/udev/rules.d에 룰을 기술 Time zone과 NTP time synq등을 설정한다. 리눅스 콘설 설정파일을 만들고 locale등을 설정한다. readline을 위한 /etc/inputrc를 만들고 /etc/shells 파일을 생성한다. Making the LFS System Bootable /etc/fstab 파일을 만들어 rootfs 와 swap파티션을 마운트 하자 리눅스 커널을 빌드하여 /boot폴더에 복사한다. grub을 설치하고 grub.cfg파일을 만들어 부팅 메뉴를 작성한다.(메뉴얼에서는 백업방법이 나와있다. 왠만하면 따라서 하자) The end chroot환경에서 로그아웃 후 마운트된 디렉토리를 해제하고 리부팅하면 끝이다. 그런데 커널패닉이 발생하니\u0026hellip;.디버깅은 다음장에\u0026hellip;.\n","date":"2023-03-19T00:00:00Z","permalink":"https://muonkmu.github.io/p/lfs_prj-04-building-the-lfs-system/","title":"[LFS_prj] 04 Building the LFS system"},{"content":"LFS 시스템 빌드를 위한 cross tool chain의 설정과정을 다룬다.\nTechical note LFS 시스템의 빌드 단계는 아래의 단계를 따른다\nStage Build Host Target Action 1 pc pc lfs Build cross-compiler cc1 using cc-pc on pc. 2 pc lfs lfs Build compiler cc-lfs using cc1 on pc. 3 lfs lfs lfs Rebuild and test cc-lfs using cc-lfs on lfs 여기서 1단계와 2단계가 cross compiler를 구축하는 단계이고 3단계는 chroot와 cross compiler를 이용한 LFS 시스템을 만드는 단계이다. cross compiler를 만드는 단계가 2단계로 구분되어 있는 이유는 하기와 같이 설명하고 있다.\nC언어는 단순컴파일러가 아니라 표준 라이브러리(glib)도 정의하는데 이는 cc1으로 컴파일되어야 함 그런데 컴파일러는 내부 라이브러리 libgcc를 이용해 컴파일하는데 이는 glib에 연결되어 있어야함 이 상호 의존 문제를 해결하기 위해 먼저 cc1 및 libgcc(스레드,예외처리 일부 기능이 부족한)를 만들고 이를 이용해 glib을 만듬(glib은 성능저하 없음) 그 다음 2단계에서 cc1을 이용해 cc-lfs를 만드며 이때 libgcc 및 llibstdc++도 다시 만듬 그리고 3단계(chroot) 환경에서 기존에 만든 패키지가 있음에도 다시 모든 패키지를 빌드하는데 이는 LSF를 안정적으로 만들어 준다.(2단계에서 만든 패키지들이 몇가지 디펜던시가 부족하므로)\n(이해하기 좀 빡세다\u0026hellip;해보면 이해간다는데 해봐도 머리속이 클리어하지 않다.)\nCompiling a Cross-Toolchain 위의 1단계, cc1을 빌드하고 glib을 만든다. 그리고 cc-lfs도 만들 수 있도록 libstdc++도 추출함\nBinutils-2.40 - Pass 1\nlinker, an assembler, and other tools for handling object files \u0026ndash;prefix=$LFS/tools, \u0026ndash;with-sysroot=$LFS 옵션으로 볼 때 host환경에서 쓴다는 것을 알 수 있다. GCC-12.2.0 - Pass 1\n-prefix=$LFS/tools, \u0026ndash;with-sysroot=$LFS 옵션으로 볼 때 host환경에서 사용 \u0026ndash;disable-shared에서 내부 라이브러리를 정적으로 쓰도록 함, 호스트의 동적 라이브러리로 인한 문제 차단 \u0026ndash;disable-libstdcxx 등 : 위에서 설명한 일부기능이 부족한 cc1이 됨 Linux-6.1.11 API Headers\n리눅스 커널에서 Glibc에 쓰이는 커널 API인 리눅스 API 헤더를 추출 Glibc-2.37\nGlibc 패키지는 메인 C 라이브러리를 포함하고 있다. 이 라이브러리는 메모리 할당, 디렉토리 검색, 파일 열기 및 닫기, 파일 읽기 및 쓰기, 문자열 처리, 패턴 대조, 산술 등의 기본 루틴을 제공한다 \u0026ndash;prefix=/usr 로 볼때 빌드된 것이 LFS 시스템 상에서 사용됨을 알 수 있다. \u0026ndash;host=$LFS_TGT, \u0026ndash;build=$(../scripts/config.guess) 는 이전의 gcc와 binutil등을 이용해서 크로스 컴파일함 Libstdc++ from GCC-12.2.0\nc++을 설치하기 위해서 Libstdc++가 필요한데 Glibc에 디펜던시를 가지므로 gcc 설치 시 설치하지 않았다. Glibc와 같이 \u0026ndash;prefix=/usr, \u0026ndash;host=$LFS_TGT, \u0026ndash;build=$(../scripts/config.guess) 구성을 사용 Cross Compiling Temporary Tools 위 기술노트의 2단계, cc-lfs를 구축한다.\nM4-1.4.19 매크로 처리기 Ncurses-6.4 문자 화면의 터미널 독립적 처리를 위한 라이브러리를 포함(clear, tab 같은) Bash-5.2.15 배쉬 쉘 Coreutils-9.1 기본적인 시스템 특성을 보여주고 설정하기 위한 유틸리티(cat, chmod, cp, dd 등등) Diffutils-3.9 diff 유틸 File-5.44 file 명령어 util Findutils-4.9.0 Findutils 패키지는 파일을 찾는 프로그램을 포함(find 등) Gawk-5.2.1 텍스트 편집에 최적화된 스크립트 언어 Grep-3.8 Gzip-1.12 Make-4.4 Patch-2.7.6 Sed-4.9 Tar-1.34 Xz-5.4.1 Binutils-2.40 - Pass 2 GCC-12.2.0 - Pass 2 Entering Chroot and Building Additional Temporary Tools 위의 빌드를 위한 임시 시스템에서 필요한 몇가지 패키지를 chroot의 분리된 환경에서 설치한다.\n상기 패키지를 host환경이 아닌 LFS 환경에서 설치하는 이유는 나와있지 않다. LFS ver9에서는 chroot로 LFS 환경으로 들어가기 이전에 모든 빌드 시스템 구축을 끝낸다.(이게 더 이해가 쉬운듯) Entering Chroot env and make VFS 현재까지 만들어진 파일 시스템은 lfs 소유인데 이는 다른 유저가 파일을 조작할 권리를 얻을 수 있으므로 소유자를 root로 변환\n1 2 3 4 chown -R root:root $LFS/{usr,lib,var,etc,bin,sbin,tools} case $(uname -m) in x86_64) chown -R root:root $LFS/lib64 ;; esac /dev 등 필요한 가상 커널 파일 시스템을 생성한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 mkdir -pv $LFS/{dev,proc,sys,run} mount -v --bind /dev $LFS/dev mount -v --bind /dev/pts $LFS/dev/pts mount -vt proc proc $LFS/proc mount -vt sysfs sysfs $LFS/sys mount -vt tmpfs tmpfs $LFS/run if [ -h $LFS/dev/shm ]; then mkdir -pv $LFS/$(readlink $LFS/dev/shm) else mount -t tmpfs -o nosuid,nodev tmpfs $LFS/dev/shm fi chroot를 이용하여 LFS환경으로 진입한다\n1 2 3 4 5 6 chroot \u0026#34;$LFS\u0026#34; /usr/bin/env -i \\ HOME=/root \\ TERM=\u0026#34;$TERM\u0026#34; \\ PS1=\u0026#39;(lfs chroot) \\u:\\w\\$ \u0026#39; \\ PATH=/usr/bin:/usr/sbin \\ /bin/bash --login 리눅스 표준 디렉토리 트리를 생성한다(리눅스 파일 시스템 계층 표준 기반)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 mkdir -pv /{boot,home,mnt,opt,srv} mkdir -pv /etc/{opt,sysconfig} mkdir -pv /lib/firmware mkdir -pv /media/{floppy,cdrom} mkdir -pv /usr/{,local/}{include,src} mkdir -pv /usr/local/{bin,lib,sbin} mkdir -pv /usr/{,local/}share/{color,dict,doc,info,locale,man} mkdir -pv /usr/{,local/}share/{misc,terminfo,zoneinfo} mkdir -pv /usr/{,local/}share/man/man{1..8} mkdir -pv /var/{cache,local,log,mail,opt,spool} mkdir -pv /var/lib/{color,misc,locate} ln -sfv /run /var/run ln -sfv /run/lock /var/lock install -dv -m 0750 /root install -dv -m 1777 /tmp /var/tmp 리눅스는 마운트된 파일시스템의 리스트를 /etc/mtab에 관리하고 이를 /proc에 노출시키므로 이를 위한 심볼릭 링크를 생성\nmtab은 mount command에 의해 자동갱신됨, fstab은 부팅 시 마운트할 목록 1 ln -sv /proc/self/mounts /etc/mtab host 파일과 /etc/passwd. /etc/group을 생성한다. 필요 시 테스트를 위한 테스트 그룹도 생성하고 재로그인 한다. 생성된 그룹은 어떤 표준에도 속하지 않는 Udev 구성의 요구사항에 의해 만들어진 그룹이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 cat \u0026gt; /etc/hosts \u0026lt;\u0026lt; EOF 127.0.0.1 localhost $(hostname) ::1 localhost EOF cat \u0026gt; /etc/passwd \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/dev/null:/usr/bin/false daemon:x:6:6:Daemon User:/dev/null:/usr/bin/false messagebus:x:18:18:D-Bus Message Daemon User:/run/dbus:/usr/bin/false systemd-journal-gateway:x:73:73:systemd Journal Gateway:/:/usr/bin/false systemd-journal-remote:x:74:74:systemd Journal Remote:/:/usr/bin/false systemd-journal-upload:x:75:75:systemd Journal Upload:/:/usr/bin/false systemd-network:x:76:76:systemd Network Management:/:/usr/bin/false systemd-resolve:x:77:77:systemd Resolver:/:/usr/bin/false systemd-timesync:x:78:78:systemd Time Synchronization:/:/usr/bin/false systemd-coredump:x:79:79:systemd Core Dumper:/:/usr/bin/false uuidd:x:80:80:UUID Generation Daemon User:/dev/null:/usr/bin/false systemd-oom:x:81:81:systemd Out Of Memory Daemon:/:/usr/bin/false nobody:x:65534:65534:Unprivileged User:/dev/null:/usr/bin/false EOF cat \u0026gt; /etc/group \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; root:x:0: bin:x:1:daemon sys:x:2: kmem:x:3: tape:x:4: tty:x:5: daemon:x:6: floppy:x:7: disk:x:8: lp:x:9: dialout:x:10: audio:x:11: video:x:12: utmp:x:13: usb:x:14: cdrom:x:15: adm:x:16: messagebus:x:18: systemd-journal:x:23: input:x:24: mail:x:34: kvm:x:61: systemd-journal-gateway:x:73: systemd-journal-remote:x:74: systemd-journal-upload:x:75: systemd-network:x:76: systemd-resolve:x:77: systemd-timesync:x:78: systemd-coredump:x:79: uuidd:x:80: systemd-oom:x:81: wheel:x:97: users:x:999: nogroup:x:65534: EOF exec /usr/bin/bash --login 마지막으로 login, agetty 및 init 들은 로그파일이 존재하지 않으면 로그를 남기지 않으므로 파일을 미리 생성해준다.\n1 2 3 4 touch /var/log/{btmp,lastlog,faillog,wtmp} chgrp -v utmp /var/log/lastlog chmod -v 664 /var/log/lastlog chmod -v 600 /var/log/btmp Building Additional Temporary Tools 하기 패키지를 설치한다.\nGettext-0.21.1 다국어화와 현지화를 위한 유틸리티가 포함, 프로그램을 현지 언어를 지원하도록 컴파일 할 수 있음 Bison-3.8.2 파서 생성기가 포함(?) Perl-5.36.0 Python-3.11.2 Texinfo-7.0.2 info 페이지를 읽고, 쓰고, 변환하는 프로그램(man보다 자세하다고 함) Util-linux-2.38.1 파일 시스템, 콘솔, 파티션 및 메시지를 처리하는 유틸리티가 포함된 패키지 Cleaning and Backup 툴체인 셋업이 완료되었으므로 필요하다면 현재구성을 백업하도록 하자. 상세 내용은 메뉴얼 참조\n","date":"2023-03-18T00:00:00Z","permalink":"https://muonkmu.github.io/p/lfs_prj-03-build-cross-toolchain/","title":"[LFS_prj] 03 Build Cross Toolchain"},{"content":"첫번째로 Host를 셋업하는 과정을 서술한다. 기본적으로 VMware에 우분투 20.04가 설치되었고 파티션이 분할 되었다는 가정하에 진행한다.\nHost 환경설정 LFS를 설치하고 빌드하는데 필요한 host의 환경을 설치하고 필요 package를 설치한다. 하기 링크를 참조한다.\nhttps://www.linuxfromscratch.org/lfs/view/stable-systemd/chapter02/hostreqs.html 상기 링크에는 host의 필요 패키지 버전 및 상태를 체크하는 스크립트도 제공되므로 실행해보도록 하자. 필자의 환경에서 스크립트를 실행 시 하기 문제점이 발견되서 수정하였다.\n/bin/sh의 심볼릭 링크가 dash가 아닌 bash로 설정 sudo dpkg-reconfigure dash 실행 후 no 선택 binutils 등 설치되지 않은 패키지 설치 binutils, bison, gawk, gcc, g++, make, texinfo 우분투에서 설치가능한 패키지의 버전을 확인하고 싶으면 apt-cache policy ${패키지 이름}으로 확인 가능하다. 필자는 패키지 버전을 특별히 버전을 매뉴얼과 맞추지 않았다.(필자는 호환을 보증하지 않는다고 표기된 패키지 버전만 신경씀) Partition Setting \u0026amp; mount 만약 설치시 LFS RootFS 및 SWAP 파티션을 설정하지 않았다면 파티션을 만들어 준다.\nhttps://www.linuxfromscratch.org/lfs/view/stable-systemd/chapter02/creatingfilesystem.html 필자는 하이퍼바이저 위에 우분투20.04를 설치했으므로 이미 Swap 공간이 /swapfile파일에 생성되어 있다. free 명령을 통해 swap공간의 존재를 확인할 수 있다. 그러므로 필자는 메뉴얼과는 다르게 별도 스왑 파티션을 생성하지 않았다. ~만약 swap 용량이 모자란다면 스왑파일 용량을 키우거나 스왑 파티션을 생성하자~ 스왑 파티션이 없으면 나중에 LFS로 부팅했을 때 swap영역이 없으므로 만들어 주는게 좋을 듯 하다. 아님 향후 LFS용 swapfile을 만들자\nLFS RootFS 파티션과 Swap 파티션을 생성한다. RootFS : fdisk 에서 커맨드 n(생성) -\u0026gt; p(primary) 로 생성 (필요시)swap : fdisk 에서 커맨드 n(생성) -\u0026gt; p(primary) 로 생성 후 t(type) -\u0026gt; 코드 82 w로 저장 해당 파일 시스템으로 포맷 1 2 sudo mkfs -v -t ext4 /dev/${RootFS_파티션} (필요시)sudo mkswap /dev/${swap_partition} 전역변수 설정 및 파티션 마운트 \u0026lsquo;mount -l\u0026rsquo; 명령으로 nosuid나 또는 nodev가 아닌지 확인하자. 1 2 3 4 export LFS=/mnt/lfs sudo mkdir -pv $LFS sudo mount -v -t ext4 /dev/sda2 $LFS sudo cat \u0026#34;/dev/sda2 /mnt/lfs ext4 default 1 1\u0026#34; \u0026gt;\u0026gt; /etc/fstab 패키지와 패치 다운로드 기본적인 리눅스 시스템을 구축하기 위해 다운로드 해야 하는 패키지와 패치를 다운로드 한다. (아마 빌드를 위한 sysroot 구축이라 생각되는데?? 아직은 확실하지 않음)\nsource 저장을 위한 폴더 생성 및 권한 설정 패키지 및 패치 다운 패키지와 패치를 다운로드 할때 수동으로 하나씩 받기 힘드므로 리스트 파일을 제공한다. 정상적으로 다운 받았는지 확인하기 위한 md5 체크섬 확인 체크섬 확인을 위한 파일도 제공하므로 이를 이용 1 2 3 4 5 6 7 8 sudo mkdir -v $LFS/sources sudo chmod -v a+wt $LFS/sources wget --directory-prefix=$LFS/sources https://www.linuxfromscratch.org/lfs/view/stable-systemd/wget-list-systemd wget --input-file=wget-list-systemd --continue --directory-prefix=$LFS/sources wget --directory-prefix=$LFS/sources https://www.linuxfromscratch.org/lfs/view/stable-systemd/md5sums pushd $LFS/sources md5sum -c md5sums popd 다운로드한 패키지와 패치의 리스트는 다음 페이지에 나와 있다.\n패키지 : https://www.linuxfromscratch.org/lfs/view/stable-systemd/chapter03/packages.html 패치 : https://www.linuxfromscratch.org/lfs/view/stable-systemd/chapter03/patches.html 최종 준비 크로스 컴파일을 위한 유저 추가 및 bash 환경 설정. 각 절차에서 나타난 옵션에 대한 자세한 설명은 매뉴얼 페이지를 참조한다.\n최소 LFS Filesystem layout 구성 LFS의 빌드를 위한 크로스 컴파일러 생성 시 필요한 최소한의 Filesystem layout을 구축한다.(필자의 추론) 크로스 컴파일용 glib이나 libstdc++등이 여기에 설치되고 LFS에 포함되는 시스템은 상기 폴더에 덮어써진다. Filesystem Layout을 위한 심볼릭 링크도 만들고 그 후 크로스 컴파일러가 설치 될 폴더를 생성한다.(이는 향후 LFS 시스템에 포함되지 않으므로 분리해놓음)\n1 2 3 4 5 6 7 8 sudo mkdir -pv $LFS/{etc,var} $LFS/usr/{bin,lib,sbin} for i in bin lib sbin; do ln -sv usr/$i $LFS/$i done case $(uname -m) in x86_64) mkdir -pv $LFS/lib64 ;; esac sudo mkdir -pv $LFS/tools LFS빌드를 위한 유저 추가 빌드를 위한 환경 구성을 위해 별도의 유저를 추가\nlfs 유저추가 및 비번 설정 생성된 파일 시스템의 소유자를 lfs로 변경 /etc/bash.bashrc 이름 변경 (우분투20.04의 경우 lfs로 로긴 시 해당 파일이 쉘 환경을 변화시키는 것을 막기 위함) lfs로 로긴 1 2 3 4 5 6 7 8 9 sudo groupadd lfs sudo useradd -s /bin/bash -g lfs -m -k /dev/null lfs chown -v lfs $LFS/{usr{,/*},var,etc,tools} chown -vh lfs $LFS/{bin,lib,sbin} case $(uname -m) in x86_64) chown -v lfs $LFS/lib64 ;; esac sudo mv -v /etc/bash.bashrc /etc/bash.bashrc.NOUSE su - lfs lfs 유저 환경설정 lfs 유저를 위환 쉘의 환경을 설정한다. 설정에 대한 상세 내용은 매뉴얼을 참조한다.\n.bash_profile은 Login Shell에서 실행(ex. ssh 로그인 쉘)되며 하기와 같이 설정 1 2 3 cat \u0026gt; ~/.bash_profile \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; exec env -i HOME=$HOME TERM=$TERM PS1=\u0026#39;\\u:\\w\\$ \u0026#39; /bin/bash EOF .bashrc은 Non-Login Shell에서 실행(ex. 로그인된 상태에서 쉘이 실행될 때)되며 하기와 같이 설정 1 2 3 4 5 6 7 8 9 10 11 12 cat \u0026gt; ~/.bashrc \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; set +h umask 022 LFS=/mnt/lfs LC_ALL=POSIX LFS_TGT=$(uname -m)-lfs-linux-gnu PATH=/usr/bin if [ ! -L /bin ]; then PATH=/bin:$PATH; fi PATH=$LFS/tools/bin:$PATH CONFIG_SITE=$LFS/usr/share/config.site export LFS LC_ALL LFS_TGT PATH CONFIG_SITE EOF 프로파일을 적용한다. 1 source ~/.bash_profile ","date":"2023-03-12T00:00:00Z","permalink":"https://muonkmu.github.io/p/lfs_prj-02-host-setting/","title":"[LFS_prj] 02 Host setting"},{"content":"리눅스의 구조를 조금 더 알아보고자 LFS를 진행해보기로 했다. 리눅스를 부트로더부터 쉘까지 구축하는 프로젝트인데 수행하고 나면 리눅스 패키지의 디펜던시를 알 수 있다고 한다. 즉, Roll Your Own(RYO) 기법으로 리눅스를 구축하는 것이다.\n목표 현재 LFS의 버전은 11.3 이며 stable 버전은 init프로그램의 종류에 따라 sysvinit 버전과 systemd 버전으로 나뉜다. 요즘 추세에 따라 LFS v11.3-systemd 를 구성하는 것을 목표로 한다.\n개발 환경 VMware를 환경에서 우분투를 설치하여 진행하였다.\n노트북에 우분투가 깔려 있었지만 LFS 마운트용 파티션 분할이 SSD 파편화 문제로 실패해서.. 다른 글들을 보면 ubuntu LiveCD boot 환경에서 진행할 수 있다고 하는데 필자는 그냥 우분투를 설치하였다. 파티션 구성은 하기와 같다.\nsda1(primary 25Gb) : ubuntu 20.04 sda2(primary 35Gb) : LFS mount 참고 공식 매뉴얼 : https://www.linuxfromscratch.org/lfs/view/stable-systemd/ 번역서(9.1버전) : https://rawcdn.githack.com/NuttyJamie/LinuxFromScratch-for-Korean/88bdabae8abf2fad511b497b0dc676e6ac95b965/9.1/BOOK/HTML/index.html 참고 자료 : http://soopsaram.com/lfs/ ","date":"2023-03-04T00:00:00Z","permalink":"https://muonkmu.github.io/p/lfs_prj-01-lfs-project-propsal/","title":"[LFS_prj] 01 LFS project propsal"},{"content":"SCNN Accelerator SCNN은 sparse encoding scheme을 이용해서 activation / weight sparsity 지원 Planar Tiled-Input Stationary-Cartesian Product-sparse (PT-IS-CP-sparse)라 부르는 새로운 Cartesian product flow를 제안 (activation / weight reuse) SCNN PT-IS-CP-Dense Dataflow PT-IS-CP-Dense dataflow는 convolution nested loop를 어떻게 분해할 것인가에 관한 것\nC X R X S 형태의 K개 filter, batch size N인 C X W X H 형태의 input activation 일 때 Input Stationary (IS) 가 적용되면 loop order는 C→W→H→K→R→S 가 됨 성능향상을 위해 blocking strategy 적용 (K output channel은 $K_c$ 사이즈의 K/$K_c$ output channel group으로 분리) K/$K_c$→C→W→H→$K_c$→R→S intra-PE parallelism을 위해 PE 내부에서 spatial reuse 활용\nfilter weight(F)와 input activation(I)가 각 buffer에서 fetch되고 이는 F X I array 곱셈기로 전송 filter weight와 input activation은 재활용 되며 partial sum은 향후 연산을 위해 메모리 접근 없이 저장됨 intra-PE parallelism을 위해 Spartial tiling 전략이 사용됨\nW X H input activation는 $W_t$ X $H_t$ Planar Tiles(PT)로 나눠져서 PE로 분배됨 또한 mutiple channel processing 지원 (C X $W_t$ X $H_t$이 PE에 할당됨) sliding window operation에서 edge에서 cross-tile dependency가 생기는데 data halo를 이용해 해결\nInput Halos : PE input buffer는 halo을 수용하기 위해 C x Wt x Ht보다 약간 큰 크기로 조정 Output Halos : PE accumulation buffer도 halo을 수용하기 위해 Kc x Wt x Ht보다 약간 큰 크기로 조정. Halo에는 출력 채널 계산이 끝날 때 누적을 완료하기 위해 인접 PE와 통신하는 불완전한 부분 합계가 포함. PT-IS-CP-Dense Dataflow의 최종 수식은 다음과 같다 SCNN PT-IS-CP-Sparse Dataflow PT-IS-CP-Sparse는 PT-IS-CP-Dense dataflow에서 파생되었고 filter weight와 input activation의 sparsity를 지원 filter weight는 Kc X R X S sparse block으로 압축, input activation은 Wt X Ht 사이즈 블럭으로 엔코딩 PE는 nonzero F 와 nonzero I를 곱해서 partial sum은 accumulator buffer에 output index와 저장됨 PT-IS-CP-Sparse는 compressed sparse index input activation / filter weigth / accumulator buffer를 패치 할 수 있도록 수정됨 SCNN Tiled Architecture SCNN은 Tiled architecture로 PT-IS-CP-Sparse를 지원\nPE는 halo를 교환 하기 위해 인접 PE와 연결되며 Layer Sequencer는 PE와 DRAM의 데이터 이동을 제어 Processing Element Architecture PE는 weight buffer, input/output activation RAM (IARAM/OARAM), multiplier array, scatter crossbar, accumulation buffer, Post-Processing Unit (PPU)으로 구성\ninput activation과 filter weight가 PE로 로드되고 multiplier array가 partial sum을 계산 후 acculumlation buffer에 저장 acculumlation buffer는 adder와 output channel entry를 가지고 있으며 double buffers 전략 사용 1개 버퍼는 partial sum을 계산 하고 다른 것은 output을 후처리를 위해 PPU로 전송 PPU는 몇 가지 다른 task를 수행 halo 영역을 위해 인접 PE와 partial sum을 교환 nonlinear activation, pooling, dropout 수행 output activation을 압축하고 ORAM에 씀 Data Compression filter weight와 input/output activation을 압축하기 위해 다른 것과 약간 수정된 엔코딩 방식 사용(책의 그림 참조)\nData vector : 0이 아닌 element 저장 index vector : 0이 아닌 element의 갯수와 이전에 0인 element의 갯수를 저장 SeerNet Accelerator Microsoft SeerNet은 quantizaition convolution을 이용해서 feature map sparsity를 예상하는 방법을 제안\nFeature Map(F)와 filter weight(W)는 Fq와 Wq로 양자화 되고 이를 이용해 quantized low bit inference를 수행하여 binary sparsity mask(M)을 생성 그리고 full precision sparse inference를 수행(앞의 M을 이용하는 듯) Low-Bit Quantization Low-Bit Quantization은 online/offline에서 filter weight를 양자화\noutput feature map의 dimenstion이 H*W 일 때 양자화 복잡도는 1/(HW) online 동작은 낮은 연산 복잡도와 오버헤드로 병렬처리를 제공하고 offline 동작은 추가 저장공간으로 양자화 오버헤드를 제거함 online quantization동안 binary mask 생성을 위한 quantized convolution이 수행되고 이 마스크를 가지고 spase convolution이 수행 됨 Efficient Quantization Full quantization 대신에 layer-by-layer quantization에 집중하고 output feature map을 예측하기 위한 low-bit quantization 적용\nReLU의 경우 output feature map의부호를 찾고 음수을 0으로 출력 시킴 Max pooling의 경우 정확도 없이 output feature map의 가장 큰 값만 찾음 Quantization flow 하기와 같음\nn-1이 양/음의 범위를 모두 커버하는 양자화 레벨 2n-1을 정의 최대 절대값 M을 찾음 양자화 값 x\u0026rsquo; = floor(X/M*2^(n-1)) Quantized Convolution 책에 Quantized Convolution, Quantized ReLU activation, Quantized batch normalization 수식 있음\nInference Acceleration Inference 성능향상을 위해 Intel AVX2 vector 연산 사용\nSparsity-Mask Encoding sparse convolution 성능 향상을 위해 row/column index vector를 이용해 sparsity mask를 엔코딩\nfeature map을 vector format으로 변환(다수의 feature map은 matrix 형태가 됨) column index에는 sparse bit의 column 위치 row index 에는 각 row column의 시작위치가 있음(책에 그림 볼 것) ","date":"2023-03-01T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-2/2/","title":"[AI HW Design] Chap08 Network Sparsity (2/2)"},{"content":"개인 프로젝트를 진행하는데 Xilinx에서 제공하는 레퍼런스의 개발환경이 제각각이다. 우선 기본적으로 IDE 2019.2 버전이 필요한데 현재 데스크탑에 설치되어 있는 개발환경은 2022.1이다. 재 설치를 하기 보다는 Docker로 가상화 공간에 별도의 개발환경을 설치해 보자\nDocker GUI How to use a GUI on the docker Docker에서 개발환경을 가상화 하는 것은 좋은데 Docker는 기본적으로 CLI 환경만 제공한다. 그러나 많은 임베디드 개발환경은 설치 및 실행에서 GUI를 필요로 하기에 Docker에서 GUI를 구성하는 방법은 아래와 같다.\ndesktop 환경이 설치된 Docker에서 VNC 이용 SSH 기반에서 X11 Forwarding을 이용해 HOST에서 GUI를 띄운다. 1번째 방법은 GUI docker image를 다운받아 설정하고 여기에 ssh의 ForwardX11 옵션을 활성화 하면 2번째 방법도 지원이 된다.\nDocker image 빌드 및 실행 선구자가 LXDE 환경의 Docker를 개발하였다. 하기 사이트에서 사용법을 읽어보자.\nhttps://hub.docker.com/r/dorowu/ubuntu-desktop-lxde-vnc/ 위의 이미지를 그대로 사용해도 되지만 시간 설정 등 몇가지 설정이 다르므로 이미지를 빌드하자. 아래는 필자의 개발환경 구성을 위한 dockerfile이다. 필자는 기본쉘로 zsh를 사용하지만 petalinux는 bash 환경에서 실행해야 함을 주의하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 #from ubuntu20.04-vnc-dockerfile FROM dorowu/ubuntu-desktop-lxde-vnc:latest # timezone setup ARG DEBIAN_FRONTEND=noninteractive ENV TZ=Asia/Seoul # package add RUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add - \\ \u0026amp;\u0026amp; apt-get update \\ \u0026amp;\u0026amp; apt-get install -y vim git openssh-server zsh fonts-powerline tzdata tio byobu language-pack-en language-pack-ko \\ \u0026amp;\u0026amp; update-locale \\ \u0026amp;\u0026amp; apt autoclean -y \\ \u0026amp;\u0026amp; apt autoremove -y # xilinx user add and password setup, and usb dev group add RUN useradd -m -s /bin/zsh xilinx \\ \u0026amp;\u0026amp; echo \u0026#39;xilinx:xilinx\u0026#39; | chpasswd \\ \u0026amp;\u0026amp; echo \u0026#34;xilinx ALL=(ALL:ALL) ALL\u0026#34; \u0026gt;\u0026gt; /etc/sudoers \\ \u0026amp;\u0026amp; usermod -a -G dialout xilinx \\ \u0026amp;\u0026amp; usermod -a -G plugdev xilinx EXPOSE 80 WORKDIR /home/xilinx ENV HOME=/home/xilinx \\ SHELL=/bin/zsh HEALTHCHECK --interval=30s --timeout=5s CMD curl --fail http://127.0.0.1:6079/api/health ENTRYPOINT [\u0026#34;/startup.sh\u0026#34;] ~ 도커를 실행해보자. 하기 파일은 필자의 실행 스크립트이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 #!/bin/bash CON_UBUNTU_VER=\u0026#34;20.04\u0026#34; CON_NAME=\u0026#34;xilinx-dev\u0026#34; CON_TAG=\u0026#34;2023-3\u0026#34; CON_USR_NAME=\u0026#34;xilinx\u0026#34; CON_USR_PWD=\u0026#34;xilinx\u0026#34; MAP_PORT_SSH=\u0026#34;8091\u0026#34; MAP_PORT_VNC=\u0026#34;8092\u0026#34; MAP_PORT_HTTP=\u0026#34;8093\u0026#34; VNC_PWD=\u0026#34;xilinx\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;######################################################\u0026#34; echo \u0026#34;ubuntu $CON_UBUNTU_VER LXDE Base Docker\u0026#34; echo \u0026#34;$CON_NAME platform : xilinx-ide \u0026#34; echo \u0026#34;Port SSH: $MAP_PORT_SSH, VNC: $MAP_PORT_VNC, HTTP: $MAP_PORT_HTTP\u0026#34; echo \u0026#34;######################################################\u0026#34; echo \u0026#34;\u0026#34; echo -n \u0026#34;Select Command 1)Run-Docker 2)Attach-Bash 3)Remove-Docker : \u0026#34; read selCmd if [ $selCmd = \u0026#34;1\u0026#34; ]; then echo -n \u0026#34;Select Resolution 1)FHD 2)QHD 3)UHD 4)Manual : \u0026#34; read selRes if [ $selRes = \u0026#34;1\u0026#34; ]; then resVal=\u0026#34;1920x1080\u0026#34; elif [ $selRes = \u0026#34;2\u0026#34; ]; then resVal=\u0026#34;2560x1440\u0026#34; elif [ $selRes = \u0026#34;3\u0026#34; ]; then resVal=\u0026#34;3840x2160\u0026#34; elif [ $selRes = \u0026#34;4\u0026#34; ]; then echo -n \u0026#34;Input Resolution ex)1920x1080 : \u0026#34; read resVal else resVal=\u0026#34;1920x1980\u0026#34; echo \u0026#34;Undefined Num, Set resolution FHD\u0026#34; fi sudo docker run -d --privileged \\ -p $MAP_PORT_HTTP:80 -p $MAP_PORT_VNC:5900 -p $MAP_PORT_SSH:22 \\ -e VNC_PASSWORD=$VNC_PWD -e RESOLUTION=$resVal -e USER=$CON_USR_NAME -e PASSWORD=$CON_USR_PWD \\ -v /dev/shm:/dev/shm -v /dev/bus/usb:/dev/bus/usb -v $HOME/Workspace:/home/$CON_USR_NAME/Workspace \\ --name $CON_NAME $CON_NAME:$CON_TAG elif [ $selCmd = \u0026#34;2\u0026#34; ]; then sudo docker exec -it $CON_NAME /bin/zsh -c \u0026#34;su - $CON_USR_NAME\u0026#34; elif [ $selCmd = \u0026#34;3\u0026#34; ]; then sudo docker stop $CON_NAME sudo docker rm $CON_NAME else echo \u0026#34;Input the collect decimal number\u0026#34; fi 이후 Docker의 VNC에 연결하여 진행한다. 기본적으로 필요한 패키지들을 설치하고 만약 .bashrc가 없어서 터미널의 색깔이 이상하다면 해당파일을 복사한다.\n1 cp /etc/skel/.bashrc $HOME Petalinux Install 사전준비 필요 시 /bin/sh가 bash가 아니라면 변경한다. 하기 명령을 수행 후 no을 선택하면 bash로 변경됨 1 sudo dpkg-reconfigure dash 필요 패키지들을 설치한다. https://docs.xilinx.com/v/u/2019.2-English/ug1144-petalinux-tools-reference-guide 1 2 3 sudo dpkg --add-architecture i386 sudo apt update gawk make net-tools libncurses5-dev tftpd zlib1g:i386 libssl-dev flex bison libselinux1 gnupg wget diffstat chrpath socat xterm autoconf libtool tar unzip texinfo zlib1g-dev gcc-multilib build-essential screen pax gzip python2.7 cpio 필요 시 locale에 en_US.utf8을 추가한다. 1 2 sudo apt-get install -y locales sudo locale-gen en_US.utf8 Install Petalinux 설치파일을 다운로드 하고 설치한다. sudo 권한 없이 설치해야하므로 설치폴더의 소유자는 현재 user로 한다.\n1 2 3 4 sudo mkdir /tools/Xilinx/Petalinux/2019.2 sudo chown -R xilinx:xilinx /tools/Xilinx/Petalinux chmod +x ${설치파일} ./${설치파일} /tools/Xilinx/Petalinux/2019.2 Vivado Install 설치파일을 다운로드 하고 설치를 진행한다.(sudo 권한으로 설치?) 필요 시 libtinfo5f를 설치한다. 1 2 3 chmod +x ${설치파일} sudo ./${설치파일} sudo apt-get install libtinfo5 Backup and import 레이어 병합을 위하여 docker의 export 명령으로 출력 시칸다. 1 sudo docker export --output=\u0026#34;${파일이름}\u0026#34; ${컨테이너명} 백업된 파일을 입력시키고 싶다면 다음을 수행한다. 1 sudo docker import -c \u0026#39;EXPOSE 80\u0026#39; -c \u0026#39;WORKDIR /root\u0026#39; -c \u0026#39;ENV HOME=/home/ubuntu SHELL=/bin/bash\u0026#39; -c \u0026#39;HEALTHCHECK --interval=30s --timeout=5s CMD curl --fail http://127.0.0.1:6079/api/health\u0026#39; -c \u0026#39;ENTRYPOINT [\u0026#34;/startup.sh\u0026#34;]\u0026#39; 아카이브.tar 이미지이름:tag Etc ssh 설치 후 자동 실행되지 않는다면 /startup.sh 파일에 ssh 서비스 스타트 명령을 기입한다. 상기 파일의 마지막 줄은 헬스체크 프로세스 시작 명령이므로 이 위에 기입하면 된다. ","date":"2023-02-28T00:00:00Z","permalink":"https://muonkmu.github.io/p/xilinx-ide-installation-based-on-docker/","title":"Xilinx IDE installation based on Docker"},{"content":"8장은 시스템 throughput을 높이기 위해 ineffectual zoro operation을 스킵하는 다양한 구조에 대해 알아본다. (feature map encoding/indexing, weight sharing/pruning, quantized prediction)\nEnergy Efficient Inference Engine (EIE) 스탠포드 대학에서 나온 유명한 논문, 알고리즘(SW) + 가속기(SW) 양쪽에 대해 최적화한 논문으로 알고 있다. 중요도에 비해 설명이 부실한 듯\u0026hellip;\nEIE는 sparse matrix-vector mutiplication을 위한 compressed network model을 지원하고 weight sharing을 다룸 Leading Non-Zero Detection Network, Central Conrol Unit, Processing Element 로 구성 Leading Non-Zero Detection Network (LNZD) LNZD는 input activation으로 부터 nonzero element를 찾아내고 이를 LNZD node로 먹임 node는 nonzero value와 index를 PE로 브로드캐스팅 함 LNZD에 PE는 4개가 연결 Central Control Unit (CCU) CCU는 network segment computation을 제어\nhost와 커뮤니케이션 하고 PE state를 모니터링 함 두가지 동작모드로 나뉨 computing mode : CCU는 LNZD로 부터 nonzero input activation을 받고 이를 PE로 브로드캐스팅, 모든 input channel이 스캔될때 까지 반복됨 I/O mode : PE는 idle, activation과 weight가 DMA에 의해 접근 됨 Processing Element (PE) 요약하면 ifmap 값을 읽어와서 여기 포인터를 이용하여 해당 weight 값과 효율적으로 곱한다는 이야기인듯..\nPE는 activation queue, pointer read unit, sparse matrix unit, arithmetic unit, activation R/W unit으로 구성 computation 동안 CCU는 nonzero input element와 index를 activation queue에 브로드캐스팅 함, PE는 큐가 다차면 input element를 처리(브로드캐스트는 중지) activation queue는 PE가 work backlog를 구축할 수 있도록 해줌(load balancing 문제 해결) pointer read unit은 activation queue의 인덱스를 이용하여 nonzero element의 시작/종료 포인터를 찾음 싱글 사이클에 이를 처리하기 위해 포인터는 LSB와 함께 odd/even SRAM에 저장(의미를 잘 모르겠음) sparse matrix read unit은 sparse matrix memory로 부터 포인터를 이용하여 0이 아닌 값을 읽음(fmap?) arithmetic unit은 activation queue와 sparse matrix memory의 0이 아닌 값 MAC 연산 연속적으로 가산기 사용되는 경우를 위한 bypass 경로 존재 (그림 보자) activation read/write unit의 경우 fully connected layrer를 위한 source/destination activation register를 가지고 있으며 이는 다음 레이어 연산시 교체됨 Deep Compression EIE는 pruning과 weight sharing을 통해 네트워크 압축하기위해 Deep Commpression을 적용, 적용 예는 다음과 같음 (책에 그림 예시를 보자) 4X4 weight matrix라면 16개의 값을 4개의 index(code book)로 만듬 index에 해당하는 weight는 Gradient를 가지고 fine-tunnig됨 MAC 연산은 weight와 input activation vector가 0이 아닌 값에 대해서 수행 EIE는 interleaved Commpressed Sparse Column(CSC) 적용 (Eyeriss와 약간 다르므로 책참조) v는 0이 아닌 값, z는 0이 아닌 값 해당 v 값 앞에 0의 개수 v,z는 하나의 large array pair에 $p_j$(벡터의 시작 포인터), $p_{j+1}$(마지막 항목 다음 번 포인터)와 함께 저장됨 Sparse Matrix Computation 4개의 PE에서 input activation vector(a)는 weight matrix(w)와 곱해짐 a를 스캔해서 0이 아닌 $a_j$는 인덱스 값과 함께 브로드캐스팅 됨 PE는 index에 대응하는 0이 아닌 $w_j$를 곱합 PE는 벡터 v를 $p_j$에서 $p_j+_1$까지만 스캔 (책에 예시 그림 있으니 참조) Cambricon-X Accelerator 병렬연산에서 Nonzero neuron을 선택하기 위해 indexing scheme을 적용 Control Processor (CP), Buffer Controller (BC), Input Neural Buffer (NBin), Output Neural Buffer (NBout), Direct Memory Access (DMA) Module, Computation Unit (CU)으로 구성 중요 element는 BC Tn indexing unit(nonzero neuron을 인덱싱하는 유닛이며 PE와 수가 같다) Computation Unit (CU) CU는 Tn개 PE로 구성되며 모든 PE는 fat tree 형태로 연결 PE는 PE Functional Unit(PEFU)와 Synapse Buffer(SB)로 구성 BC로 부터 neuron을, local BC로 부터 synaps를 읽어서 PEFU에 제공하며 output neuron은 BC에 다시 씀 Tn개 PEFU는 Tm개 곱셈기와 가산기를 가져서 TnXTm 곱셈이 가능 SB는 synapse를 저장하고 메모리 access를 최소화 하기 위해 디자인, 책에서는 하기 예시를 듬(책그림 참조) PE는 4개이고 output neuron 0이 input neruon 2개 연결, output neuron 1이 input neruon 5개 연결 output neuron 0의 weight는 address 0에 output neuron 1의 weight는 address 1/2의 SB에 저장 output neuron 0 계산 시 SB를 1번 읽고 output neuron 1은 두 번 읽음 synapse의 수는 output neuron마다 다를 수 있기 때문에 SB가 비동기적으로 데이터를 로드하여 전체 성능을 향상 Buffer Controller BC는 Indexing Module (IM)과 BC Functional Unit (BCFU) 로 구성 BCFU는 인덱싱을 위해 neuron을 저장 IM은 BC의 nonzero nueron을 구분하고 nonzero indexed nueron만 전송 BC는 input neurons을 NBin에서 PE로 보내거나 BCFU로 제공, PE의 계산결과는 BCFU에 저장 또는 NBout에 쓰여짐 IM에는 두가지 하기 두가지 옵션이 있음 (책에 그림에 잘 나와 있음) direct indexing : nonzero nueron의 여부를 0/1로 표현한 binary string 사용 step indexing : nonzero nueron의 거리를 사용 step indexing이 area와 power를 적게 소모함 ","date":"2023-02-25T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/","title":"[AI HW Design] Chap08 Network Sparsity (1/2)"},{"content":"6장은 In-Memory Computation, 7장은 Near-Memory Computation 에 대한 내용이다. 현재 개발하고자 하는 가속기와 동떨어지는 내용이라 판단해서 개요만 보고 스킵할 예정이다. 삼성이나 하이닉스를 다녀야 쓸모있지 않을까 싶다.\nIn-Memory Computation 여기서는 메모리와 로직을 stacking 하는 방식의 Processor-In-Memory(PIM)에 대해 설명한다. 다른 방식의 PIM도 있는 걸로 아는데\u0026hellip;\nNeurocube Architecture Nerocube는 parallel neural processing unit과 High Bandwidth Memory(HBM)을 스택한 Hybrid Memory Cube(HMC)를 이용 이는 stacked memrory에서 PE로 직접 데이터 로드가 가능함(레이턴시 감소) Teris Accelerator Teris는 Eyeriss의 3D Memory와 함께 Row Stationary(RS) dataflow 채택 NeuroStream Accelerator NeuroStream은 HMC의 modular extension인 Smart Memory Cube(SMC)를 이용 Near-Memory Computation DiDianNao Supercomputer 대용량 eDRAM을 통해 DianNao의 memory bottleneck을 해결하고자 함 모든 synapse를 수용할 후 있는 거대 storage를 제공하는 Neural Function Unit(NFU)를 지닌 16개의 tile로 구성 NFU는 4개의 eDRAM bank와 time-interleaved 통신(?) 함 (eDRAM의 레이턴시가 크기 때문) Cnvlutin Accelerator DiDianNao에서 파생되었으며 다수의 DiDianNao를 고속 인터페이스로 연결, 거대 parallel mutiplication lane 구조 채택 ","date":"2023-02-21T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap06-07-in/near-memory-computation/","title":"[AI HW Design] Chap06 \u0026 07 In/Near Memory Computation"},{"content":"앞의 포스트가 너무 길어져서 Eyeriss version2 부분은 현 포스트로 나눔\nEyeriss Accelerator Ver.2 irregular data pattern 과 network sparsity 지원을 위한 Eyeriss V2 공개\n새로운 NoC 구조 : data reuse가 적을 땐 external memory에서 더 많은 데이터를 PE로 가져오고 data reuse가 많을 땐 spartial Data를 sharing CSC 엔코딩 적용 RS+ 적용 Eyeriss V1이 GLB\u0026lt;-\u0026gt;PE 연결을 위해 flat multicast NoC를 사용했지만 V2에서는 flexible and mesh NoC 사용, 이 hierarchical mesh는 GLB cluster, Router Cluster, PE cluster로 구성되며 하기 3가지 타입의 데이터 이동 지원\nifmap은 GLB cluster에 로드됨, 이는 GLB 메모리에 저장되고 Router Cluster로 전송 psum은 GLB 메모리에 저장 되고, ofmap은 external memory에 바로 저장 됨 fmap은 Router Cluster로 전송되고 PE spad에 저장 (책에 v1 과 v2 구조에 대한 비교 그림이 있으니 찾아보자) Hierarchical Mesh Network (HM-NoC) 전통적인 Network-on-Chip 구조는 하기와 같으며 장단점을 가지고 있음 (책 그림 참조)\nBroadcast Network : high reuse / low bandwidth Unicast Network : high bandwidth / low reuse All-to-All Network : high reuse, high bandwidth / scale difficulty Eyeriss V2는 RS+를 지원하기 위해 HM-NoC 구조를 제안, all-to-all network에서 파생되었으나 4가지 모드를 가짐\nBroadcast: single input and single weight Unicast: multiple inputs and multiple weights Grouped multicast: shared weights Interleaved multicast: shared inputs HM-NoC는 source, destination, router로 구성되며 design phase에서 cluster로 그룹핑되고 operation mode에서는 고정됨.\nRouter cluster가 다른 cluster와 one-to-one, many-to-many, source/destination 구조로 연결 Router cluster는 4개의 source/destination port를 가지며 4가지 routing mode (broadcast, unicate, grouped/interleaved multicast)를 가짐 책에서는 다음과 같이 예시를 듬\nConvolution layer : ifmap과 fmap이 reuse 되며 grouped multicast 또는 interleaved mode 로 구성 Depth-wise Convolution layer : fmap만 reuse 되며 fmap이 PE로 broadcast, ifmap은 GLB에서 로드 Fully connected layer : ifmap이 모든 PE로 broadcast, fmap은 unicast mode로 로드 Input Activation HM-NoC Router Cluster 안의 3개 ifmap router는 GLB cluster의 ifmap SRAM과 연결 ifmap routerd의 3개의 source/destination port 다른 클러스터와 연결, 1개는 메모리에서 데이터로드, 1개는 PE 연결 책에 그림과 상세 설명이 있으니 참조하자 Filter Weight HM-NoC Router Cluster 안의 각 fmap router는 PE cluster 안의 PE row와 연결 vertical mesh는 사라지고 horizontal mesh 만 데이터 reuse를 위해 남음 책에 그림과 상세 설명이 있으니 참조하자 Partial Sum HM-NoC Router Cluster 안의 4개의 psum router는 GLB cluster의 psum SRAM과 PE cluster의 PE column과 연결됨 horizontal mesh는 사라지고 vertical mesh만 psum accumulation을 위해 남음 책에 그림과 상세 설명이 있으니 참조하자 Compressed Sparse Column (CSC) Format Eyeriss V2는 ifmap과 fmap 둘 다에 CSC 포맷 적용 (zero operation skipping), CSC 포맷의 구조는 다음과 같다\nData vector : 0이 아닌 값 Counter vector : Data vector의 item 기준, 해당 열에서 앞에 있는 0의 값의 갯수 Address vector : 각 열을 기준으로 이전 열까지 0이 아닌 item의 누적 갯수 Eyeriss V2는 PE는 zero operation skip을 위해 7 pipeline stage와 5 spad (ifmap, fmap, psum 저장)로 수정\n첫째로 non-zero data인지 확인하기 위해 address를 검사하고 fmap 로드 전 먼저 ifmap을 로드(zero ifmap skip을 위해) ifmap/fmap이 0이 아니면 계산 pipeline 수행, fmap이 0이면 pipeline disable Row Stationary Plus (RS+) Dataflow PE utilization을 높이기 위해 RS+ dataflow 적용\nmodel dimension을 다른 PE dimension에 매핑하기 위해 데이터를 tiling, spatial fragmentation 함 depth-wise convolution 시 PE utilization이 낮은 문제점 해결 ","date":"2023-02-19T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/","title":"[AI HW Design] Chap05 Convolution Optimization (3/3)"},{"content":"유명한 MIT의 Eyeriss Accelerator 논문이다. 아직까지 관련 프로젝트가 진행 중인 것으로 보이며 찾아보면 관련 논문에 대하여 리뷰해논 자료가 꽤 많다. 잘 소개된 곳 한 곳 (허락없는 링크..)\nhttps://m.blog.naver.com/PostView.naver?isHttpsRedirect=true\u0026blogId=kangdonghyun\u0026logNo=220990374125 Eyeriss Accelerator Eyeriss Accelerator는 data access를 최소화하기 위한 Row Stationary (RS) Dataflow를 제안하며 다음과 같은 특징을 지님\nSpartial architecture with sequential processing configuration Spartial architecture can exploit high compute parallelism using direct communication between an array of relatively simple processing engines (PEs). Row Stationary (RS) Dataflow 구현 Four level memory hierarchy : PE scratch pad와 inter-PE 통신을 최대한 이용하고 Global Buffer와 외부 메모리의 data transfer를 최소화 point-to-point \u0026amp; multicast Network-on-Chip(NoC) 아키텍쳐 지원 Run-Length Compression (RLC) 포맷 지원 : zero operation 제거 Eyeriss System Architecture Eyeriss는 과 communication link clock 두가지 clock domain을 가짐 data processing core clock : 12X14 PE array, Global Buffer(GLB), RLC codec, ReLu 배치, PE가 local scratchpad를 이용하여 연산하거나 PE가 인접 PE 또는 GLB와 통신 하는것을 가능하게 함 communication link clock Four level memory hierarchy GLB \u0026lt;-\u0026gt; external memory : asynchronous FIFO 이용 PE \u0026lt;-\u0026gt; GLB : NoC 이용 ReLU \u0026lt;-\u0026gt; RLC codec local temporary data storage using scratchpad 분리된 clock으로 PE는 다른 PE와 같은 클럭 시간에 독립적으로 동작가능하고 link clock은 external memory와 64bit 양방향 버스로 data 전송을 제어 Eyeriss Acc는 Convolution network를 레이어 단위로 진행 첫째로 PE array를 레이어 function/size에 맞게 구성하고 매핑 수행 및 전송 패턴을 결정 input feature map 및 filter map은 external memory에서 PE로 로드되고 output feature map은 다시 external memory로 쓰여짐 2D convolution to 1D multiplication convolution을 수행할 때 2D feature/filter map을 1D로 바꾸어 수행해서 PE에 순차적으로 로딩한다는 이야기를 길게 써놓음 (궁금하면 책을 보자) 2D convolution을 1D vector 와 Toeplitz 행렬(대각선의 성분이 모두 같은 매트릭스)의 곱으로 변환된다 책에 어떤 순서로 feature/filter 1D vector가 PE에 로드/계산되는지 그림으로 있다. Stationary Dataflow 칩에 대한 이야기는 아니고 이전의 stationary 전략에 대해 소개한다. (연산을 어떤 데이터를 고정, 이동 시킬지)\nOutput Stationary Patial Sum의 read/write를 local accumulation을 통해 최소화 Weight Stationary filter map을 local buffer에 두고 계속 활용 Input Stationary input feature map을 local buffer에 두고 계속 활용 다른 전략보다 효율이 안좋은데 약점은 다른 전략보다 convolution연산에 더 많은 cycle이 필요 Row Stationary (RS) Dataflow Eyeriss는 1D vector multiplication 수행하는데 RS dataflow 전략을 사용\nfilter map 행은 PE들에서 수평하게 재사용 input feature map 행은 PE들에서 대각적으로 재사용 partial sum 행은 PE들에서 수직적으로 재사용 RS dataflow에서 데이터는 계산 동안 PE에 저장됨(데이터 이동 최소화) time-interleaved approach를 통해 fmap과 ifmap은 같은 clock cycle 내에서 재활용 계산이 완료되면 Psum은 근접 PE들로 이동(다음 계산을 위해서) Filter Reuse fmap이 spad에 로드 되고 고정된다. 다수의 ifmap도 spad에 로드되고 사슬처럼 연걸됨 Input Feature Maps Reuse ifmap이 먼저 PE에 로드 되고 2개의 fmap은 time-interleaved(연산?) 됨 1개의 ifmap으로 2개의 fmap과 1D 연산 수행 이 것은 전체적인 스피드를 올려주지만 fmap과 psum의 time-interleave 연산을 지원하기 위해 큰 spad가 필요 Partial Sums Reuse fmap/ifmap 둘 다 PE에 로드되며 둘 다 time-interleaved 함 psum은 같은 채널 끼리 합쳐짐 fmap/ifmap 둘 다 PE에 로드되어야 하므로 필요한 spad의 용량이 증가 Run-Length Compression (RLC) ReLU 연산 결과 0 값이 많아 지므로 이는 network의 sparsity가 도입됨 zero computation을 피하기 위해 Eyeriss는 64 bit RLC 포맷을 도입 ([5bit]앞에 값이 0인 element 갯수 + [16bit]0이 아닌 값)*3 + [1bit]마지막 item인지 나타내는 flag 첫번째 layer의 ifmap 값을 제회하고 모든 fmap/ifmap은 RLC 포맷으로 external memory에 저장 External Memory에서 입출력 될 때, RLC encoder/decoder를 통과하게 됨 Glabal Buffer (GLB) external memory와 데이터 전송을 위해 GLB 채택 GLB에는 fmap/ifmap/ofmap/psum 이 저장 GLB는 PE가 연산하는 동안 다음 fmap을 preload Processing Element (PE) Architecture PE는 fmap, ifmap, psum을 위한 3가지 타입의 spad를 가짐 datapath는 3가지 pipeline stage에 의해 구성(spad access, fmap/ifmap multiplication, psum accumulation) 16bit 연산을 사용하며 32bit 연산결과는 16bit로 절삭 값이 O인 ifmap이 발견되면 spad에서 fmap 값을 읽는 것과 연산 로직을 끔(전력소모를 줄이기 위해) Network-on-Chip (NoC) NoC는 GLB와 PE 사이의 데이터 이동을 관리, 하기 2개로 구분 Global Input Network (GIN) : GLB \u0026lt;-\u0026gt; PE 간 single cycle multicast 이용 데이터 전송 Y-Bus는 12개의 X-bus와 연결되며 X-bus는 14개의 PE가 연결 top level controller는 \u0026lt;row,col\u0026gt; 태그를 생성하고 Y-bus / PE의 Multicast controller가 tag를 비교하여 데이터 전송 결정 책에 AlexNet의 예시 있음 Global Output Network (GON) : 별다른 설명 없음 ","date":"2023-02-18T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/","title":"[AI HW Design] Chap05 Convolution Optimization (2/3)"},{"content":"Convolution은 90% 이상의 Computing resource를 사용하며, data access를 줄이기 위해 feature maps reuse / filter weights reuse / partial sum reuse 같은 전략이 사용된다. 이번 챕터에서는 filter decomposition과 Row Stationary(RS) flow를 설명한다.\nDeep Convolution Neural Network Accelerator (DCNN) DCNN은 클라우드 타겟이 아닌 엣지 디바이스 타겟으로 UCLA에서 개발, 다음과 같은 특징을 지님\nStreaming data flow minimizes data access 병렬 컴퓨팅을 위해 bandwidth 향상이 아닌 Interleaving architecture Large-size filter decomposition supports arbitrary convolution window 추가적인 pooling functional unit을 통한 Convolution Unit(CU)의 workload 감소 System Architecture DCNN의 구성은 다음과 같다\nBuffer Bank 중간 데이터 저장 및 외부 메모리와 데이터 교환 목적 Layer Input 용, Layer Output 용 2가지 셋으로 나누어짐 또한 odd/even channel/feature를 위한 Bank A와 Bank B로 나누어짐(Interleaved) Column Buffer Buffer banck의 데이터를 CU engine의 input data type으로 remap Convolution Unint(CU) engine CU engin은 kernel size 3X3까지 지원하는 16개의 Convolution unit으로 구성 16bit fixed-point 연산 local Prefetch unit이 DMA로 부터 weight/bias 값을 주기적으로 업데이트 함 Accumulation (ACCU) buffer convolution 동안 partial sum 연산 또는 Max pooling 연산 수행 Control Command는 외부 메모리에서 128-depth FIFO로 로드 되며 하기 2가지로 분류됨 configure command : network layer를 구성하고 pooling/ReLU function 활성화 excution command : convolution/pooing 초기화 및 필터 decompose 기술 Filter Decomposition 다양한 필터 커널 사이즈 대응을 위해 3X3 CU engine을 이용하여 filter decomposition 기술 이용 커널 사이즈가 3의 배수가 아니면 zero-padding 이용 convolution 후 결과는 one output feature map으로 재결합 됨 상세 사항은 책 참조 Streaming Architecture 데이터의 이동을 최소화하기위해 Filter Weights Reuse와 Input Channel Reuse 사용\nFilter Weights Reuse 3X3 convolution : filter weight는 CU engine에 저장되고 input feature map이 CU engine으로 공급되며 연산이 완료 될 때까지 filter weight는 업데이트 되지 않음 1X1 convolution : CU unit의 9개 곱셈기 중 7개는 off 되고 2개만 odd/even 채널의 partial sum 계산을 위해 사용 Buffer Bank의 output bandwidth를 최대화하기 위해 구조는 하기와 같다(input cannel 및 Column buffer 구조 이야기 같은데) 16개의 row 데이터는 odd/even data set으로 나뉨 2개의 FIFO는 각 데이터 셋에 페어링 됨 (8개의 row 데이터) 8개의 input row data는 10개의 overlapped data로 매핑 Input Channel Reuse 1X1 convolution을 위해 interleaved architecture 사용 (16개 데이터 셋이 odd/even 채널로 2개 데이터셋 구분됨) 2개의 데이터 셋은 다른 filter weight와 곱해져 32개 output이 나옴 출력은 같은 odd/even 채널 끼리 더해짐 위의 내용과 비슷한 것 같은데 filter weight가 이동하고 input 값이 고정이라는 걸 다시 설명한 듯 Pooling pooling function은 average pooling과 max pooing 이 다른 구조로 분리되어 있음\nAverage Pooling Average Pooling function은 Convolution layer에서 Inpu/Output 채널이 같은 사이즈인 CU엔진에 의해 구현 kernel의 사이즈가 pooling window와 일치하는 대응되는 filter weight는 1/K^2으로 되고 나머지는 0으로 된 후 convolution 수행 Max Pooling Max pooling은 ACCU에서 별도 모듈로 구현 Max pooling 모듈은 scratch pad에 있는 8개의 output 값과 연결되며 이는 다른 stride를 지원하기 위해 MUX와 연결 MUX의 출력은 MAX Pooling 계산기로 가는데 이는 3개의 입력과 1개의 output feedback 입력을 받아 계산하며 인풋이 없어질때까지 연산 반복 Convolution Unit(CU) Engine 3X3 CU engine은 9개의 PE(input feature와 filter weight를 곱함)와 1개의 ADDER로 구성 다른 커널 사이즈 지원을 위해 PE는 On/Off 기능을 가짐 상세 내용은 책 참조 (그림과 3X3일 때, 1X1일때 예시 있음) Accumulation (ACCU) Buffer ACCU는 scratch pad에 partial sums 과 stores output feature maps을 저장 ACCU는 partial product accumulation을 위한 Ping-pong buffer, Max pooling을 위한 temporary storage, readout block으로 구성 (책에 그림 참조) Convolution이 진행되는 동안 1개 버퍼는 덧셈만 하고 다른 하나의 버퍼는 Max pooling을 진행, convolution이 완료된후 각 버퍼의 연결이 switch 됨 Model Compression Model Compression을 하기 위해 training 과정에서 모델을 pruning하고 filter weights를 codebook으로 quantization 함. ","date":"2023-02-12T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/","title":"[AI HW Design] Chap05 Convolution Optimization (1/3)"},{"content":"Target Board는 현재 소유하고 있는 ZCU104를 사용하기로 하고 EVB의 번들 카메라인 See3CAM_CU30를 사용하기로 하였다. 출력은 보드에 DP/HDMI가 있는데 DP는 PS 영역이며 HDMI는 사용자가 PL영역에 구성해야 한다. 그래서 DP로 결정. 선정 사유는 역시 Reference를 구하기 쉽다는것에 있다. 상기 ZCU104 + See3CAM_CU30의 reference design은 xilinx의 Embedded-Reference-Platforms 또는 Zynq-UltraScale-MPSoC-VCU-TRD-2022.1에서 확인할 수 있으나 필자는 봐도 어떻게 구성되어 있는지 잘 모르겠다\u0026hellip;\n기본 지식 See3CAM_CU30은 USB3.0 카메라이므로 리눅스에서 Usb Video Clss (UVC)를 gadget을 사용하여 연결한다. UVC : 웹캠이나 캠코더 같은 비디오 스트리밍이 가능한 장치를 기술하는 USB device class Video4Linux2(v4l2) 비디오 캡쳐 시스템을 위한 디바이스 드라이버의 모음이자 표준 API MIPI/USB camera 카메라등을 지원하는 것으로 봐선 UVC 위에서 표준 추상화 계층을 제공하는 것 같다. 출력은 기본적으로 Frame buffer 및 X11 + DRM KMS 구조를 지닌다. petalinux config 이전 포스트인 zcu104 개발환경 설정에서 다음 드라이버 및 프로그램을 설치한다.\nkernel 커널은 하기 모듈이 필요하다. BSP를 사용했다면 거의 바꿀 것 없지만 petalinux-config -c kernel로 다음 기능을 확인하자.\n카메라 입력 : USB gadget driver, web camera/video driver 모니터 출력 : xilinx DRM KMS driver, frame buffer driver 1 2 3 4 5 6 7 8 9 10 11 12 13 CONFIG_MEDIA_CAMERA_SUPPORT CONFIG_MEDIA_CONTROLLER CONFIG_VIDEO_V4L2_SUBDEV_API CONFIG_VIDEO_ADV_DEBUG CONFIG_MEDIA_USB_SUPPORT CONFIG_USB_VIDEO_CLASS CONFIG_USB_VIDEO_CLASS_INPUT_EVDEV CONFIG_V4L_PLATFORM_DRIVERS CONFIG_VIDEO_XILINX 및 그 외 CONFIG_DRM_XLNX 및 그 외 (필요한지??) CONFIG_USB 및 기타 가젯 필요한거 CONFIG_USB_GADGET_XILINX CONFIG_USB_CONFIGFS 및 그외 (필요한지 잘 모르겠음) RootFS petalinux-config -c rootfs RootFS에는 gstreamer/opencv/x11/v4lutil/gcc 패키지그룹, gstreamer 라이브러리, vim, python3 등을 설치한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 CONFIG_packagegroup-petalinux-gstreamer CONFIG_packagegroup-petalinux-opencv CONFIG_packagegroup-petalinux-x11 CONFIG_packagegroup-petalinux-v4lutils CONFIG_packagegroup-core-buildessential CONFIG_vim CONFIG_python3 CONFIG_python3-shell (?) CONFIG_python3-threading (?) CONFIG_python3-multiprocessing (?) CONFIG_gstreamer1.0 CONFIG_gstreamer1.0-plugins-base CONFIG_gstreamer1.0-plugins-good 카메라 및 프래임 버퍼 테스트 상기 설정으로 빌드 및 부팅 후 USB 캠을 연결한다. 그 후 아래 명령어로 카메라의 정보를 확인 할 수 있다.\nv4l2-ctl --list-devices : 연결된 디바이스 확인 v4l2-ctl -d ${디바이스번호} --all : 카메라 capability 등 모든 정보의 출력 v4l2-ctl -d ${디바이스번호} --list-formats-ext : 지원하는 포멧 확인. DP 포트와 모니터를 연결하면 Frame buffer /dev/fb# 이 생성됨을 확인할 수 있다. fbset명령으로 정보를 조해할 수 있다.\npython기반으로 opencv를 이용해서 카메라의 영상을 캡쳐 및 Framebuffer로 출력해 보자. 하기 코드는 테스트 용으로 카메라 및 프레임 버퍼의 설정 부분은 제외했으므로 출력이 이상할 수 있으니 필요하면 자신의 환경에 맞게 고쳐야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import os import cv2 import time capture = cv2.VideoCapture(0) time.sleep(0.1) # (success, reference) = capture.read() # cv2.imwrite(\u0026#39;${이미지 저장 경로}/${저장 이름}\u0026#39;,reference) while 1 : (ret, capFrame) = capture.read() frame16 = cv2.cvtColor(capFrame, cv2.COLOR_BGR2BGR565) fbframe = cv2.resize(frame16, (1920,1080)) with open(\u0026#39;/dev/fb0\u0026#39;, \u0026#39;rb+\u0026#39;) as buf: buf.write(fbframe) capture.release() cv2.destroyAllWindows() 참고 자료 https://github.com/Xilinx/Embedded-Reference-Platforms-User-Guide/tree/2019.2 https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/2322268161/Zynq+UltraScale+MPSoC+VCU+TRD+2022.1 https://www.hackster.io/whitney-knitter/using-a-usb-web-camera-with-the-minized-5783b1 https://www.e-consystems.com/blog/camera/products/getting-started-with-xilinx-zynq-ultrascale-mpsoc-zcu104-evaluation-kit-and-see3cam_cu30_chl_tc_bx/ https://m.blog.naver.com/overcrash3/120105061216?referrerCode=1 ","date":"2023-02-08T00:00:00Z","permalink":"https://muonkmu.github.io/p/yolo_acc_prj-04-camera-%EC%9E%85%EB%A0%A5-%EB%B0%8F-dp-%EC%B6%9C%EB%A0%A5-%EA%B5%AC%EC%84%B1/","title":"[YOLO_Acc_prj] 04 Camera 입력 및 DP 출력 구성"},{"content":"zcu104 petalinux를 포팅하기 위한 일주일 간 삽질의 기록이다. xilinx에서 제공하는 training reference를 따라하면 간단하지만 이는 SD카드에 커널과 루트파일 시스템을 삽입하는 방법이다. 실제 개발의 편의성을 위해 TFTP 및 NFS를 이용하여 부팅하는 방법을 다룬다.\n목표 하기 boot config를 지원하는 petalinux의 포팅 방법 설명 (vivado/petalinux 2022.1 기반)\njtag로 u-boot까지 다운로드 및 부팅 -\u0026gt; u-boot에서 tftp/pxe로 리눅스 kernel 및 device-tree 로드 명령-\u0026gt; 커널에 의한 NFS로 RootFS 로드 SD카드로 u-boot까지 다운로드 및 부팅 -\u0026gt; u-boot에서 tftp/pxe로 리눅스 kernel 및 device-tree 로드 명령-\u0026gt; 커널에 의한 NFS로 RootFS 로드 SD카드로 u-boot, 리눅스 kernel, device-tree 로드 -\u0026gt; 커널에 의한 NFS로 RootFS 로드 Hardware description config 우선 베이스 프로젝트는 리눅스 포팅이 목적이므로 PS영역만 셋업한다. xilinx에서 제공하는 training reference를 그대로 따라해도 무방하다. (https://xilinx.github.io/Embedded-Design-Tutorials/docs/2022.2/build/html/docs/Introduction/ZynqMPSoC-EDT/3-system-configuration.html) zcu104_bsp에 사용되는 hw config을 보고 싶다면 후에 기술할 bsp에 기반한 petalinux 프로젝트 hardware 폴더에 관련 프로젝트가 들어있다. vivado project follow vivado에서 zcu104보드 프로젝트를 만든다. Project is an extensible Vitis platform 은 vitis에서 xrt 라이브러리 등을 사용할 때 필요하다. 현 프로젝트에서는 미선택 board 세팅에서 ZCU104를 선택 create block design을 선택하여 디자인 블럭 생성 zynq_mpsoc ip 를 추가하고 borad preset을 적용한다. 지금은 PL 영역이 필요없으로 AXI_HPM/HP 포트를 미사용으로 설정 Validate Design 으로 디자인 검증 후 Create HDL Wrapper 로 래퍼를 생성한다. Generate Block Design을 실행 후 bit stream (*.bit) 을 생성한다. 현재는 pl 영역의 디자인이 없으므로 bit-stream을 생성하지 않아도 무관한다. Export Hardware로 xsa 파일을 생성한다. 현재는 pl부분의 디자인이 없으므로 bit-stream을 포함하지 않아도 되며 포함하여도 상관없다. Petalinux porting project create 처음에는 기초부터 시작하고자 base template 프로젝트로 시작하였지만 포팅 시 부팅이 잘 안되었다. 1 2 petalinux-create -t project --name ${프로젝트 이름} --template zynqMP petalinux-config --get-hw-description=${xsa파일} --silentconfig 그래서 xilinx에서 제공하는 bsp 기반으로 프로젝트를 만들었다. 1 2 petalinux-create -t project --name ${프로젝트 이름} -s ${bsp파일} petalinux-config --get-hw-description=${xsa파일} --silentconfig 위의 두 프로젝트의 폴더/파일을 비교해보면 BSP를 위해 커널등이 어떻게 설정되어 있는지 알 수 있다. bsp 기반으로 만들어진 프로젝트의 경우 README에 BSP가 어떤 설정을 가지고 만들어져 있는지 나와 있다. 위의 기본 템플릿 프로젝트와 파일과 비교해서 보면 몇가지 설정에 대한 설명이 누락되었음을 알 수 있다. tftp boot config 이전 post인 ubuntu 환경 설정을 내용을 참고하여 host에 tftp 서버를 설정한다. petalinux-config 명령을 실행하여 Image Packaging Configuration \u0026gt; Copy final images to tftpboot에 host tftp서버 폴더를 지정한다. 만약 향후에 RootFS를 INITRAMFS으로 할려고 할 시 built-in FIT image를 위한 임시 ram 사이즈가 작아서 부팅 시 \u0026ldquo;There\u0026rsquo;s no \u0026lsquo;/dev\u0026rsquo; on rootfs\u0026rdquo; 에러가 난다. 이럴 경우 petalinux-config의 Image packaging configuration \u0026gt; INITRAMFS/INITRD Image name 을 petalinux-image-minimal로 변경 한다. NFS rootFS config 이전 post인 ubuntu 환경 설정을 내용을 참고하여 host에 NFS 서버를 설정한다. petalinux-config 명령을 실행하여 Image Packaging Configuration 에서 하기 내역을 설정 Root File System Type에서 NFS를 선택 Location of NFS root directory에 host nfs 폴더를 지정 NFS Server IP address 에서 host ip를 지정 petalinux-config -c kernel에서 하기 내역이 설정 되어 있는지 확인 Networking support \u0026gt; IP: kernel level configuration 의 IP:DHCP support, IP:BOOTP support, IP:RARP support File systems \u0026gt; Network file systems \u0026gt; Root file systems의 NFS 체크 상기 내역까지가 매뉴얼의 내용인데 적용해보면 nfs 버전 문제로 nfs RootFS가 마운트 되지 않는다. bootargs에서 nfs version을 3으로 변경한다. petalinux-config에서 DTG Setting \u0026gt; Kernel Bootargs \u0026gt; generate boot args automatically를 해제 (해제하기전에 설정되어 있는 bootargs를 copy) 위에 복사한 것을 bootargs를 작성하는 란에 붙여넣고 nfsroot부분에 nfsvers=3 추가 ex) earlycon console=ttyPS0,115200 clk_ignore_unused root=/dev/nfs nfsroot=192.168.1.30:/home/minwook/xlx_nfsrfs,tcp,nfsvers=3 ip=dhcp rw MAC 설정 u-boot 부팅 시 마다 아이피가 달라지지 않도록 MAC를 설정한다. petalinux-config 명령의 Subsystem AUTO Hardware Setting \u0026gt; Ethernet Setting \u0026gt; Ethernet MAC address 사실 zcu104의 맥 주소는 부팅 시 eeprom에서 읽어 온다는데 u-boot에서는 안되는 것 같다(사실 잘 모르겠다.) build 및 부팅 준비 u-boot 및 커널 등을 빌드한다. jtag로 부팅 시키기 위해서는 pre-built 폴더에 이미지들이 준비되어 있어야 한다. petalinux-package를 이용해 준비하자. host의 NFS 서버 폴더에 RootFS를 압축 해제 시켜 NFS 부팅을 준비한다. 향후 SD 카드 등에 부트로더/부트 스크립트를 복사할 경우를 대비하여 부팅이미지를 생성하자. 1 2 3 4 5 petalinux-build petalinux-package --prebuilt cd images/linux tar -xzf rootfs.tar.gz -C ${NFS 서버 폴더} petalinux-package --boot --fsbl zynqmp_fsbl.elf --fpga system.bit --pmufw pmufw.elf --atf bl31.elf --u-boot u-boot.elf 빌드가 정상적으로 완료되면 이전에 지정한 host의 tftp 폴더에 build된 image들이 자동으로 복사된다. 향후 u-boot에서 tftp로 커널 등을 로드할 때 tftp 서버의 pxelinux.cfg 폴더 내 어떤 이미지를 로드할 것인지에 대한 설정을 파일에서 읽어온다. pxelinux.cfg 폴더의 default 파일을 보면 tftp 서버에서 kernel, dtb, RootFS를 로드한다는 것을 알 수 있다. 우리는 NFS에서 RootFS를 로드 할 예정이므로 default 파일의 RootFS 로딩 스크립트 부분을 삭제한다. 크로스 컴파일 환경 설정 향후 application의 개발 시 host에서 크로스 컴파일을 진행하고 싶다면 sdk를 만들어 sysroot를 설정하면 된다. 1 2 3 4 petalinux-build --sdk petalinux-package --sysroot -d ${SDK_설치_폴더} unset LD_LIBRARY_PATH source ${SDK_설치_폴더}/environment-setup-cortexa72-cortexa53-xilinx-linux Petalinux Booting jtag boot jtag로 u-boot까지 로딩한다. 보드의 boot-cfg 스위치를 jtag로 맞춘다. SD카드 등이 필요없지만 속도가 느리다. host에 USB를 연결하고 터미널을 오픈 후 하기 명령을 수행하면 부팅이 시작된다.부트 스크립트 로딩 전 대기 카운터에서 엔터를 누르면 u-boot 커맨드 입력이 가능하다. 만약 rlwrap: warning 이 발행한다면 현재 $TERM에 rlwrap이 없는 것이므로 쉘의 $TERM의 값을 xterm으로 변경한다. 1 2 export TERM=xterm petalinux-boot --jtag --prebuilt 2 --hw_server-url tcp:127.0.0.1:3121 u-boot에서 커널 로딩 u-boot에서 command를 이용하여 tftp서버에서 커널과 dtb를 로드한다.\ndhcp로 target ip 획득 tftp 서버의 ip 및 target의 ip의 환경변수 설정 이부분은 petalinux-config의 u-boot Configuration \u0026gt; u-boot script configuration \u0026gt; Pre bootenv 에서 설정이 가능할 것이라 생각되는데 해보지 않음 tftp 서버에서 설정파일 로드 (pxelinux.cfg/default) tftp 부팅 (이후 RootFS의 로드는 세팅에 따른다.) 1 2 3 4 5 dhcp setenv serverip ${host_ip} setenv ipaddr ${target_ip} pxe get pxe boot SD카드 + NFS RootFS jtag로 부팅하면 편하긴 하지만 느리고 매번 리셋이 필요할 때마다 부팅명령을 다시 넣어줘야 한다. 이를 SD카드로 부팅시켜 해결할 수 있다. 보드의 boot-cfg 스위치를 SD 카드로 변경한다. SD카드가 준비되어 있지 않다면 SD카드를 파티션 설정을 해야 한다. 첫번째 파티션은 부트로더, 부팅스크립트 등을 위한 파티션이며 최소 500MB이며 FAT 파일 형식 두번째 파티션은 RootFS용으로 EXT4 형식이어야 한다. SD카드에 FSBL, U-boot, bitstream인 BOOT.bin 를 넣어 놓고 u-boot 까지 부팅 시킨 후 이후 커널 및 RootFS를 로딩할 수 있다. SD카드에 커널의 내용이 변경되지 않은 경우 부팅스크립트 boot.scr와 커널/디바이스트리 image.ub를 넣어 놓고 자동으로 NFS에서 RootFS를 로딩하게 하는 방법도 가능하다. 참고 자료 https://xilinx.github.io/Embedded-Design-Tutorials/docs/2022.2/build/html/index.html https://github.com/Xilinx/Vitis-Tutorials https://docs.xilinx.com/r/2021.1-English/ug1144-petalinux-tools-reference-guide/Configure-TFTP-Boot ","date":"2023-02-03T00:00:00Z","permalink":"https://muonkmu.github.io/p/yolo_acc_prj-03-zcu104-%EA%B0%9C%EB%B0%9C%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95/","title":"[YOLO_Acc_prj] 03 zcu104 개발환경 설정"},{"content":"본 챕터에서는 graph-based deep-learnig accelerator 중 Graphcore IPU에 대해 알아본다. Graphcore IPU는 Microsoft와 Dell의 차세대 데이터 센터 딥러닝 가속기로 선정되었다.\nGraphcore Intelligence Processing Unit(IPU) Graphcore IPU는 fine-grained operation을 수행하기 위하여 graph theory를 적용하며 MIMD paralleism을 제공\nFine-Grained : 하나의 작업을 작은 단위의 프로세스로 나눈 뒤 다수의 호출을 통해, 작업 결과를 생성해내는 방식, 반대말은 Coarse-Grained. Intelligence Processor Unit Architecture IPU는 tiles라 불리는 1216 PE로 구성 PE는 256kb local memory를 가지며 레지스터 파일을 제외한 추가 memory storage를 가지지 않음 tiles 간 IPU Exchange라 불리는 on-chip interconnect로 연결되어 있으며 IPU간 연결을 위한 IPU link를 제공 IPU는 6개의 개별 processing thread를 제공하며 각 thread는 별개의 instruction과 excution flow를 제공 각 tile은 static round-robin schedule에 따라 thread 들을 순환한다. Accumulating Matrix Product (AMP) Unit IPU는 pipelined structure AMP를 채택, AMP는 64bit mix-precison 또는 16bit single-point 연산을 클럭 사이클 마다 수행 가능 mix-precison : 훈련 중에 모델에서 16-bit 및 32-bit 부동 소수점 유형을 모두 사용하여 더 빠르게 실행하고 메모리를 적게 사용하는 것, 모델의 특정 부분을 32-bit 유형으로 유지 Memory Architecture PE 당 256kb local memory를 가지며 IPU는 총 304Mb 메모리를 가짐 각 tile은 21bit address space를 가지며 6개의 execution unit과 이를 공유함 Interconnect Architecture IPU 간 연결은 IPU link를 사용하며 2개 IPU 연결은 3개의 IPU link를 사용 (65Gb/s) Host완s PCIE-4로 연결 Bulk Synchronous Parallel (BSP) Model IPU operation은 BSP 모델을 기반으로 하며, BSP 모델은 아래 3개의 Phase Operation으로 구분된다. Computation Phase : 모든 프로세서가 로컬 메모리로 computation을 수행하며 프로세서간 어떤 통신도 없다. Communication Phase : 각 프로세서는 정보를 교환하며 어떤 computation도 없다. Barrier Synchronization : 모든 프로세서는 computation이나 communication 없이 모든 프로세서가 barrier에 도달할 때까지 대기한다. IPU는 computation이 시작되기 전에 instruction이 코어로 전송되는 BSP 모델을 실행한다. core는 computation을 수행하고 이가 끝난 뒤 다른 코어와 communication을 수행한다. 그후 모든 코어는 동기화를 수행한다. 결론 Blaize GSP와 Graphcore IPU는 분산 프로세서를 통해 거대 parallel operation을 처리할 수 있기에 Cloud-base application에 좋은 솔루션이다. 그러나 이들은 power/area 문제로 임베디드 추론 application에는 적합하지 않다. ","date":"2023-01-28T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-2/2/","title":"[AI HW Design] Chap04 Streaming Graph Theory (2/2)"},{"content":"본 챕터에서는 graph-based deep-learnig accelerator에 대해 공부한다 (Blaize GSP, Graphcore IPU). 이들은 Mutiple Instructions Multiple Data(MIMD) 처럼 동작한다.\nBlaize Graph Streaming Processor(GSP) 무언가 칩에 대한 설명보단 Graph Streaming 기본 이론에 대한 내용이 주를 이룬다.\nStream Graph Model Stream Graph는 네트워크 트래픽, 데이터베이스 등에 널리 쓰이는 모델로 dynamic stream data를 처리하기 위해 data stream model(TCS)을 사용 거대 그래프 스트리밍 Transit(T), 큰 데이터 처리 Compute(C), 일시/롱텀 데이터 저장 Store(S) Turnstile 모델이 TCS모델 중에서 데이터 출발/도착과 같은 data behavior을 가장 잘 표현하며 task scheduling에 사용. Depth First Scheduling Approach Blaize GSP는 뉴럴넷모델을 Direct Acyclic Graph(DAG) format (V,E)로 변환 V는 PE vertex, E는 PE간 weighted connection을 위한 edge scheduling을 위해 Depth First Scheduling (DFS)를 사용하며 dynamic graph excution을 가능하게 하고 sparse/conditional graph를 지원한다. (dfs 설명은 유명하니 생략) GSP는 4가지 Parallelism을 달성했다. 자세한 설명은 책 참조 Task parallelism, Thread parallelism, Data parallelism, Instructon parallelism Graph Streaming Processor Architecture GSP는 다음 그림과 같은 구조로 되어 있음 Streaming Processing은 Sequential Processing에 비해 하기와 같은 장점이 있음 (책에 두가지 방법에 대해 비교 그림있음) Small intermediate buffer for local processing Cached data is easily supported Memory bandwidth is reduced to improve the performance with less power Support both task and data-parallel processing Data is sent to the next node when it is ready GSP는 opeartion을 데이터가 준비되는 즉시 기다리지 않고 수행하도록 스케줄링 함으로써 성능을 향상시키고 메모리 access를 감소시킴 ","date":"2023-01-27T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-1/2/","title":"[AI HW Design] Chap04 Streaming Graph Theory (1/2)"},{"content":"Git을 사용하다보면 막히는 부분이 항상 생긴다. 이런 부분에 대해 간단히 정리 해 놓는다.\nGit remote branch 가져오기 Git을 사용하다보면 원격저장소에 있는 branch를 local로 가져와야 경우가 있다. 이럴 때 git checkout -t {저장소 이름} 을 사용하면 된다.\n필요 시 git remote를 갱신 필요 시 remote 브랜치 확인 remote 브랜치 가져오기 1 2 3 git remote update git branch -r git checkout -t {origin/저장소 이름} 만약 브랜치의 이름을 변경하여 가져오고 싶다면 git checkout -b {생성할 branch 이름} {원격 저장소의 branch 이름} 만약 checkout 시 -t 옵션을 제외하면 ‘detached HEAD’ 상태로 소스를 보고 변경 해볼 수도 있지만 변경사항들은 commit, push 할 수 없으며 다른 branch로 checkout하면 사라진다. ","date":"2023-01-20T00:00:00Z","permalink":"https://muonkmu.github.io/p/git-%EC%82%AC%EC%9A%A9-tip-%EC%A0%95%EB%A6%AC/","title":"git 사용 tip 정리"},{"content":"Target Model을 YOLOv3_tiny로 정한 것은 다른 이유가 있는 것은 아니고 간단하고 레퍼런스가 쉽게 구할 수 있어서이다. 사실 프로젝트가 YOLOv3 tini의 경우 매우 가볍기 때문에 가속기로의 의미는 크게 없다고 생각한다. 그러나 YOLO-X 모델 같은 가속기를 구현하기 위해서는 Sparse Matrix operation 등이 적용 가능한 NPU와 같은 구조를 잡는 것이 필요할 것이라 생각되어 미루어 두기로 한다. 우선 간단한 가속기를 구현하는 것에 의미를 둔다.\nYOLO reference YOLO v3 tiny은 YOLO v3에서 FPN 을 덜어내고 경량화 시킨 구조이다. 라즈베리 파이 CPU에서도 돌릴 수 있다고 한다. ( 실제로 돌려보니 정확도가 좀 떨어지는 것 같다. 바운딩 박스도 이상하게 쳐지고) 기본적인 코드는 darknet git에서 구할 수 있다. 사용법 및 설명은 홈페이지에서 볼 수 있다. (https://pjreddie.com/darknet/yolo/) darknet repo pull make(GPU 사용 예정이라면 Makefile 수정) pre-trained 된 weights 다운 test 1 2 3 4 5 git clone https://github.com/pjreddie/darknet cd darknet make wget https://pjreddie.com/media/files/yolov3-tiny.weights ./darknet detect cfg/yolov3-tiny.cfg yolov3-tiny.weights data/dog.jpg yolov3-tiny.cfg 파일을 보면 Model의 구조를 알 수 있다. 각 layer에 대한 설명은 누군가 Darknet을 pytorch로 변환하면서 분석해 놓은 것이 있으니 이를 참조한다. (https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/) YOLO v3 Structure Model을 도식화하면 다음과 같다. 참고 자료 https://wikidocs.net/181704 https://deep-learning-study.tistory.com/411 ","date":"2023-01-16T00:00:00Z","permalink":"https://muonkmu.github.io/p/yolo_acc_prj-02-yolo-v3-tiny-%EB%B6%84%EC%84%9D/","title":"[YOLO_Acc_prj] 02 YOLO v3 tiny 분석"},{"content":"본 챕터에서는 몇가지 주요한 Paralle Architecture에 대하여 소개한다. 이 페이지에서는 Microsoft Catapult Fabric Accelerator에 대해서 기술한다.\nMicrosoft Catapult Fabric Accelerator 마이크로소프트는 Brainwave Project를 CPU와 FPGA를 쓰는 형태로 변경 48개의 FPGA가 2개의 Half rack (pod)에 그룹화되고 네트워크로 연결 Brainwave는 train된 DNN 모델을 Catapult Fabric이라 불리는 synthesized softcore에 컴파일 후 narrow precision approach를 적용 모델의 파라메터는 softcore 내 상주 System Configuration Catapult Fabric은 synthesized softcore이며 이는 RTL의 recompilation 없이 low level SW library를 통해 reconfigure 될 수 있다. Catapult Fabric은 Shell과 Role 두 파트로 나눌 수 있음 (상세사항은 책을 참조) Shell : 모든 application에서 재사용 가능한 programmable logic (통신/off-chip 등의 인터페이스를 말하는 듯) Roll : Application logic (그림을 보면 Softcore를 지칭하는 듯) Catapult Fabric Architecture Catapult Fabric은 Matrix-Vector Multiplier(MVM), MultiFunction Uint(MFU), Vector Arbitration Network로 구성 MVM : Matrix-Vector 및 Vector-Vector 연산 수행, PRF/VRF/MRF에 ifmap/fmap 저장 VAN : PRF/DRAM/IO-queue 간 데이터 전송 담당 Matrix-Vector Multiplier FP16을 MS-FP8/MS-FP-9으로 변환하여 연산(mantissa가 2~3bit) input data는 VRF, filter weight는 MFR에 저장 3개의 tile engine과 3개의 accumulator에서 병렬 연산을 지원(상세내용을 책을 참조) MVM의 출력은 MFU로 연결, MFU는 vector-vector operation, activation등을 수행 Hierarchical Decode and Dispatch (HDD) Catapult Fabric은 전통적인 scalar processor(Single Instruction Single Data)를 채택 scheduler는 6개의 Decoder가 4-layer구조로 배치 (상세 내용은 책 참조) Sparse Matrix-Vector Multiplication (SMVM) SMVM을 위해 Condensed Interleaved Sparse Representation (CISR) encoding 사용 Compressed Sparse Raw (CSR) 포맷이 가변 row 길이로 인해 Parallel contorl이 어려움을 극복 첫 번째 0이 아닌 요소가 첫 번째 슬롯에 배치, 해당 열 인덱스는 인덱스 배열에서 동일한 순서로 배치, 행 요소가 모두 사용되면 다음 두 행 요소가 빈 슬롯에 할당, 이를 반복 (상세 내용은 책 참조,사실 이해가 잘 안감) ","date":"2023-01-13T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/","title":"[AI HW Design] Chap03 Parallel Architecture (3/3)"},{"content":"현재까지 DL에 대해 공부한 것과 현업에서 배운 것을 섞어보고자 개인 프로젝트를 진행할 예정이다. 실력이 미천하여 성능, 효율성은 미뤄두고 가장 빠르고 쉽게 구현하는 것을 목표로 한다. 생각보다 오래 걸릴 듯 하다.\nGOAL 카메라의 입력을 받아 Real-time으로 Object detection을 수행하는 FPGA 기반 ECU 개발 현재 개발되어 있는 기반 설계 및 IP가 전무하기에 PPA 보다는 빠른 구현에 목표를 둔다.\nSPEC (TBD) target B/D : ZCU104 input : Full HD camera (interface MIPI or USB 중 쉬운거) output : real time image showing a bounding box (interface HDMI) Algorithm : YOLO v3 Tiny (이를 선택한 특별한 이유는 없고 reference 구하기 쉽고 간단해서 이다) miniaml 20 fps Design Flow (TBD) architecture design and spec fix camera interface design output interface design YOLO Core design (HLS + verilog) ","date":"2023-01-13T00:00:00Z","permalink":"https://muonkmu.github.io/p/yolo_acc_prj-01-yolov3-acc-personal-project-propsal/","title":"[YOLO_Acc_prj] 01 YOLOv3 Acc Personal project propsal"},{"content":"GPU를 이용하여 Deep learning 모델을 구성하고자 하였으나 다른 기술 블로그에서 기술한대로 수행하여도 동작이 되지 않는다. 해당 공식 문서들을 참고하여 설치하는 법을 기술한다.\n목적 GPU 및 pytorch 기반 개발을 위한 PC 개발 환경 구성 GPU 활용을 위해 nvidia driver, cuda, cudnn 설치와 conda 환경에서 pytorch를 설치하는 방법을 다룬다. 환경 OS : ubuntu 20.04 LTS GPU : nvidia 1080ti python 3.8.10 and GCC 9.4.0 설치 절차 우선 모듈의 dependency를 확인해야 한다. 현재 pytorch에서 우분투 20.04를 지원하는 플랫폼은 CUDA 11.7이므로 CUDA 11.7 버전과 이에 적합한 nvidia driver를 설치 해야한다. 필요한 nvidia driver는 사실 cuda를 설치 해보면 dependency 체크를 하면서 필요한 버전을 알려준다.(더 좋은 방법이 있을지도)\nNvidia Driver 설치 nvidia driver 설치 여부 및 현재 설치된 버전을 확인한다. 1 nvidia-smi 필요 시 기존의 nvidia driver를 삭제한다. 1 2 3 sudo apt-get remove --purge \u0026#39;nvidia-.*\u0026#39; sudo apt-get autoremove sudo apt-get autoclean 설치가능한 드라이버를 확인한다. 1 ubuntu-drivers devices 만약 필요한 드라이버 목록에서 없다면 저장소를 추가한다. 1 2 sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update 원하는 드라이버를 설치 후 재부팅 한다. 현재 저자의 환경에서는 5.25가 필요하므로 이 버전을 예로 설명한다. 1 2 sudo apt install nvidia-driver-525 sudo reboot cuda 설치 cuda 홈페이지에서 현재 내 설정에 맞는 runfile을 다운 가능하나 저자는 이상하게 설치가 안되었다. 하기 페이지를 활용하여 네트워크 Repo에서 설치하자 (https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#prepare-ubuntu)\nRemove outdated signing key Install the new cuda-keyring package Install CUDA SDK reboot 1 2 3 4 5 6 sudo apt-key del 7fa2af80 wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb sudo dpkg -i cuda-keyring_1.0-1_all.deb sudo apt update sudo apt-get install cuda-11-7 sudo reboot 환경변수를 등록한다. 1 2 export PATH=/usr/local/cuda-11.7/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-11.7/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} cudnn 설치 cudnn 역시 package 파일로 설치가 잘 안되서 tar 파일로 설치하였다. (https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html)\n필요한 cudnn 라이브러리 tar를 cudnn 홈페이지에서 다운받는다. 파일의 압축을 풀고 cuda 라이브러리에 파일을 복사한다. 1 2 3 4 tar -xvf cudnn-linux-x86_64-8.x.x.x_cudaX.Y-archive.tar.xz sudo cp cudnn-*-archive/include/cudnn*.h /usr/local/cuda/include sudo cp -P cudnn-*-archive/lib/libcudnn* /usr/local/cuda/lib64 sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn* conda 설치 모듈들의 dependency 및 버전 관리를 위해 가상환경인 conda를 사용하기로 하였다.\n자신의 파이썬 환경에 맡는 miniconda 설치 파일을 받고 이를 실행한다. 1 Miniconda3-latest-Linux-x86_64.sh 자신이 사용할 가상환경을 만들고 이를 실행한다. 1 2 conda create -n {my_env} conda activate {my_env} 터미널 실행 시 자동으로 conda 환경이 실행되는 것을 막을려면 다음을 수행한다. 1 conda config --set auto_activate_base false pytorch 설치 conda 환경에서 pytorch 홈페이지를 참고하여 pytorch를 설치한다. 자신이 원하는 구성을 고르면 Run command를 알려준다.\n1 conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia 설치 확인 conda 환경에서 python을 터미널을 실행한 후 pytorch cuda 설정 사용 가능 여부가 True로 출력되면 정상\n1 2 import torch print(torch.cuda.is_available()) ","date":"2023-01-13T00:00:00Z","permalink":"https://muonkmu.github.io/p/dl-%EA%B0%9C%EB%B0%9C%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95-cuda-cudnn-conda-pytorch/","title":"DL 개발환경 설정 (cuda, cudnn, conda, pytorch)"},{"content":"본 강좌에서는 Object Detection의 개념과 이를 위한 YOLO알고리즘의 기초에 대하여 정리한다.\nhttps://www.coursera.org/learn/convolutional-neural-networks 하기 블로그에 더 잘 정리되어 있다..(누군지 존경스럽다.)\nhttps://junstar92.tistory.com/140 Object Localization Classification with localization : object class 뿐 아니라, 알고리즘이 object를 대상으로 bounding box를 표시하는 것을 의미\n이를 위해 이미지를 CNN에 입력 출력으로 class 뿐만 아니라 이미지에 object가 존재할 확률($p_c$), Bounding box의 위치 및 크기를 같이 출력(Bx, By, Bh, Bw) Loss function은 MSE(mean squared error)를 사용한다면 Y 각 요소의 에러의 합과 같다. Pc가 0일 경우 Pc의 에러만 사용한다. $$ L(\\hat{y},y) = \\begin{cases} (\\hat{y}_1-y_1)^2+(\\hat{y}_2-y_2)^2+\\cdots+(\\hat{y}_8-y_8)^2 \u0026amp; \\text{if } y_1=1 \\\\ (\\hat{y}_1-y_1)^2 \u0026amp; \\text{if } y_1=0 \\end{cases} $$\nMSE를 예시로 설명했지만, $c_1$, $c_2$, $c_3$에는 log-likelihood loss와 softmax를 사용하고, bounding box 정보에는 MSE를, 그리고 $p_c$ 에는 Logistic Regression Loss를 사용할 수도 있다.\nLandmark Detection Bounding box가 아닌 일반적인 Face recognition이나 pose detection의 같은 일반적인 경우 이미지의 주요 포인트(landmark)를 X와 Y의 좌표로 나타낼 수 있다. Object Detection Sliding Windows Detection 알고리즘을 사용해서 Object Detection을 위해 ConvNet을 사용하는 방법 알아본다. (CS231n 강의에서는 Sliding window는 하지말라던데 아마 이해를 위해 넣어놓은 것 같다.) 방법은 하기와 같다. object의 클래스를 구분할 수 있는 모델 생성 전체 이미지 중 특정 size의 window를 골라 탐색 window 살짝 옮겨서 반복 더 큰 박스를 이용하여 반복 그러나 이 방법은 computing cost가 높다. 다음 절에서 이를 줄일 수 있는 방법을 알아본다 Convolutional Implementation of Sliding Windows Sliding window 방법은 매우 느린데 이를 해결하기 위해 FC(Full connected) layer를 Convolutional Layer로 튜닝하는 것을 알아보자. 절차는 하기와 같다 FC layer를 이와 같은 output을 낼 수 있는 Filter로 변환 sliding window 시 각각 수행이 아닌 convolution처럼 한번에 연산. 이렇게 하면 중복되는 연산은 공유가 가능하다. 그러나 이 방법은 bounding box의 위치가 정확하지 않다는 단점이 있는데 이를 아래 방법으르 해결한다. Bounding Box Predictions Sliding window 방법은 object가 그 위치에 있지 않거나 일부분만 걸칠 수 있는데 이를 YOLO 알고리즘으로 극복 가능하다. 전체 이미지에 3x3 grid 를 설정(보통은 19x19 사용) 위에서 배운 object localization을 각각의 grid에 적용, 즉 이해한바로는 test set에서 각각의 그리드에 localization방법으로 labeling하고 학습 각 grid에 object가 존재한다면 object의 중간점을 위해서 object를 할당한다. 이때 object의 크기는 1이 넘어갈 수 있다.(gird를 넘어가거나 클 수 있으므로) Bounding box를 설정하는 방법은 여러가지가 있지만(ex. PCA 이용), YOLO논문을 살펴보면 잘 동작할 수 있도록 파라미터화 된 것들이 있다. Intersection Over Union Intersection over union(IoU)은 Object Detection이 잘 동작하는지 판단하기 위한 함수 labeling 된 bounding box와 예측한 bounding box의 전체 넓이와 겹치는 부분 넓이의 비율을 계산 보통 0.5 이상이면 예측한 bounding box의 결과가 옳다고 판단 Non-max Suppression 현재까지 알아본 Object detection의 문제점은 한 Object를 여러번 탐지할 수 있다는 것이다. 즉 한 object가 한 그리드 이상에의 면적을 차지할 경우 이 object의 중심점이 여러 Cell에서 탐지 될 수 있다. 이 경우에 Non-max suppression을 사용하면 알고리즘이 하나의 object를 하나의 cell에서 한번만 탐지할 수 있다. 만약 분류 class 가 1개여서 $p_c$가 class의 확률이라 가정한다. (실제로는 클래스는 여러개) $p_c$를 조사하여 가장 큰 것만 취함 나머지 box와 $p_c$값이 가장 큰 박스와 IoU 조사 IoU가 높은 박스는 제거 만약 class가 여러개라면 class 당 non-max suppression을 수행한다. Anchor Boxes 현재까지 소개한 알고리즘의 문제점 중 하나는 각 grid cell이 오직 하나의 object만 감지할 수 있다는 것이며 이를 anchor box라는 아이디어를 가지고 해결할 수 있다. anchor 박스의 모양을 미리 정의 각각의 anchor box는 각 output을 가지게 한다. anchor box의 선택은 manual로 선택을 할 수도 있고, K-mean알고리즘을 통해서 얻고자하는 유형의 object모양 끼리 그룹화 할 수도 있다. YOLO Algorithm 위의 내용을 모두 종합하여 YOLO object detection algorithm을 정리해보자 이미지의 anchor box와 grid 수를 정하고 이와 같이 labeling된 데이터 셋으로 모델을 학습 상기 모델로 추론을 수행하게 되면 각 grid cell은 anchor box 수만큼의 bounding box를 가질 수 있다. 여기서 낮은 확률을 가지는 예측결과는 제거하고 각 class에 non-max suppression을 적용하여 최종 예측 결과를 얻는다. YOLO 알고리즘은 가장 효과적인 Object Detection 알고리즘 중 하나 ","date":"2023-01-10T00:00:00Z","permalink":"https://muonkmu.github.io/p/cnn-week-03-object-detection/","title":"[CNN] week 03 Object detection"},{"content":"본 챕터에서는 몇가지 주요한 Paralle Architecture에 대하여 소개한다. 이 페이지에서는 NVDLA와 Google TPU에 대해서 기술한다.\nNVIDIA Deep Learning Accelerator (NVDLA) NVDLA는 FPGA로 구성 가능한 추론을 위한 오픈소스 아키텍쳐 (http://nvdla.org) Primitive functional blocks으로 CNN을 지원 (convolution, activation, pooling, normalization) 각 블럭은 next layer의 active와 configuration을 위한 double buffer를 가짐 next layer의 operation은 active operation이 완료되어야 시작 independent mode와 pipeline을 사용하는 fused mode가 있음 Figure. NVDLA core architecture Convolution Operation Direct convolution, Image input convolution, winograd convolution, Batch convolution 지원 (상세내역은 책 참조) Single Data Point Operation(SDP) SDP는 linear functions와 Look-up Table nonlinear functions을 통해 activation과 normalizatin을 지원 (상세내역은 책 참조) Planar Data Operation(PDP) PDP는 maximum/minimum/average pooling을 지원 Multiplane Operation Cross Channel Data Processor(CPD)은 Local Response Normalization(LRN)을 수행 Data Memory and Reshape Operations bridge DMA는 외부 메모리와 메모리 인터페이스간 데이터 전송을 담당 data reshape engine은 data trasnformations, splitting, slicing, merging, contraction, reshape transpose 를 담당 System Configuration NVDLA는 small/large system model로 구현할 수 있음 small system model : IoT 기기와 같이 작은 모델을 위한 모델, 복잡도와 storage를 낮추고 single task를 수행 large system model : mutiple task를 위한 coprocessor와 메모리 인터페이스 추가 External Interface NVDLA는 외부와 통신을 위한 Configuration Space Bus(CSB), Data backbone(DBB), SRAM interface, Interrupt interface를 가짐 (상세내용은 책 참조) Software Design NVDLA SW는 Trained model을 parser/compiler/optimizer를 통해 loadable로 변환 User Mode Driver(UMD)에 의해 Loadalbe이 로딩 되고 Job이 Kernel Mode Driver(KMD)로 제출됨, KMD는 스케줄링 수행 Google Tensor Processing Unit(TPU) 구글은 speech recognition 수요 해결을 위해 TPU v1(stand alone)과 v2/v3(cloud)를 개발 TPU v1은 하기 스펙으로 MLP 0/1, CNN 0/1, RNN 0/1 6가지 neural network application을 수행 가능 256 × 256 eight bits MAC unit 4 Mb on-chip Accumulator Memory (AM) 24 Mb Unified Buffer (UB) – activation memory 8 Gb off-chip weight DRAM memory Two 2133 MHz DDR3 channels TPU는 6가지 neural network application을 수행할 수 있음 Multi-layer perceptron(MLP) 0/1, Convolution Neural Network(CNN) 0/1, Recurrent Neural Network(RNN) 0/1 System Architecture TPU v1은 매트릭스 연산을 Matrix Multiply Unit(MMU)에서 수행 MMU는 256 × 256 eight bits MAC unit이며 16bit 연산을 수행할 경우 성능은 8bit 대비 절반(Sparse matrix 연산을 지원하지 않음) Weight FIFO는 matrix weight를 8Gb DRAM에서 읽어오며 activation, pooling, normalization 후 중간 연산 결과를 24Mb Unified Buffer에 저장 Multipy-Accumulate(MAC) Systolic Array Systolic array는 TPU의 핵심이자 High throughput / low latency를 가진 SIMD pipeline. 책에 별 설명이 없으므로 이에 대한 내용은 더 찾아보는 것이 좋다(다른 많은 곳에 잘 나와 있음) 단점은 전력 소모가 많다는 것(데이터 센터 등에 적합) New Brain Floating-point Format TPU v1은 input data를 FP32에서 INT8로 quantization하여 연산하며 이에 따라 안정성/정확도 문제 발생 이를 위해 IEEE FP16 대신 Brain Floating Point format (BFP16) 사용 BFP16 : Mantissa를 7bit으로 줄이고 exponent를 FP32와 같은 8bit으로 늘림 Sign 1bit, Exponent 8bit, Mantissa 7bit multiplier area와 power를 줄이고 Scaling loss 없이 FP32와 동일한 정확도를 얻음 Performance Comparision roof-line model의 관점에서 볼 때 TPU가 가장 높은 peak performance를 달성했다. roof-line model은 Y축이 성능을 나타내며(평평한 부분이 최고 성능), X축이 byte 당 operation intensity 부가 설명을 하자면 한번에 얼마나 많은 연산을 수행하게 할 때 성능이 어디까지 올라가는지 지표, loof-line은 메모리 Bandwith 때문에 걸림 Cloud TPU configuration TPU v2/v3는 v1에서 DDR을 HBM으로 바꾸고 v1을 Pod로 묶음 Cloud Software Architecture 구글은 cloud computation을 위해 새로운 SW 아키텍쳐를 개발 Model을 TensorFlow를 통해 computational graph로 해석 상세 내용은 책을 참조 ","date":"2022-12-30T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/","title":"[AI HW Design] Chap03 Parallel Architecture (2/3)"},{"content":"IDEC에서 수강한 KiCad 강좌 요약 이다. 디자인 플로우 중 Symbol and Schemetic에 대해 정리한다.\nKiCad 개요 회로도 및 PCB가 함께 설계되는 오픈소스 통합 설계도구 거버 /드릴/ 부품위치 파일 생성 및 PCB 계산기, 거버 뷰어, 3D 뷰어, SPICE 시뮬레이터 포함 프로젝트 기반 관리로 한번에 하나의 프로젝트만 열 수 있음 파일구성 *.kicad_pro : 회로도와 pcb간 공유되는 설정이 포함된 프로젝트 파일 *.kicad_sch : 모든 정보와 구성 요소 자체를 포함시키는 회로도 파일 *.kicad_sym : 회로도 심볼 라이브러리 파일로 심볼 요소 설명을 포함 *.kicad_pcb : pcb 보드 파일 *.pretty : 풋프린트 라이브러리 폴더 *.kicad_dru : pcb 사용자 설계 규칙 파일 *.net. : 회로도에 의해 생성되는 넷리스트 파일 KiCad PCB design workflow 프로젝트 생성 회로도 그리기 회로도 심볼을 심볼 라이브러리에서 찾아 지정된 선 연결, 심볼이 없을 경우 새로 심볼을 새로 만듬 각 구성 요소에 대해 풋프린트를 배정하고 풋프린트가 없는 경우 풋프린트를 생성하여 반영 회로도 완성 시 전기 규칙 점검(ERC 수행) pcb 편집기로 전송하여 레이아웃 시작(넷리스트 생성 및 부품 간 선 연결 일치 시킴) 기판 크기(Edge.Cuts) 그리기 및 풋프린트 위치를 선정 배치 배치 후 요소 사이 트랙 연결 트랙은 규정에 따라 전류 용량, 임피던스, 고전압 누화 등을 고려 선폭/선간 설정 (pcb계산기 참조) 트랙은 신호선의 경우 보통 12mil, 6mil 이하로 하면 pcb 제작 단가 상승 레이아웃이 완료되고 설계 규칙 검사(DRC) 및 수정 거버 파일 제작 출력 및 PCB 제작 의회 프로젝트 관리 창 Tip 1 : 프로젝트 생성 시 템플릿을 지정하여 생성 가능 (큰 회사에서 기초 설정 등을 지정한 형식) Tip 2 : 환경 설정에서 텍스트 편집기를 등록하면 텍스트 편집기 사용이 가능하다. Symbol 생성 필요 시 심볼 라이브러리 생성 (파일-\u0026gt;새라이브러리) 라이브러리 선택 후 새 심볼 생성 생성된 심볼에서 레퍼런스, 심볼값을 원하는 위치로 이동 외형선 그리기, 핀 부가, 핀 더블 클릭 하여 속성(이름, 번호, 유형 등) 설정 필요 시 원점 설정 (단축키 space)(심볼 로딩 위치 및 로테이션 시 회전 점) 저장 Tip) 편집 시 원하는 위치에 지정할 수 없을 때 그리드 속성을 편집하여 그리드 간격을 조절하자 회로도 그리기 프로젝트 매니저에서 {프로젝트 이름}.kicad_sch 파일을 연다 심볼을 배치한다 (전원의 경우 pspice 라이브러리는 시뮬레이션 용이니 Power라이브러리 사용) 선을 연결하고 텍스트 위치 조정한다. 레퍼런스 (부품번호, ex. R100) 지정자 채우기 로 레퍼런스 설정 PCB 풋 프린트 배정 ERC 수행/수정 및 BOM 출력 ","date":"2022-12-28T00:00:00Z","permalink":"https://muonkmu.github.io/p/kicad-01-symbol-and-schemetic/","title":"[KiCad] 01 Symbol and Schemetic"},{"content":"IDEC에서 수강한 KiCad 강좌 요약 이다. 디자인 플로우 중 Footprint와 PCB에 대해 정리한다.\nFootprint Footprint design flow footprint 편집기를 연다 실부품 측정 또는 데이터 시트를 참조하여 부품 치수 확인 필요 시 풋프린트 라이브러리 생성 (파일-\u0026gt;새라이브러리) 라이브러리 선택 후 새 풋프린트 생성 하거나 유사한 부품 불러 온 후 다른 이름으로 저장 십자선 커서 이용하여 원점에서 스페이스바를 눌러 원점 위치 선정 실크 레이어에 부품 외형선 그리기 패드와 홀 위치 시킨 후 사이즈 속성 편집 패드 위치에 맞게 번호 편집, 핀번호는 심볼과 일치하도록 할 것 저장 SMD Component Footprint SMD 부품의 경우 패드의 속성을 SMD로 변경 뒷면 실장 component 뒷면에 실장할 경우 레이어를 관련 레이어를 B.* 레이어로 변경해야 한다. F.Cu, F.Silkscreen, F.Courtyard, F.Fab 내용을 B.* 레이어로 이동 PCB design 프로젝트 매니저에서 PCB 편집기 열기 회로도 PCB 전환(F8)을 이용하여 회로도에서 컴포넌트를 로딩 Edge.Cuts layer에서 PCB 외형선을 그리기 외형선 내부에 컴포넌트 배치 및 컴포넌트 레퍼런스/value 위치 조정 필요한 텍스트를 Silkscreen에 부가 트랙 설정 및 배선 (일반적으로 신호선 12mil, 전원선 30mil) 동박면 씌우기 (GND와 연결) DRC 검사 Plot을 통해 거버/드릴링/포지션 파일 생성 및 검사 ","date":"2022-12-28T00:00:00Z","permalink":"https://muonkmu.github.io/p/kicad-02-footprint-and-pcb/","title":"[KiCad] 02 Footprint and PCB"},{"content":"본 chapter에서는 Reinforcement Learning에 대해서 알아보자\nVideo : https://www.youtube.com/watch?v=lvoHnicueoE\u0026list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv\u0026index=15 Slide : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture14.pdf (TODO: 귀차니즘의 압박으로 정리를 안했다.. 근데 강의가 무척 어려워서 잘 이해가 안된다.)\n","date":"2022-12-23T00:00:00Z","permalink":"https://muonkmu.github.io/p/cs231n-chap-14-reinforcement-learning/","title":"[CS231n] Chap 14 Reinforcement Learning"},{"content":"Petalinux Booting and Packaging petalinux Packaging petalinux-package 명령을 이용하여 하기 내역을 수행 할 수 있다. .BIN 또는 .MCS파일을 생성 (\u0026ndash;boot 옵션) BSP (.BSP 파일) 또는 Package image 생성 (\u0026ndash;bsp, \u0026ndash;image 옵션) prebuilt 디렉토리 생성 (\u0026ndash;prebuilt 옵션) Vitis 를 위한 sysroot 설치 (\u0026ndash;sysroot 옵션) petalinux booting QEMU, SD card, Jtag, TFTP, QSPI에 의한 Booting을 지원한다. (jtag Boot는 속도가 느려 잘 사용안함)\nPetalinux Debugging 상세 내용은 교제의 Petalinux Application Debugging 및 LAB5 참조\nPetalinux는 Application Debugging 시 System Debugger(Vitis) 와 GNU Debugger를 지원한다. Vitis는 Target Communication Framework(TCF)와 Xilinx System DBugger(XSDB)를 이용한 Debugging 환경을 제공 일반적인 Linux GNU Debugger 지원 System Debugger 방법 https://xilinx.github.io/Embedded-Design-Tutorials/docs/2022.2/build/html/docs/Introduction/ZynqMPSoC-EDT/ZynqMPSoC-EDT.html 참조 petalinux-config -c rootfs를 이용하여 Root File system에 하기 내역을 포함 시킨다. tcf-agent (default enable) openssh-sftp-server dropbear (default disable) 이미지를 빌드 하고 디버깅하고자 하는 Application을 실행 시킨다. QEMU의 경우 GEM0만 연결되어 있으므로 필요 시 GEM3 등의 Device Tree를 추가하여 빌드한다. vitis를 실행 시키고 *.XSA 파일 등을 이용하여 platform project를 구성한다. platform project에 빈 linux Applicaiton domain을 추가한다. 4)항의 항목내 Debug configuration을 이용하여 Single Application Debug를 추가한다. target 보드의 debug IP/port를 설정하고 파일 패스를 설정한다. GNU Debuger GNU 디버거를 사용하기 위해서는 Root file System에 gdbserver를 포함하여야 한다. Custom HW and Driver Development Xilinx는 Custop IP에 대한 디바이스 제어를 위해 하기의 방법을 제안한다. Linux Device Driver 제작 mmap의 사용 (사용이 쉽다. 인터럽트 핸들링이 안됨) User space I/O (UIO 사용) (간단한 IRQ핸들링이 된다, Latency가 가변적이고 DMA가 지원되지 않는다) Petalinux는 빌드 시 Device Tree Generator가 DTSI/DTS파일을 생성하고 DTB를 만든다 *.XSA 파일을 분석하여 기본적인 DTSI/DTS 파일을 만든다 {project-root}/components/plnx_workspace/device-tree/device-tree에 생성되는 DTSI파일은 다음과 같다 pl.dtsi : memory-mapped PL IP node pcw.dtsi : Dynamic properties of the PS peripheral system-top.dts : boot argument 와 console, memory information zynqmp.dtsi : PS peri and CPU information zynqmp-clk-ccf.dtsi : IP peri를 위한 clock information Custop IP 추가 등 Device tree를 업데이트 하기 위해 하기 DTSI를 업데이트 한다. {project-root}/project-spec/meta-user/recipes-bsp/device-tree/files/system-user.dtsi Custom HW and Petalinux 개발 절차 Custop IP를 개발(RTL 등) 후 Vivado IP Packer를 통하여 IP-XACT Standard Format으로 패키징 한다. Vivado를 이용하여 1)항의 IP와 기타 사용자 IP를 조합하여 *.XSA 파일을 생성한다. petalinux-creat -t project -n {project 이름}를 이용하여 project를 생성하고 *.XSA 파일을 import한다. petalinux-creat -t module -n {driver 이름}을 이용하여 모듈을 생성한다. {project-root}/project-spec/meta-user/recipes-bsp/device-tree/files/system-user.dtsi에 Custom IP에 관련된 Device tree를 업데이트한다. 작성 시 pl.dtsi를 확인하여 module name 및 address 등을 확인한다. 모듈 내부 드라이버 파일을 작성하고 Yocto 레시피를 수정한다. 커널에 로딩할 지 모듈로 rootfs에 등록할지 결정한 후 빌드한다. ","date":"2022-12-22T00:00:00Z","permalink":"https://muonkmu.github.io/p/petalinux-petalinux-advance/","title":"[Petalinux] Petalinux Advance"},{"content":"Petalinux Basic petalinux 정의 Petalinux는 xillinx FPGA를 위한 임베디드 리눅스 개발 툴로 YOCTO 프로젝트 Wrapper이다. Hardware description file(*.XSA) 또는 BSP 파일을 입력으로 리눅스 이미지 생성 Petalinux 프로젝트의 레이아웃은 프로젝트 생성 시, XSA import 시, build 시 추가/달라짐 (교재 p66을 참조 및 https://docs.xilinx.com/r/2021.1-English/ug1144-petalinux-tools-reference-guide/Image-Selector?tocId=nfcK0XF5PXQyI2ebTdA8fA) 기본 명령어 및 Design Flow 상세 내용은 교제의 Petalinux Tool : Design Flow 및 LAB2 참조\n프로젝트 생성\npetalinux-create -t {type} -n {name} \u0026ndash;template {기초 템플릿} 1 petalinux-create -t project -n test_prj --template zynqMP 프로젝트 설정 : Hardware Description 및 boot, rootfs, kernel\npetalinux-config 또는 petalinux-config -c {rootfs/kernel/device-tree/u-boot} 1 2 cd test_prj petalinux-config --get-hw-description={xsa file} --silentconfig 프로젝트 빌드\npetalinux-build 또는 petalinux-build -c {rootfs/kernel/device-tree/u-boot} 생성되는 파일은 하기와 같다 boot.scr: A u-boot boot script image.ub: U-boot wrapped Linux kernel image rootfs.tar.gz: Compressed root file system tar ball 그외 Pakage를 위한 파일 1 petalinux-build 프로젝트 패키징\n.BIN 또는 .MCS 생성 ( = fsbl + ssbl + pmu + bitstream) .BIN 은 다음과 내용을 포함한다. Platform Loader and Manager (PLM) PS Management (PSM) firmware Platform Device Image (PDI) ARM trusted firmware u-boot Device tree blob 1 petalinux-package --boot --fsbl zynqmp_fsbl.elf --u-boot u-boot.elf --pmufw pmufw.elf --fpga system.bit 부트\nSD카드에 이미지 복사(BOOT.BIN, Image, rootfs.cpio.gz.u-boot, boot.scr) 후 보드 부팅 qemu로 에뮬레이션 가능 1 petalinux-boot --qemu --kernel TFTP를 위한 Jtag 부트 1 petalinux-boot --jtag --prebuilt 2 --hw_server-url tcp:127.0.0.1:3121 Application development 상세 내용은 교제의 p133 Petalinux Application Development 및 LAB3 참조\nPetalinux의 project가 생성된 상태에서 petalinux-create를 사용하여 app을 생성\nproject-spec/meta-user/recipes-apps/{app_name}에서 생성된 파일(bb 및 source) 확인 가능 1 petalinux-create -t apps --name helloworld --template c source 및 makefile을 생성 또는 복사한다.\nproject-spec/meta-user/recipes-apps/{app_name}/file에서 수정한다. Yocto Recipe file를 수정한다.\nproject-spec/meta-user/recipes-apps/{app_name}의 {app_name}.bb파일에 관련파일을 등록한다. root filesystem에 등록한다.\npetalinux-config -c rootfs 수행 후 apps 메뉴에서 등록 build 후 /usr/bin에서 app을 확인 가능하다.\n프로젝트 설정 상세 내용은 교제의 p150 Customizing the project 참조 petalinux-config를 이용하여 하기 설정이 가능하다\nfirmware version 정보 root filesystem 종류 : INITRD, INITRAMFS JFFS2, UBI/UBIFS, NFS, EXT4(SD/eMMC\u0026hellip;) U-boot 이미지 저장 위치 : bootenv 조절을 통해 Jtag/DDR, QSPI, NAND의 image offset을 조정할 수 있다. Primary Flash(QSPI?)의 파티션 조절 가능 File system package를 조절하여 Kernel image size 및 Root file system 이미지 사이즈를 줄일 수 있다. TFTP 부팅을 위한 pre-built 이미지 위치를 설정할 수 있다 NFS 또는 SD card를 통한 Root file system 로딩을 설정 할 수 있다. Root file system customize 상세 내용은 교제의 p212 Customizing the Root File System 참조\ncustom applications, libraries, module을 추가하거나 생성 가능 pre-compiled applications, libraries, module을 추가하거나 생성 가능 YOCTO layer, recipes 또는 package 추가 가능 ","date":"2022-12-19T00:00:00Z","permalink":"https://muonkmu.github.io/p/petalinux-petalinux-basic/","title":"[Petalinux] Petalinux Basic"},{"content":"목표 개인 기술 정리를 위한 블로그의 생성 markdown 사용이 편리한 github.io를 이용하기로 결정 빌드가 빠른 HUGO framework을 사용 (github에서는 Jekyll framework가 기본이나 컨텐츠가 쌓이면 빌드가 느려지는 단점이 있음) Hugo theme는 STACK을 사용 개발 환경 Oracle Cloud Arm server Ubuntu 20.40 code-server 사전 준비 GO 설치 Hugo는 GO로 작성되 있으므로 GO를 설치한다.\nref : https://go.dev/doc/install 필요 시 GO의 설치 경로를 PATH에 등록한다. Hugo 설치 리눅스의 경우 패키지 관리자를 이용하여 설치가 가능하나 이 경우 old 버전이 설치된다. STACK 테마의 경우 최신버전과 hugo extension이 필요하므로 Go를 이용하여 설치한다. https://gohugo.io/installation/linux/ 1 go install -tags extended github.com/gohugoio/hugo@latest 필요 시 Hugo의 설치 경로를 PATH에 등록한다. git repo 생성 hosting을 위한 repo를 생성한다. repo의 이름은 {git ID}.github.io 형식 ex) muonkmu.github.io 호스팅 목적이므로 repo는 public hugo 빌드 전 소스를 보관할 repo를 생성한다. 이름은 상관 없음 ex) blog 소스 보관용이므로 public/private은 개인 취향 블로그 작성 및 배포 hugo 프로젝트 생성 및 테마 설정 프로젝트를 생성 후 폴더 이동, 하기 예제의 이름은 hugoBlog로 가정 1 2 hugo new site hugoBlog cd hugoBlog git 초기화 및 테마 설정 하기 예제에서는 Stack 테마 사용 clone으로 테마 소스를 themes폴더에 넣을 수도 있으나 submodule을 추천 1 2 git init git submodule add https://github.com/CaiJimmy/hugo-theme-stack.git themes/hugo-theme-stack config파일 설정 config.toml을 수정, 하기 예제에서는 stack 테마의 예제 파일을 복사/수정 한다. config.yaml의 baseurl, theme, title 등을 수정한다. 1 2 3 rm config.toml cp themes/hugo-theme-stack/exampleSite/config.yaml ./ cp themes/hugo-theme-stack/exampleSite/content ./ 1 2 3 4 5 baseurl: https://muonkmu.github.io/ languageCode: en-us theme: hugo-theme-stack paginate: 7 title: MW Devlog 컨텐츠 작성 및 테스트 categories, post, page 등을 작성한다. 하기 예제에서는 stack 테마의 예제 파일을 복사/수정 한다. content/post 내 예제 파일을 참조하여 post를 작성한다(예제포스트는 지워도 된다.) 1 2 rm -r content cp themes/hugo-theme-stack/exampleSite/content ./ 테스트 서버를 구동하여 동작을 확인한다. 하기 예제에는 orcle 서버에서 개발하는 것을 가정, 내부 바인딩과 포트를 별도로 할당였다(오라클 서버에서 방화벽에 우선적으로 포트을 열어둬야 함) 웹 브라우저로 테스트 서버에 접속해 동작을 확인한다. 1 hugo server -D --bind=0.0.0.0 -p 8070 빌드 및 배포 github repo를 연결한다. 소스 repo에 프로젝트 폴더를 연결 host repo에 public 폴더를 연결 1 2 3 git remote add origin https://github.com/muonkmu/blog.git rm -r public git submodule add -b master https://github.com/muonkmu/muonkmu.github.io.git public 소스를 빌드한다. 하기 예제에서는 stack 테마의 사용 경우이다. 1 hugo -t hugo-theme-stack 빌드 및 소스 파일을 push 한다. 1 2 3 4 5 6 7 8 9 10 cd public git add . git commit -m \u0026#34;first commit\u0026#34; git branch -M main git push origin main cd .. git add . git commit -m \u0026#34;first commit\u0026#34; git branch -M main git push origin main (option)배포에 시 사용할 쉘 스크립트를 작성한다. ex)deploy.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #!/bin/bash hugo -t hugo-theme-stack cd public git add . msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; git push origin main cd .. git add . msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; git push origin main Debug HUGO 받침 분리 표기 문제 사용하던 중 \u0026lsquo;가\u0026rsquo; 받침이 분리되어 표기되는 문제가 발견되었다. ex) \u0026lsquo;각\u0026rsquo; 이 \u0026lsquo;가ㄱ\u0026rsquo; 로 표기 구글링을 해보니 Droid Sans Fallback 폰트의 문제라고 생각되어 관련 폰트를 삭제하여 문제를 해결 ./themes/hugo-theme-stack/assets/scss/variables.scss 의 --sys-font-family, --zh-font-family 변수 내 Droid Sans 관련 폰트를 모두 삭제한다. ","date":"2022-12-13T00:00:00Z","permalink":"https://muonkmu.github.io/p/github-blog-%EB%A7%8C%EB%93%A4%EA%B8%B0/","title":"Github blog 만들기"},{"content":"본 챕터에서는 몇가지 주요한 Paralle Architecture에 대하여 소개한다. 이 페이지에서는 CPU와 GPU에 대해서 우선 기술한다.\nIntel Central Processing Unit (CPU) https://www.intel.com/content/www/us/en/developer/articles/technical/xeon-processor-scalable-family-technical-overview.html CPU는 병렬 프로세싱을 위해 Single Instruction Single Data (SISD) architecture에서 Single Instruction Multiple Data (SIMD)로 진화함. 그러나 이는 딥러닝과 같은 거대 병렬 처리에 적합하지 못하여 2017년 딥러닝 어플리케이션을 위한 Xeon processor scalable family (purley platform) 발표 Purley platform은 하기 특징을 가짐 Skylake mesh architecture 이전 Grantley platform에서는 Last-Level Chache(LLC)등이 Intel Quick Path Interconnect(QPI) ring achitecture로 연결 상기 구조는 코어 증가 시 코어 마다 사용가능한 bandwidth가 줄어들어서 메모리 latency가 증가 Grantley platform에서는 Intel Ultra Path Interconnect(UPI) mesh archictecture로 업그레이드 Comnined Home Agent(CHA)가 통합, 이는 LLC 등의 주소 정보 지도를 작성하며 이는 mesh 연결에서 목적지까지의 라우팅 정보를 제공 Fig1. Intel Xeon processor Scalable family mesh architecture Intel Ultra Path Interconnect UPI는 어드레스를 공유하는 mutiple processor coherent interconnect UPI는 vertical/horizontal path를 통한 한 코어에서 다른 코어로의 최단 경로를 제공 2소켓, 4소켓 링, 8소켓+크로스바 등 다양한 구조 지원 SubNon-Unified Memory Access Clustering 플랫폼은 모든 코어/LLC를 반씩 + 메모리 컨트롤를 1개씩 가진 SNC 0,1 도메인을 가짐 각 도메인은 각 메모리 컨트롤러에 매핑되는 유니크한 LLC 주소를 가지며 이는 LLC access latency를 낮춤 Cache Hierarchy Change 하기 그림과 같이 변경 LLC 및 MLC size 변경으로 hit rate 증가 Figure 11. Generational cache comparison single/Multiple Socket Parallel Processing UPI와 sub-NUMA의 지원으로 딥러닝 worker process들은 코어셋이나 싱글소켓, 다중소켓에 assign 될 수 있음 Advanced vector software extension Intel Advanced Vector Extension 512(Intel AVX-512)가 Vector Neural Network Instruction(VNNI)를 지원하는 AVX-512)_VNNI로 발전 대충 더 빨라지고 8/16/32 FP vector 연산을 지원한다는 듯(자세한 사항은 책 참조) Math Kernel Library for Deep Neural Network(MKL-DNN) Convolution, pooling, activation, batch normalization으로 구성된 최적화된 MKL-DNN 지원 key feature는 prefetching, data reuse, cache blocking, data layout, vectorization, register blocking이며 자세한 사항은 책 참조 NVIDIA Graphics Processing Unit (GPU) GPU 장점 : 효율적인 floating point 연산, high speed memory support Turing architecture를 개발함 (NVLink2를 위한 HBM2 적용, 캐시 구조 변경 등등) Tensor Core Architecture tensor core란 : 행렬연산 및 MAC를 위한 전용 코어 Turing Tensor core는 이전(Pascal)이 matrix row by row만 지원했으나 4X4X4 연산을 지원하도록 변경 INT8, INT4를 지원하며 정확도를 낮추면 연산 속도 증가 Matrix사이즈가 크면 이를 나누어 연산, 다양한 size의 매트릭스 연산에 대응 가능 https://www.nvidia.com/ko-kr/data-center/tensor-cores/ Winograd Transform 곱셈 횟수를 줄일 수 있는 Winograd Transform을 지원 상기 변환에 대한 연산식은 책과 다른 자료를 참조할 것 Simultaneous Multithreading (SMT) SMT의 경우 Matrix는 행렬을 여러 그룹으로 나누고 이를 병렬로 처리 (Single Instruction Multiple Thread, SIMT 방식) 연산 후 하위 그룹을 재그룹 시킴 High Bandwidth Memory (HBM2) Memory Bottleneck해결을 위해 HBM2 적용 (memory die를 TSV로 뚫어서 스택함) HBM2는 GPU와 NVLink2로 연결됨 NVLink2 Configuration NVLink는 엔비디아가 개발한 와이어 기반 통신 프로토콜 시리얼 멀티 레인 근범위 통신 링크 (PCIE의 속도 문제 해결) Turing 아키텍쳐는 sing MIO를 two×8 bidirectional differential pair NVLink2로 대체 CPU/GPU 메모리 간 directly load/store/atomic 가능 (데이터를 GPU메모리에서 바로 읽을 수 있고 CPU cache에 바로 저장 가능) 다양한 구성을 지원한다. (책을 참조하자) ","date":"2022-12-12T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-1/3/","title":"[AI HW Design] Chap03 Parallel Architecture (1/3)"},{"content":"목적 Ubuntu 20.04 LTS 설치 후 나에게 맞는 설정 및 설정 방법 정리 유의사항 설치 시 언어는 영어, 키보드 영어 자판으로 설치를 권장 개인 설정 Nvidia 그래픽 카드 설정 설치 가능한 드라이버 확인 1 ubuntu-drivers devices 권장 드라이버 설치 1 sudo ubuntu-drivers autoinstall 한영키 동작 설정 입력기 설치 : setting → Region and Language → Input Source → Korean(Hangul) 추가 1항의 추가된 항목 설정에서 Hangul Toggle Key를 Hangul만 남김(option) /usr/share/X11/xkb/symbols/altwin 편집 4행의 key \u0026lt;RALT\u0026gt; ... 부분에서 symbols[Gropu1] = [ Alt_R, Meta_R ] 부분을 [ Hangul ] 로 수정한다. VNC 설치 tigerVNC 설치 1 sudo apt-get install tigervnc-standalone-server tigervnc-xorg-extension 비밀번호 설정 1 vncpasswd ~/.vnc/xstartup 작성 1 2 3 4 5 6 #!/bin/sh # Start Gnome 3 Desktop [ -x /etc/vnc/xstartup ] \u0026amp;\u0026amp; exec /etc/vnc/xstartup [ -r $HOME/.Xresources ] \u0026amp;\u0026amp; xrdb $HOME/.Xresources vncconfig -iconic \u0026amp; dbus-launch --exit-with-session gnome-session \u0026amp; vnc 서버 실행 1 vncserver -localhost no vnc 서버 종료 1 vncserver -kill :2 설정변경 : $\u0026gt;sudo vim /etc/vnc.conf 1 2 $geometry = \u0026#34;1920x1080\u0026#34;; $depth = \u0026#34;16\u0026#34;; SSH 설치 서버 설치 1 sudo apt install openssh-server 실행여부 확인 1 sudo systemctl status ssh 서버 실행 1 2 sudo systemctl enable ssh sudo systemctl start ssh xforward 설정 팡일의 /etc/ssh/ssh_config 의 x11Forward no → x11Forward yes로 변경 ssh서버 재실행 및 클라언트 실행 시 -X 옵션 추가 ZSH/om-my-zsh 설치 및 설정 zsh 설치 1 sudo apt-get install zsh 설치확인 1 cat /etc/shells 기본쉘 변경 1 chsh -s $(which zsh) oh-my-zsh 설치(curl설치필요) 1 sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; 테마변경 ~/.zshrc 파일 내 ZSH_THEME=\u0026quot;agnoster\u0026quot; 로 변경 글자깨질 시 Powerline폰트 설치 1 sudo apt-get install fonts-powerline 커맨드라인 컴퓨터 이름 감추기 ~/.zshrc 하단에 하기 내용 추가 1 2 3 4 5 prompt_context() { if [[ \u0026#34;$USER\u0026#34; != \u0026#34;$DEFAULT_USER\u0026#34; || -n \u0026#34;$SSH_CLIENT\u0026#34; ]]; then prompt_segment black default \u0026#34;%(!.%{%F{yellow}%}.)$USER\u0026#34; fi } zsh-autosuggestions 플러그인 설치 1 git clone https://github.com/zsh-users/zsh-autosuggestions.git $ZSH_CUSTOM/plugins/zsh-autosuggestions zsh-syntax-highlighting 플러그인 설치 1 git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $ZSH_CUSTOM/plugins/zsh-syntax-highlighting autojump 설치 1 2 3 git clone https://github.com/wting/autojump.git cd autojump ./install.py 사용법 j [디렉토리 명] 또는 j -s 플러그인 활성화\n~/.zshrc 파일 내 plugins=(git zsh-autosuggestions zsh-syntax-highlighting autojump) 로 변경 줄바꿈 적용(멀티라인 입력)\n~/.oh-my-zsh/themes/agnoster.zsh-theme파일 수정 prompt_hg 하단에 prompt_newline 추가 후 파일 최하단 하기 프롬프트 추가 1 2 3 4 5 6 7 8 9 10 prompt_newline() { if [[ -n $CURRENT_BG ]]; then echo -n \u0026#34;%{%k%F{$CURRENT_BG}%}$SEGMENT_SEPARATOR %{%k%F{blue}%}$SEGMENT_SEPARATOR\u0026#34; else echo -n \u0026#34;%{%k%}\u0026#34; fi echo -n \u0026#34;%{%f%}\u0026#34; CURRENT_BG=\u0026#39;\u0026#39; } (option) TFPT 설치 xilinx petalinux를 사용할 생각이라면 tftp 설치가 필요하다\ntftp 설치 1 2 sudo apt-get update sudo apt-get install tftpd-hpa 서비스 확인 1 sudo service tftpd-hpa status 설정 파일 /etc/default/tftpd-hpa 를 원하는 대로 수정한다. 다른 것은 크게 의미가 없고 up/down 위치인 TFTP_DIRECTORY 정도만 수정 수정 후 디렉토리 권한 설정을 해준다. 1 2 3 4 vim /etc/default/tftpd-hpa sudo mkdir {tftp-dir} sudo chmod 777 {tftp-dir} sudo chown -R tftp:tftp {tftp-dir} 설정 완료 후 재시작 1 sudo service tftpd-hpa restart (option) NFS server 설치 nfs서버 패키지 설치 nfs 서버용 폴더를 만들고 모든 클라이언트 머신이 공유 디렉토리에 액세스하기 위하여 권한 제거 및 파일의 권한 제거 /etc/exports 파일을 편집하여 공유할 폴더를 지정하고 클라언트 및 실행 권한 설정 exportfs로 설정된 폴더 내보내기 nfs_server 재시작 1 2 3 4 5 6 7 sudo apt install nfs-kernel-server mkdir ${공유폴더} sudo chown -R nobody:nogroup ${공유폴더} sudo chmod 777 ${공유폴더} sudo echo \u0026#39;${공유폴더} 192.168.1.1/24(rw,sync,no_root_squash,no_subtree_check)\u0026#39; \u0026gt;\u0026gt; /etc/exports sudo exportfs -a sudo service nfs-kernel-server restart ","date":"2022-12-12T00:00:00Z","permalink":"https://muonkmu.github.io/p/ubuntu-20.04-%EA%B0%9C%EC%9D%B8-%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95/","title":"Ubuntu 20.04 개인 환경 설정"},{"content":"목적 클라우드 서버를 이용하여 원격으로 접속 가능한 개발 서버의 구축 최종 목표 고정 IP를 가진 ubuntu 서버 무료 클라우드 서버 중 오라클이 ARM64-4core/24GB ram/200GB storage VM 머신 제공 (타사 대비 월등히 좋음) 원격 개발을 위한 code-server 설치 서버 구축 클라우드 서버 구축 오라클 클라우드 Free tier 가입 리전은 원하는 곳(춘천이 빠르고 ARM 서버 리소스가 남음) 카드 정보를 기입(실제로 결제가 되지는 않음) 가입 완료 후 하단의 Create a VM instance 시작 instance Name 입력 image는 원하는거 선택, ex) canonical Ubuntu 20.04 shape는 Ampere 선택 core는 4, memory는 24GB 까지 무료 상기 리소스를 나누어 무료 VM를 생성할 수 있다.ex) 2core-12GB 인스턴스 2개 무료 VCN이 없다면 페이지에서 VCN을 생성하여 연결 본인의 PC에서 SSH를 생성하여 Public키를 업로드 한다. http://taewan.kim/oci_docs/98_misc_tips/ssh_key_pairs/ 부트 볼륨 생성 Specify a custom boot volume size을 클릭 후 원하는 볼륨생성 200GB까지 무료이며 상기 리소스를 나누어 무료 VM생성 가능 Create로 생성 해당 리전의 리소스가 부족하여 생성이 안되는 경우가 있다. 상기의 경우 리소스가 풀릴 때 까지 기다리거나 유료계정으로 업그레이드 (승인되는데 시간 걸림) 유료 계정이 되더라도 무료 리소스까지만 쓰면 과금이 되지 않는다. 클라우드 서버 환경 설정 고정 IP 설정 Compute \u0026gt; Instances \u0026gt; Instance Details \u0026gt; Attached VNICs \u0026gt; VNIC Details \u0026gt; IPv4 Addresses 상기 경로에서 NO PUBLIC IP 선택하여 IP 삭제 후 RESERVED PUBLIC IP로 변경 우분터 사용자 계정 생성(option) ssh 로그인 현재 계정 ubuntu 암호 생성 사용자 계정 생성 생성 계정에 sudo 권한 부여 계정 변경 ssh 비번으로 접속 설정 /etc/ssh/sshd_config파일의 PasswordAuthentication 값을 \u0026ldquo;yes\u0026quot;로 변경 클라우드 포트 개방 Networking \u0026gt; Virtual Cloud Networks \u0026gt; {사용중인 VNC} \u0026gt; Security List Details 상기 경로에서 포트 개방 추가 우분투 방화벽 포트 개방 1 sudo iptables -I INPUT 5 -p tcp --dport 8070 -m state --state NEW,ESTABLISHED -j ACCEPT code-server 설치 code-server 다운로드 및 설치 https://coder.com/docs/code-server/latest/install 1 curl -fsSL https://code-server.dev/install.sh | sh 서비스로 실행하기 위해 systemctl로 enable 1 sudo systemctl enable --now code-server@$USER 외부 접속을 위해 .config/code-server/config.yaml파일을 수정한다. 1 2 3 4 bind-addr: 0.0.0.0:{포트번호} auth: password password: {비밀번호} cert: false 서비스를 재시작 후 동작을 확인한다. 1 2 sudo systemctl restart --now code-server@$USER sudo systemctl status code-server@$USER chrome 브라우저에서 접속 시 이미지가 안보일 경우 하기 세팅을 수행 chrome://flags 설정 의 Insecure origins treated as secure Enable 후 http://{접속IP}:{접속Port} 추가 ","date":"2022-12-12T00:00:00Z","permalink":"https://muonkmu.github.io/p/%EC%9B%90%EA%B2%A9-%EA%B0%9C%EB%B0%9C-%EC%84%9C%EB%B2%84-%EA%B5%AC%EC%B6%95/","title":"원격 개발 서버 구축"},{"content":"본 chapter에서는 Gradient를 구하기 위한 Backpropagation을 이해하고 Neural Network의 기본에 대해 설명한다.\nVideo : https://www.youtube.com/watch?v=d14TUNcbn1k Slide : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf (TODO: 귀차니즘의 압박으로 정리를 안해놓았지만 언젠간 해야지)\nBackpropagation Chain rule Sigmoid gate example Patterns in backward flow Gradients add at branches Vectorized operations Neural Network Artificial Neural Network Activation Function Neural networks Architectures ","date":"2022-10-25T00:00:00Z","permalink":"https://muonkmu.github.io/p/cs231n-chap-04-introduction-to-neural-networks/","title":"[CS231n] Chap 04 Introduction to Neural Networks"},{"content":"본 chapter에서는 딥러닝의 기본 개념인 Loss Function, Regularization, Optization(Gradient Descent)에 대해 다룬다\nVideo : https://www.youtube.com/watch?v=h7iBpEHGVNc Slide : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture3.pdf (TODO: 귀차니즘의 압박으로 정리를 안해놓았지만 언젠간 해야지)\nLoss function Regularization Softmax and SVM Optimization Image Feature ","date":"2022-10-17T00:00:00Z","permalink":"https://muonkmu.github.io/p/cs231n-chap-03-loss-function-and-optimization/","title":"[CS231n] Chap 03 Loss Function and Optimization"},{"content":"본 chapter에서는 Computer Vision의 핵심 Task 중 하나인 Image classification에 대해 이해하고 초기의 방법인 K-Nearest Neighbor Algorithm과 Linear Classification에 대하여 다룬다.\nVideo : www.youtube.com/watch?v=OoUX-nOEjG0\u0026list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk\u0026index=2 Slide : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture2.pdf (TODO: 귀차니즘의 압박으로 정리를 안해놓았지만 언젠간 해야지)\nImage Classification 개요 K-Nearest Neighbor Algorithm Linear Classification ","date":"2022-10-08T00:00:00Z","permalink":"https://muonkmu.github.io/p/cs231n-chap-02-image-classification/","title":"[CS231n] Chap 02 Image classification"},{"content":"6개월에 걸쳐 수료를 완료 했다. 3개월 코스라고 하던데\u0026hellip;\nhttps://www.coursera.org/learn/machine-learning homework repo : https://github.com/muonkmu/Coursera_AndrewNg_ML_Program.git (TODO: 귀차니즘의 압박으로 정리를 안해놓았지만 언젠간 해야지)\n","date":"2022-01-25T00:00:00Z","permalink":"https://muonkmu.github.io/p/coursera_ml-course-certificate/","title":"[Coursera_ML] Course certificate"},{"content":"이번 강의에서는 대규모의 대규모의 데이터가 있을 때, 처리하는 알고리즘에 대해서 알아보자.\nhttps://www.coursera.org/learn/machine-learning homework repo : https://github.com/muonkmu/Coursera_AndrewNg_ML_Program.git (TODO: 귀차니즘의 압박으로 정리를 안해놓았지만 언젠간 해야지)\n","date":"2022-01-07T00:00:00Z","permalink":"https://muonkmu.github.io/p/coursera_ml-week_10-gradient-descent-with-large-datasets/","title":"[Coursera_ML] Week_10) Gradient Descent with Large Datasets"}]