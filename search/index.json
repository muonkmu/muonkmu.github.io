[{"content":"SCNN Accelerator SCNN은 sparse encoding scheme을 이용해서 activation / weight sparsity 지원 Planar Tiled-Input Stationary-Cartesian Product-sparse (PT-IS-CP-sparse)라 부르는 새로운 Cartesian product flow를 제안 (activation / weight reuse) SCNN PT-IS-CP-Dense Dataflow PT-IS-CP-Dense dataflow는 convolution nested loop를 어떻게 분해할 것인가에 관한 것\nC X R X S 형태의 K개 filter, batch size N인 C X W X H 형태의 input activation 일 때 Input Stationary (IS) 가 적용되면 loop order는 C→W→H→K→R→S 가 됨 성능향상을 위해 blocking strategy 적용 (K output channel은 $K_c$ 사이즈의 K/$K_c$ output channel group으로 분리) K/$K_c$→C→W→H→$K_c$→R→S intra-PE parallelism을 위해 PE 내부에서 spatial reuse 활용\nfilter weight(F)와 input activation(I)가 각 buffer에서 fetch되고 이는 F X I array 곱셈기로 전송 filter weight와 input activation은 재활용 되며 partial sum은 향후 연산을 위해 메모리 접근 없이 저장됨 intra-PE parallelism을 위해 Spartial tiling 전략이 사용됨\nW X H input activation는 $W_t$ X $H_t$ Planar Tiles(PT)로 나눠져서 PE로 분배됨 또한 mutiple channel processing 지원 (C X $W_t$ X $H_t$이 PE에 할당됨) sliding window operation에서 edge에서 cross-tile dependency가 생기는데 data halo를 이용해 해결\nInput Halos : PE input buffer는 halo을 수용하기 위해 C x Wt x Ht보다 약간 큰 크기로 조정 Output Halos : PE accumulation buffer도 halo을 수용하기 위해 Kc x Wt x Ht보다 약간 큰 크기로 조정. Halo에는 출력 채널 계산이 끝날 때 누적을 완료하기 위해 인접 PE와 통신하는 불완전한 부분 합계가 포함. PT-IS-CP-Dense Dataflow의 최종 수식은 다음과 같다 SCNN PT-IS-CP-Sparse Dataflow PT-IS-CP-Sparse는 PT-IS-CP-Dense dataflow에서 파생되었고 filter weight와 input activation의 sparsity를 지원 filter weight는 Kc X R X S sparse block으로 압축, input activation은 Wt X Ht 사이즈 블럭으로 엔코딩 PE는 nonzero F 와 nonzero I를 곱해서 partial sum은 accumulator buffer에 output index와 저장됨 PT-IS-CP-Sparse는 compressed sparse index input activation / filter weigth / accumulator buffer를 패치 할 수 있도록 수정됨 SCNN Tiled Architecture SCNN은 Tiled architecture로 PT-IS-CP-Sparse를 지원\nPE는 halo를 교환 하기 위해 인접 PE와 연결되며 Layer Sequencer는 PE와 DRAM의 데이터 이동을 제어 Processing Element Architecture PE는 weight buffer, input/output activation RAM (IARAM/OARAM), multiplier array, scatter crossbar, accumulation buffer, Post-Processing Unit (PPU)으로 구성\ninput activation과 filter weight가 PE로 로드되고 multiplier array가 partial sum을 계산 후 acculumlation buffer에 저장 acculumlation buffer는 adder와 output channel entry를 가지고 있으며 double buffers 전략 사용 1개 버퍼는 partial sum을 계산 하고 다른 것은 output을 후처리를 위해 PPU로 전송 PPU는 몇 가지 다른 task를 수행 halo 영역을 위해 인접 PE와 partial sum을 교환 nonlinear activation, pooling, dropout 수행 output activation을 압축하고 ORAM에 씀 Data Compression filter weight와 input/output activation을 압축하기 위해 다른 것과 약간 수정된 엔코딩 방식 사용(책의 그림 참조)\nData vector : 0이 아닌 element 저장 index vector : 0이 아닌 element의 갯수와 이전에 0인 element의 갯수를 저장 SeerNet Accelerator Microsoft SeerNet은 quantizaition convolution을 이용해서 feature map sparsity를 예상하는 방법을 제안\nFeature Map(F)와 filter weight(W)는 Fq와 Wq로 양자화 되고 이를 이용해 quantized low bit inference를 수행하여 binary sparsity mask(M)을 생성 그리고 full precision sparse inference를 수행(앞의 M을 이용하는 듯) Low-Bit Quantization Low-Bit Quantization은 online/offline에서 filter weight를 양자화\noutput feature map의 dimenstion이 H*W 일 때 양자화 복잡도는 1/(HW) online 동작은 낮은 연산 복잡도와 오버헤드로 병렬처리를 제공하고 offline 동작은 추가 저장공간으로 양자화 오버헤드를 제거함 online quantization동안 binary mask 생성을 위한 quantized convolution이 수행되고 이 마스크를 가지고 spase convolution이 수행 됨 Efficient Quantization Full quantization 대신에 layer-by-layer quantization에 집중하고 output feature map을 예측하기 위한 low-bit quantization 적용\nReLU의 경우 output feature map의부호를 찾고 음수을 0으로 출력 시킴 Max pooling의 경우 정확도 없이 output feature map의 가장 큰 값만 찾음 Quantization flow 하기와 같음\nn-1이 양/음의 범위를 모두 커버하는 양자화 레벨 2n-1을 정의 최대 절대값 M을 찾음 양자화 값 x\u0026rsquo; = floor(X/M*2^(n-1)) Quantized Convolution 책에 Quantized Convolution, Quantized ReLU activation, Quantized batch normalization 수식 있음\nInference Acceleration Inference 성능향상을 위해 Intel AVX2 vector 연산 사용\nSparsity-Mask Encoding sparse convolution 성능 향상을 위해 row/column index vector를 이용해 sparsity mask를 엔코딩\nfeature map을 vector format으로 변환(다수의 feature map은 matrix 형태가 됨) column index에는 sparse bit의 column 위치 row index 에는 각 row column의 시작위치가 있음(책에 그림 볼 것) ","date":"2023-03-01T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-2/2/","title":"[AI HW Design] Chap08 Network Sparsity (2/2)"},{"content":"개인 프로젝트를 진행하는데 Xilinx에서 제공하는 레퍼런스의 개발환경이 제각각이다. 우선 기본적으로 IDE 2019.2 버전이 필요한데 현재 데스크탑에 설치되어 있는 개발환경은 2022.1이다. 재 설치를 하기 보다는 Docker로 가상화 공간에 별도의 개발환경을 설치해 보자\nDocker GUI How to use a GUI on the docker Docker에서 개발환경을 가상화 하는 것은 좋은데 Docker는 기본적으로 CLI 환경만 제공한다. 그러나 많은 임베디드 개발환경은 설치 및 실행에서 GUI를 필요로 하기에 Docker에서 GUI를 구성하는 방법은 아래와 같다.\ndesktop 환경이 설치된 Docker에서 VNC 이용 SSH 기반에서 X11 Forwarding을 이용해 HOST에서 GUI를 띄운다. 1번째 방법은 GUI docker image를 다운받아 설정하고 여기에 ssh의 ForwardX11 옵션을 활성화 하면 2번째 방법도 지원이 된다.\nDocker image 다운 및 실행 선구자가 LXDE 환경의 Docker를 개발하였다. 하기 사이트에서 사용법을 읽어보자.\nhttps://hub.docker.com/r/dorowu/ubuntu-desktop-lxde-vnc/ 다음과 같은 순서로 도커 이미지를 설치하고 실행하자(약 100GB의 용량이 필요하다)\n도커 이미지 다운 필요시 이미지 이름 변경 및 기존 이미지 삭제 해당 이미지의 설명을 참조하여 도커를 실행 1 2 3 4 sudo docker pull dorowu/ubuntu-desktop-lxde-vnc sudo docker image tag dorowu/ubuntu-desktop-lxde-vnc:lastest ${이미지이름}:${이미지tag} sudo docker rmi dorowu/ubuntu-desktop-lxde-vnc sudo docker run -d -p ${HTTP_PORT}:80 -p ${VNC_PORT}:5900 -p ${SSH_PORT}:22 -e RESOLUTION=1920x1080 -e VNC_PASSWORD=xilinx -e USER=xilinx -e PASSWORD=xilinx -v /dev/shm:/dev/shm -v ${작업 디렉토리 마운팅}:/home/xilinx/Workspace --name xilinx-dev ${이미지이름}:${이미지tag} 이후 Docker의 VNC에 연결하여 진행한다. 기본적으로 필요한 패키지들을 설치하고 만약 .bashrc가 없어서 터미널의 색깔이 없다면 해당파일을 복사한다.\n1 2 sudo apt-get install openssh-server cp /etc/skel/.bashrc $HOME Petalinux Install 사전준비 필요 패키지들을 설치한다. https://docs.xilinx.com/v/u/2019.2-English/ug1144-petalinux-tools-reference-guide 1 2 3 sudo dpkg --add-architecture i386 sudo apt update gawk make net-tools libncurses5-dev tftpd zlib1g:i386 libssl-dev flex bison libselinux1 gnupg wget diffstat chrpath socat xterm autoconf libtool tar unzip texinfo zlib1g-dev gcc-multilib build-essential screen pax gzip python2.7 cpio locale에 en_US.utf8을 추가한다. 1 2 sudo apt-get install -y locales sudo locale-gen en_US.utf8 Install Petalinux 설치파일을 다운로드 하고 설치한다. sudo 권한 없이 설치해야하므로 설치폴더의 소유자는 현재 user로 한다.\n1 2 3 4 sudo mkdir /tools/Xilinx/Petalinux/2019.2 sudo chown -R xilinx:xilinx /tools/Xilinx/Petalinux chmod +x ${설치파일} ./${설치파일} /tools/Xilinx/Petalinux/2019.2 Vivado Install 설치파일을 다운로드 하고 설치를 진행한다.(sudo 권한으로 설치?) 필요 시 libtinfo5f를 설치한다. 1 2 3 chmod +x ${설치파일} sudo ./${설치파일} sudo apt-get install libtinfo5 ","date":"2023-02-28T00:00:00Z","permalink":"https://muonkmu.github.io/p/xilinx-ide-installation-based-on-docker/","title":"Xilinx IDE installation based on Docker"},{"content":"8장은 시스템 throughput을 높이기 위해 ineffectual zoro operation을 스킵하는 다양한 구조에 대해 알아본다. (feature map encoding/indexing, weight sharing/pruning, quantized prediction)\nEnergy Efficient Inference Engine (EIE) 스탠포드 대학에서 나온 유명한 논문, 알고리즘(SW) + 가속기(SW) 양쪽에 대해 최적화한 논문으로 알고 있다. 중요도에 비해 설명이 부실한 듯\u0026hellip;\nEIE는 sparse matrix-vector mutiplication을 위한 compressed network model을 지원하고 weight sharing을 다룸 Leading Non-Zero Detection Network, Central Conrol Unit, Processing Element 로 구성 Leading Non-Zero Detection Network (LNZD) LNZD는 input activation으로 부터 nonzero element를 찾아내고 이를 LNZD node로 먹임 node는 nonzero value와 index를 PE로 브로드캐스팅 함 LNZD에 PE는 4개가 연결 Central Control Unit (CCU) CCU는 network segment computation을 제어\nhost와 커뮤니케이션 하고 PE state를 모니터링 함 두가지 동작모드로 나뉨 computing mode : CCU는 LNZD로 부터 nonzero input activation을 받고 이를 PE로 브로드캐스팅, 모든 input channel이 스캔될때 까지 반복됨 I/O mode : PE는 idle, activation과 weight가 DMA에 의해 접근 됨 Processing Element (PE) 요약하면 ifmap 값을 읽어와서 여기 포인터를 이용하여 해당 weight 값과 효율적으로 곱한다는 이야기인듯..\nPE는 activation queue, pointer read unit, sparse matrix unit, arithmetic unit, activation R/W unit으로 구성 computation 동안 CCU는 nonzero input element와 index를 activation queue에 브로드캐스팅 함, PE는 큐가 다차면 input element를 처리(브로드캐스트는 중지) activation queue는 PE가 work backlog를 구축할 수 있도록 해줌(load balancing 문제 해결) pointer read unit은 activation queue의 인덱스를 이용하여 nonzero element의 시작/종료 포인터를 찾음 싱글 사이클에 이를 처리하기 위해 포인터는 LSB와 함께 odd/even SRAM에 저장(의미를 잘 모르겠음) sparse matrix read unit은 sparse matrix memory로 부터 포인터를 이용하여 0이 아닌 값을 읽음(fmap?) arithmetic unit은 activation queue와 sparse matrix memory의 0이 아닌 값 MAC 연산 연속적으로 가산기 사용되는 경우를 위한 bypass 경로 존재 (그림 보자) activation read/write unit의 경우 fully connected layrer를 위한 source/destination activation register를 가지고 있으며 이는 다음 레이어 연산시 교체됨 Deep Compression EIE는 pruning과 weight sharing을 통해 네트워크 압축하기위해 Deep Commpression을 적용, 적용 예는 다음과 같음 (책에 그림 예시를 보자) 4X4 weight matrix라면 16개의 값을 4개의 index(code book)로 만듬 index에 해당하는 weight는 Gradient를 가지고 fine-tunnig됨 MAC 연산은 weight와 input activation vector가 0이 아닌 값에 대해서 수행 EIE는 interleaved Commpressed Sparse Column(CSC) 적용 (Eyeriss와 약간 다르므로 책참조) v는 0이 아닌 값, z는 0이 아닌 값 해당 v 값 앞에 0의 개수 v,z는 하나의 large array pair에 $p_j$(벡터의 시작 포인터), $p_{j+1}$(마지막 항목 다음 번 포인터)와 함께 저장됨 Sparse Matrix Computation 4개의 PE에서 input activation vector(a)는 weight matrix(w)와 곱해짐 a를 스캔해서 0이 아닌 $a_j$는 인덱스 값과 함께 브로드캐스팅 됨 PE는 index에 대응하는 0이 아닌 $w_j$를 곱합 PE는 벡터 v를 $p_j$에서 $p_j+_1$까지만 스캔 (책에 예시 그림 있으니 참조) Cambricon-X Accelerator 병렬연산에서 Nonzero neuron을 선택하기 위해 indexing scheme을 적용 Control Processor (CP), Buffer Controller (BC), Input Neural Buffer (NBin), Output Neural Buffer (NBout), Direct Memory Access (DMA) Module, Computation Unit (CU)으로 구성 중요 element는 BC Tn indexing unit(nonzero neuron을 인덱싱하는 유닛이며 PE와 수가 같다) Computation Unit (CU) CU는 Tn개 PE로 구성되며 모든 PE는 fat tree 형태로 연결 PE는 PE Functional Unit(PEFU)와 Synapse Buffer(SB)로 구성 BC로 부터 neuron을, local BC로 부터 synaps를 읽어서 PEFU에 제공하며 output neuron은 BC에 다시 씀 Tn개 PEFU는 Tm개 곱셈기와 가산기를 가져서 TnXTm 곱셈이 가능 SB는 synapse를 저장하고 메모리 access를 최소화 하기 위해 디자인, 책에서는 하기 예시를 듬(책그림 참조) PE는 4개이고 output neuron 0이 input neruon 2개 연결, output neuron 1이 input neruon 5개 연결 output neuron 0의 weight는 address 0에 output neuron 1의 weight는 address 1/2의 SB에 저장 output neuron 0 계산 시 SB를 1번 읽고 output neuron 1은 두 번 읽음 synapse의 수는 output neuron마다 다를 수 있기 때문에 SB가 비동기적으로 데이터를 로드하여 전체 성능을 향상 Buffer Controller BC는 Indexing Module (IM)과 BC Functional Unit (BCFU) 로 구성 BCFU는 인덱싱을 위해 neuron을 저장 IM은 BC의 nonzero nueron을 구분하고 nonzero indexed nueron만 전송 BC는 input neurons을 NBin에서 PE로 보내거나 BCFU로 제공, PE의 계산결과는 BCFU에 저장 또는 NBout에 쓰여짐 IM에는 두가지 하기 두가지 옵션이 있음 (책에 그림에 잘 나와 있음) direct indexing : nonzero nueron의 여부를 0/1로 표현한 binary string 사용 step indexing : nonzero nueron의 거리를 사용 step indexing이 area와 power를 적게 소모함 ","date":"2023-02-25T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/","title":"[AI HW Design] Chap08 Network Sparsity (1/2)"},{"content":"6장은 In-Memory Computation, 7장은 Near-Memory Computation 에 대한 내용이다. 현재 개발하고자 하는 가속기와 동떨어지는 내용이라 판단해서 개요만 보고 스킵할 예정이다. 삼성이나 하이닉스를 다녀야 쓸모있지 않을까 싶다.\nIn-Memory Computation 여기서는 메모리와 로직을 stacking 하는 방식의 Processor-In-Memory(PIM)에 대해 설명한다. 다른 방식의 PIM도 있는 걸로 아는데\u0026hellip;\nNeurocube Architecture Nerocube는 parallel neural processing unit과 High Bandwidth Memory(HBM)을 스택한 Hybrid Memory Cube(HMC)를 이용 이는 stacked memrory에서 PE로 직접 데이터 로드가 가능함(레이턴시 감소) Teris Accelerator Teris는 Eyeriss의 3D Memory와 함께 Row Stationary(RS) dataflow 채택 NeuroStream Accelerator NeuroStream은 HMC의 modular extension인 Smart Memory Cube(SMC)를 이용 Near-Memory Computation DiDianNao Supercomputer 대용량 eDRAM을 통해 DianNao의 memory bottleneck을 해결하고자 함 모든 synapse를 수용할 후 있는 거대 storage를 제공하는 Neural Function Unit(NFU)를 지닌 16개의 tile로 구성 NFU는 4개의 eDRAM bank와 time-interleaved 통신(?) 함 (eDRAM의 레이턴시가 크기 때문) Cnvlutin Accelerator DiDianNao에서 파생되었으며 다수의 DiDianNao를 고속 인터페이스로 연결, 거대 parallel mutiplication lane 구조 채택 ","date":"2023-02-21T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap06-07-in/near-memory-computation/","title":"[AI HW Design] Chap06 \u0026 07 In/Near Memory Computation"},{"content":"앞의 포스트가 너무 길어져서 Eyeriss version2 부분은 현 포스트로 나눔\nEyeriss Accelerator Ver.2 irregular data pattern 과 network sparsity 지원을 위한 Eyeriss V2 공개\n새로운 NoC 구조 : data reuse가 적을 땐 external memory에서 더 많은 데이터를 PE로 가져오고 data reuse가 많을 땐 spartial Data를 sharing CSC 엔코딩 적용 RS+ 적용 Eyeriss V1이 GLB\u0026lt;-\u0026gt;PE 연결을 위해 flat multicast NoC를 사용했지만 V2에서는 flexible and mesh NoC 사용, 이 hierarchical mesh는 GLB cluster, Router Cluster, PE cluster로 구성되며 하기 3가지 타입의 데이터 이동 지원\nifmap은 GLB cluster에 로드됨, 이는 GLB 메모리에 저장되고 Router Cluster로 전송 psum은 GLB 메모리에 저장 되고, ofmap은 external memory에 바로 저장 됨 fmap은 Router Cluster로 전송되고 PE spad에 저장 (책에 v1 과 v2 구조에 대한 비교 그림이 있으니 찾아보자) Hierarchical Mesh Network (HM-NoC) 전통적인 Network-on-Chip 구조는 하기와 같으며 장단점을 가지고 있음 (책 그림 참조)\nBroadcast Network : high reuse / low bandwidth Unicast Network : high bandwidth / low reuse All-to-All Network : high reuse, high bandwidth / scale difficulty Eyeriss V2는 RS+를 지원하기 위해 HM-NoC 구조를 제안, all-to-all network에서 파생되었으나 4가지 모드를 가짐\nBroadcast: single input and single weight Unicast: multiple inputs and multiple weights Grouped multicast: shared weights Interleaved multicast: shared inputs HM-NoC는 source, destination, router로 구성되며 design phase에서 cluster로 그룹핑되고 operation mode에서는 고정됨.\nRouter cluster가 다른 cluster와 one-to-one, many-to-many, source/destination 구조로 연결 Router cluster는 4개의 source/destination port를 가지며 4가지 routing mode (broadcast, unicate, grouped/interleaved multicast)를 가짐 책에서는 다음과 같이 예시를 듬\nConvolution layer : ifmap과 fmap이 reuse 되며 grouped multicast 또는 interleaved mode 로 구성 Depth-wise Convolution layer : fmap만 reuse 되며 fmap이 PE로 broadcast, ifmap은 GLB에서 로드 Fully connected layer : ifmap이 모든 PE로 broadcast, fmap은 unicast mode로 로드 Input Activation HM-NoC Router Cluster 안의 3개 ifmap router는 GLB cluster의 ifmap SRAM과 연결 ifmap routerd의 3개의 source/destination port 다른 클러스터와 연결, 1개는 메모리에서 데이터로드, 1개는 PE 연결 책에 그림과 상세 설명이 있으니 참조하자 Filter Weight HM-NoC Router Cluster 안의 각 fmap router는 PE cluster 안의 PE row와 연결 vertical mesh는 사라지고 horizontal mesh 만 데이터 reuse를 위해 남음 책에 그림과 상세 설명이 있으니 참조하자 Partial Sum HM-NoC Router Cluster 안의 4개의 psum router는 GLB cluster의 psum SRAM과 PE cluster의 PE column과 연결됨 horizontal mesh는 사라지고 vertical mesh만 psum accumulation을 위해 남음 책에 그림과 상세 설명이 있으니 참조하자 Compressed Sparse Column (CSC) Format Eyeriss V2는 ifmap과 fmap 둘 다에 CSC 포맷 적용 (zero operation skipping), CSC 포맷의 구조는 다음과 같다\nData vector : 0이 아닌 값 Counter vector : Data vector의 item 기준, 해당 열에서 앞에 있는 0의 값의 갯수 Address vector : 각 열을 기준으로 이전 열까지 0이 아닌 item의 누적 갯수 Eyeriss V2는 PE는 zero operation skip을 위해 7 pipeline stage와 5 spad (ifmap, fmap, psum 저장)로 수정\n첫째로 non-zero data인지 확인하기 위해 address를 검사하고 fmap 로드 전 먼저 ifmap을 로드(zero ifmap skip을 위해) ifmap/fmap이 0이 아니면 계산 pipeline 수행, fmap이 0이면 pipeline disable Row Stationary Plus (RS+) Dataflow PE utilization을 높이기 위해 RS+ dataflow 적용\nmodel dimension을 다른 PE dimension에 매핑하기 위해 데이터를 tiling, spatial fragmentation 함 depth-wise convolution 시 PE utilization이 낮은 문제점 해결 ","date":"2023-02-19T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/","title":"[AI HW Design] Chap05 Convolution Optimization (3/3)"},{"content":"유명한 MIT의 Eyeriss Accelerator 논문이다. 아직까지 관련 프로젝트가 진행 중인 것으로 보이며 찾아보면 관련 논문에 대하여 리뷰해논 자료가 꽤 많다. 잘 소개된 곳 한 곳 (허락없는 링크..)\nhttps://m.blog.naver.com/PostView.naver?isHttpsRedirect=true\u0026blogId=kangdonghyun\u0026logNo=220990374125 Eyeriss Accelerator Eyeriss Accelerator는 data access를 최소화하기 위한 Row Stationary (RS) Dataflow를 제안하며 다음과 같은 특징을 지님\nSpartial architecture with sequential processing configuration Spartial architecture can exploit high compute parallelism using direct communication between an array of relatively simple processing engines (PEs). Row Stationary (RS) Dataflow 구현 Four level memory hierarchy : PE scratch pad와 inter-PE 통신을 최대한 이용하고 Global Buffer와 외부 메모리의 data transfer를 최소화 point-to-point \u0026amp; multicast Network-on-Chip(NoC) 아키텍쳐 지원 Run-Length Compression (RLC) 포맷 지원 : zero operation 제거 Eyeriss System Architecture Eyeriss는 과 communication link clock 두가지 clock domain을 가짐 data processing core clock : 12X14 PE array, Global Buffer(GLB), RLC codec, ReLu 배치, PE가 local scratchpad를 이용하여 연산하거나 PE가 인접 PE 또는 GLB와 통신 하는것을 가능하게 함 communication link clock Four level memory hierarchy GLB \u0026lt;-\u0026gt; external memory : asynchronous FIFO 이용 PE \u0026lt;-\u0026gt; GLB : NoC 이용 ReLU \u0026lt;-\u0026gt; RLC codec local temporary data storage using scratchpad 분리된 clock으로 PE는 다른 PE와 같은 클럭 시간에 독립적으로 동작가능하고 link clock은 external memory와 64bit 양방향 버스로 data 전송을 제어 Eyeriss Acc는 Convolution network를 레이어 단위로 진행 첫째로 PE array를 레이어 function/size에 맞게 구성하고 매핑 수행 및 전송 패턴을 결정 input feature map 및 filter map은 external memory에서 PE로 로드되고 output feature map은 다시 external memory로 쓰여짐 2D convolution to 1D multiplication convolution을 수행할 때 2D feature/filter map을 1D로 바꾸어 수행해서 PE에 순차적으로 로딩한다는 이야기를 길게 써놓음 (궁금하면 책을 보자) 2D convolution을 1D vector 와 Toeplitz 행렬(대각선의 성분이 모두 같은 매트릭스)의 곱으로 변환된다 책에 어떤 순서로 feature/filter 1D vector가 PE에 로드/계산되는지 그림으로 있다. Stationary Dataflow 칩에 대한 이야기는 아니고 이전의 stationary 전략에 대해 소개한다. (연산을 어떤 데이터를 고정, 이동 시킬지)\nOutput Stationary Patial Sum의 read/write를 local accumulation을 통해 최소화 Weight Stationary filter map을 local buffer에 두고 계속 활용 Input Stationary input feature map을 local buffer에 두고 계속 활용 다른 전략보다 효율이 안좋은데 약점은 다른 전략보다 convolution연산에 더 많은 cycle이 필요 Row Stationary (RS) Dataflow Eyeriss는 1D vector multiplication 수행하는데 RS dataflow 전략을 사용\nfilter map 행은 PE들에서 수평하게 재사용 input feature map 행은 PE들에서 대각적으로 재사용 partial sum 행은 PE들에서 수직적으로 재사용 RS dataflow에서 데이터는 계산 동안 PE에 저장됨(데이터 이동 최소화) time-interleaved approach를 통해 fmap과 ifmap은 같은 clock cycle 내에서 재활용 계산이 완료되면 Psum은 근접 PE들로 이동(다음 계산을 위해서) Filter Reuse fmap이 spad에 로드 되고 고정된다. 다수의 ifmap도 spad에 로드되고 사슬처럼 연걸됨 Input Feature Maps Reuse ifmap이 먼저 PE에 로드 되고 2개의 fmap은 time-interleaved(연산?) 됨 1개의 ifmap으로 2개의 fmap과 1D 연산 수행 이 것은 전체적인 스피드를 올려주지만 fmap과 psum의 time-interleave 연산을 지원하기 위해 큰 spad가 필요 Partial Sums Reuse fmap/ifmap 둘 다 PE에 로드되며 둘 다 time-interleaved 함 psum은 같은 채널 끼리 합쳐짐 fmap/ifmap 둘 다 PE에 로드되어야 하므로 필요한 spad의 용량이 증가 Run-Length Compression (RLC) ReLU 연산 결과 0 값이 많아 지므로 이는 network의 sparsity가 도입됨 zero computation을 피하기 위해 Eyeriss는 64 bit RLC 포맷을 도입 ([5bit]앞에 값이 0인 element 갯수 + [16bit]0이 아닌 값)*3 + [1bit]마지막 item인지 나타내는 flag 첫번째 layer의 ifmap 값을 제회하고 모든 fmap/ifmap은 RLC 포맷으로 external memory에 저장 External Memory에서 입출력 될 때, RLC encoder/decoder를 통과하게 됨 Glabal Buffer (GLB) external memory와 데이터 전송을 위해 GLB 채택 GLB에는 fmap/ifmap/ofmap/psum 이 저장 GLB는 PE가 연산하는 동안 다음 fmap을 preload Processing Element (PE) Architecture PE는 fmap, ifmap, psum을 위한 3가지 타입의 spad를 가짐 datapath는 3가지 pipeline stage에 의해 구성(spad access, fmap/ifmap multiplication, psum accumulation) 16bit 연산을 사용하며 32bit 연산결과는 16bit로 절삭 값이 O인 ifmap이 발견되면 spad에서 fmap 값을 읽는 것과 연산 로직을 끔(전력소모를 줄이기 위해) Network-on-Chip (NoC) NoC는 GLB와 PE 사이의 데이터 이동을 관리, 하기 2개로 구분 Global Input Network (GIN) : GLB \u0026lt;-\u0026gt; PE 간 single cycle multicast 이용 데이터 전송 Y-Bus는 12개의 X-bus와 연결되며 X-bus는 14개의 PE가 연결 top level controller는 \u0026lt;row,col\u0026gt; 태그를 생성하고 Y-bus / PE의 Multicast controller가 tag를 비교하여 데이터 전송 결정 책에 AlexNet의 예시 있음 Global Output Network (GON) : 별다른 설명 없음 ","date":"2023-02-18T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/","title":"[AI HW Design] Chap05 Convolution Optimization (2/3)"},{"content":"Convolution은 90% 이상의 Computing resource를 사용하며, data access를 줄이기 위해 feature maps reuse / filter weights reuse / partial sum reuse 같은 전략이 사용된다. 이번 챕터에서는 filter decomposition과 Row Stationary(RS) flow를 설명한다.\nDeep Convolution Neural Network Accelerator (DCNN) DCNN은 클라우드 타겟이 아닌 엣지 디바이스 타겟으로 UCLA에서 개발, 다음과 같은 특징을 지님\nStreaming data flow minimizes data access 병렬 컴퓨팅을 위해 bandwidth 향상이 아닌 Interleaving architecture Large-size filter decomposition supports arbitrary convolution window 추가적인 pooling functional unit을 통한 Convolution Unit(CU)의 workload 감소 System Architecture DCNN의 구성은 다음과 같다\nBuffer Bank 중간 데이터 저장 및 외부 메모리와 데이터 교환 목적 Layer Input 용, Layer Output 용 2가지 셋으로 나누어짐 또한 odd/even channel/feature를 위한 Bank A와 Bank B로 나누어짐(Interleaved) Column Buffer Buffer banck의 데이터를 CU engine의 input data type으로 remap Convolution Unint(CU) engine CU engin은 kernel size 3X3까지 지원하는 16개의 Convolution unit으로 구성 16bit fixed-point 연산 local Prefetch unit이 DMA로 부터 weight/bias 값을 주기적으로 업데이트 함 Accumulation (ACCU) buffer convolution 동안 partial sum 연산 또는 Max pooling 연산 수행 Control Command는 외부 메모리에서 128-depth FIFO로 로드 되며 하기 2가지로 분류됨 configure command : network layer를 구성하고 pooling/ReLU function 활성화 excution command : convolution/pooing 초기화 및 필터 decompose 기술 Filter Decomposition 다양한 필터 커널 사이즈 대응을 위해 3X3 CU engine을 이용하여 filter decomposition 기술 이용 커널 사이즈가 3의 배수가 아니면 zero-padding 이용 convolution 후 결과는 one output feature map으로 재결합 됨 상세 사항은 책 참조 Streaming Architecture 데이터의 이동을 최소화하기위해 Filter Weights Reuse와 Input Channel Reuse 사용\nFilter Weights Reuse 3X3 convolution : filter weight는 CU engine에 저장되고 input feature map이 CU engine으로 공급되며 연산이 완료 될 때까지 filter weight는 업데이트 되지 않음 1X1 convolution : CU unit의 9개 곱셈기 중 7개는 off 되고 2개만 odd/even 채널의 partial sum 계산을 위해 사용 Buffer Bank의 output bandwidth를 최대화하기 위해 구조는 하기와 같다(input cannel 및 Column buffer 구조 이야기 같은데) 16개의 row 데이터는 odd/even data set으로 나뉨 2개의 FIFO는 각 데이터 셋에 페어링 됨 (8개의 row 데이터) 8개의 input row data는 10개의 overlapped data로 매핑 Input Channel Reuse 1X1 convolution을 위해 interleaved architecture 사용 (16개 데이터 셋이 odd/even 채널로 2개 데이터셋 구분됨) 2개의 데이터 셋은 다른 filter weight와 곱해져 32개 output이 나옴 출력은 같은 odd/even 채널 끼리 더해짐 위의 내용과 비슷한 것 같은데 filter weight가 이동하고 input 값이 고정이라는 걸 다시 설명한 듯 Pooling pooling function은 average pooling과 max pooing 이 다른 구조로 분리되어 있음\nAverage Pooling Average Pooling function은 Convolution layer에서 Inpu/Output 채널이 같은 사이즈인 CU엔진에 의해 구현 kernel의 사이즈가 pooling window와 일치하는 대응되는 filter weight는 1/K^2으로 되고 나머지는 0으로 된 후 convolution 수행 Max Pooling Max pooling은 ACCU에서 별도 모듈로 구현 Max pooling 모듈은 scratch pad에 있는 8개의 output 값과 연결되며 이는 다른 stride를 지원하기 위해 MUX와 연결 MUX의 출력은 MAX Pooling 계산기로 가는데 이는 3개의 입력과 1개의 output feedback 입력을 받아 계산하며 인풋이 없어질때까지 연산 반복 Convolution Unit(CU) Engine 3X3 CU engine은 9개의 PE(input feature와 filter weight를 곱함)와 1개의 ADDER로 구성 다른 커널 사이즈 지원을 위해 PE는 On/Off 기능을 가짐 상세 내용은 책 참조 (그림과 3X3일 때, 1X1일때 예시 있음) Accumulation (ACCU) Buffer ACCU는 scratch pad에 partial sums 과 stores output feature maps을 저장 ACCU는 partial product accumulation을 위한 Ping-pong buffer, Max pooling을 위한 temporary storage, readout block으로 구성 (책에 그림 참조) Convolution이 진행되는 동안 1개 버퍼는 덧셈만 하고 다른 하나의 버퍼는 Max pooling을 진행, convolution이 완료된후 각 버퍼의 연결이 switch 됨 Model Compression Model Compression을 하기 위해 training 과정에서 모델을 pruning하고 filter weights를 codebook으로 quantization 함. ","date":"2023-02-12T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/","title":"[AI HW Design] Chap05 Convolution Optimization (1/3)"},{"content":"Target Board는 현재 소유하고 있는 ZCU104를 사용하기로 하고 EVB의 번들 카메라인 See3CAM_CU30를 사용하기로 하였다. 출력은 보드에 DP/HDMI가 있는데 DP는 PS 영역이며 HDMI는 사용자가 PL영역에 구성해야 한다. 그래서 DP로 결정. 선정 사유는 역시 Reference를 구하기 쉽다는것에 있다. 상기 ZCU104 + See3CAM_CU30의 reference design은 xilinx의 Embedded-Reference-Platforms 또는 Zynq-UltraScale-MPSoC-VCU-TRD-2022.1에서 확인할 수 있으나 필자는 봐도 어떻게 구성되어 있는지 잘 모르겠다\u0026hellip;\n기본 지식 See3CAM_CU30은 USB3.0 카메라이므로 리눅스에서 Usb Video Clss (UVC)를 gadget을 사용하여 연결한다. UVC : 웹캠이나 캠코더 같은 비디오 스트리밍이 가능한 장치를 기술하는 USB device class Video4Linux2(v4l2) 비디오 캡쳐 시스템을 위한 디바이스 드라이버의 모음이자 표준 API MIPI/USB camera 카메라등을 지원하는 것으로 봐선 UVC 위에서 표준 추상화 계층을 제공하는 것 같다. 출력은 기본적으로 Frame buffer 및 X11 + DRM KMS 구조를 지닌다. petalinux config 이전 포스트인 zcu104 개발환경 설정에서 다음 드라이버 및 프로그램을 설치한다.\nkernel 커널은 하기 모듈이 필요하다. BSP를 사용했다면 거의 바꿀 것 없지만 petalinux-config -c kernel로 다음 기능을 확인하자.\n카메라 입력 : USB gadget driver, web camera/video driver 모니터 출력 : xilinx DRM KMS driver, frame buffer driver 1 2 3 4 5 6 7 8 9 10 11 12 13 CONFIG_MEDIA_CAMERA_SUPPORT CONFIG_MEDIA_CONTROLLER CONFIG_VIDEO_V4L2_SUBDEV_API CONFIG_VIDEO_ADV_DEBUG CONFIG_MEDIA_USB_SUPPORT CONFIG_USB_VIDEO_CLASS CONFIG_USB_VIDEO_CLASS_INPUT_EVDEV CONFIG_V4L_PLATFORM_DRIVERS CONFIG_VIDEO_XILINX 및 그 외 CONFIG_DRM_XLNX 및 그 외 (필요한지??) CONFIG_USB 및 기타 가젯 필요한거 CONFIG_USB_GADGET_XILINX CONFIG_USB_CONFIGFS 및 그외 (필요한지 잘 모르겠음) RootFS petalinux-config -c rootfs RootFS에는 gstreamer/opencv/x11/v4lutil/gcc 패키지그룹, gstreamer 라이브러리, vim, python3 등을 설치한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 CONFIG_packagegroup-petalinux-gstreamer CONFIG_packagegroup-petalinux-opencv CONFIG_packagegroup-petalinux-x11 CONFIG_packagegroup-petalinux-v4lutils CONFIG_packagegroup-core-buildessential CONFIG_vim CONFIG_python3 CONFIG_python3-shell (?) CONFIG_python3-threading (?) CONFIG_python3-multiprocessing (?) CONFIG_gstreamer1.0 CONFIG_gstreamer1.0-plugins-base CONFIG_gstreamer1.0-plugins-good 카메라 및 프래임 버퍼 테스트 상기 설정으로 빌드 및 부팅 후 USB 캠을 연결한다. 그 후 아래 명령어로 카메라의 정보를 확인 할 수 있다.\nv4l2-ctl --list-devices : 연결된 디바이스 확인 v4l2-ctl -d ${디바이스번호} --all : 카메라 capability 등 모든 정보의 출력 v4l2-ctl -d ${디바이스번호} --list-formats-ext : 지원하는 포멧 확인. DP 포트와 모니터를 연결하면 Frame buffer /dev/fb# 이 생성됨을 확인할 수 있다. fbset명령으로 정보를 조해할 수 있다.\npython기반으로 opencv를 이용해서 카메라의 영상을 캡쳐 및 Framebuffer로 출력해 보자. 하기 코드는 테스트 용으로 카메라 및 프레임 버퍼의 설정 부분은 제외했으므로 출력이 이상할 수 있으니 필요하면 자신의 환경에 맞게 고쳐야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import os import cv2 import time capture = cv2.VideoCapture(0) time.sleep(0.1) # (success, reference) = capture.read() # cv2.imwrite(\u0026#39;${이미지 저장 경로}/${저장 이름}\u0026#39;,reference) while 1 : (ret, capFrame) = capture.read() frame16 = cv2.cvtColor(capFrame, cv2.COLOR_BGR2BGR565) fbframe = cv2.resize(frame16, (1920,1080)) with open(\u0026#39;/dev/fb0\u0026#39;, \u0026#39;rb+\u0026#39;) as buf: buf.write(fbframe) capture.release() cv2.destroyAllWindows() 참고 자료 https://github.com/Xilinx/Embedded-Reference-Platforms-User-Guide/tree/2019.2 https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/2322268161/Zynq+UltraScale+MPSoC+VCU+TRD+2022.1 https://www.hackster.io/whitney-knitter/using-a-usb-web-camera-with-the-minized-5783b1 https://www.e-consystems.com/blog/camera/products/getting-started-with-xilinx-zynq-ultrascale-mpsoc-zcu104-evaluation-kit-and-see3cam_cu30_chl_tc_bx/ https://m.blog.naver.com/overcrash3/120105061216?referrerCode=1 ","date":"2023-02-08T00:00:00Z","permalink":"https://muonkmu.github.io/p/mw_project-camera-%EC%9E%85%EB%A0%A5-%EB%B0%8F-dp-%EC%B6%9C%EB%A0%A5-%EA%B5%AC%EC%84%B1/","title":"[MW_project] Camera 입력 및 DP 출력 구성"},{"content":"zcu104 petalinux를 포팅하기 위한 일주일 간 삽질의 기록이다. xilinx에서 제공하는 training reference를 따라하면 간단하지만 이는 SD카드에 커널과 루트파일 시스템을 삽입하는 방법이다. 실제 개발의 편의성을 위해 TFTP 및 NFS를 이용하여 부팅하는 방법을 다룬다.\n목표 하기 boot config를 지원하는 petalinux의 포팅 방법 설명 (vivado/petalinux 2022.1 기반)\njtag로 u-boot까지 다운로드 및 부팅 -\u0026gt; u-boot에서 tftp/pxe로 리눅스 kernel 및 device-tree 로드 명령-\u0026gt; 커널에 의한 NFS로 RootFS 로드 SD카드로 u-boot까지 다운로드 및 부팅 -\u0026gt; u-boot에서 tftp/pxe로 리눅스 kernel 및 device-tree 로드 명령-\u0026gt; 커널에 의한 NFS로 RootFS 로드 SD카드로 u-boot, 리눅스 kernel, device-tree 로드 -\u0026gt; 커널에 의한 NFS로 RootFS 로드 Hardware description config 우선 베이스 프로젝트는 리눅스 포팅이 목적이므로 PS영역만 셋업한다. xilinx에서 제공하는 training reference를 그대로 따라해도 무방하다. (https://xilinx.github.io/Embedded-Design-Tutorials/docs/2022.2/build/html/docs/Introduction/ZynqMPSoC-EDT/3-system-configuration.html) zcu104_bsp에 사용되는 hw config을 보고 싶다면 후에 기술할 bsp에 기반한 petalinux 프로젝트 hardware 폴더에 관련 프로젝트가 들어있다. vivado project follow vivado에서 zcu104보드 프로젝트를 만든다. Project is an extensible Vitis platform 은 vitis에서 xrt 라이브러리 등을 사용할 때 필요하다. 현 프로젝트에서는 미선택 board 세팅에서 ZCU104를 선택 create block design을 선택하여 디자인 블럭 생성 zynq_mpsoc ip 를 추가하고 borad preset을 적용한다. 지금은 PL 영역이 필요없으로 AXI_HPM/HP 포트를 미사용으로 설정 Validate Design 으로 디자인 검증 후 Create HDL Wrapper 로 래퍼를 생성한다. Generate Block Design을 실행 후 bit stream (*.bit) 을 생성한다. 현재는 pl 영역의 디자인이 없으므로 bit-stream을 생성하지 않아도 무관한다. Export Hardware로 xsa 파일을 생성한다. 현재는 pl부분의 디자인이 없으므로 bit-stream을 포함하지 않아도 되며 포함하여도 상관없다. Petalinux porting project create 처음에는 기초부터 시작하고자 base template 프로젝트로 시작하였지만 포팅 시 부팅이 잘 안되었다. 1 2 petalinux-create -t project --name ${프로젝트 이름} --template zynqMP petalinux-config --get-hw-description=${xsa파일} --silentconfig 그래서 xilinx에서 제공하는 bsp 기반으로 프로젝트를 만들었다. 1 2 petalinux-create -t project --name ${프로젝트 이름} -s ${bsp파일} petalinux-config --get-hw-description=${xsa파일} --silentconfig 위의 두 프로젝트의 폴더/파일을 비교해보면 BSP를 위해 커널등이 어떻게 설정되어 있는지 알 수 있다. bsp 기반으로 만들어진 프로젝트의 경우 README에 BSP가 어떤 설정을 가지고 만들어져 있는지 나와 있다. 위의 기본 템플릿 프로젝트와 파일과 비교해서 보면 몇가지 설정에 대한 설명이 누락되었음을 알 수 있다. tftp boot config 이전 post인 ubuntu 환경 설정을 내용을 참고하여 host에 tftp 서버를 설정한다. petalinux-config 명령을 실행하여 Image Packaging Configuration \u0026gt; Copy final images to tftpboot에 host tftp서버 폴더를 지정한다. 만약 향후에 RootFS를 INITRAMFS으로 할려고 할 시 built-in FIT image를 위한 임시 ram 사이즈가 작아서 부팅 시 \u0026ldquo;There\u0026rsquo;s no \u0026lsquo;/dev\u0026rsquo; on rootfs\u0026rdquo; 에러가 난다. 이럴 경우 petalinux-config의 Image packaging configuration \u0026gt; INITRAMFS/INITRD Image name 을 petalinux-image-minimal로 변경 한다. NFS rootFS config 이전 post인 ubuntu 환경 설정을 내용을 참고하여 host에 NFS 서버를 설정한다. petalinux-config 명령을 실행하여 Image Packaging Configuration 에서 하기 내역을 설정 Root File System Type에서 NFS를 선택 Location of NFS root directory에 host nfs 폴더를 지정 NFS Server IP address 에서 host ip를 지정 petalinux-config -c kernel에서 하기 내역이 설정 되어 있는지 확인 Networking support \u0026gt; IP: kernel level configuration 의 IP:DHCP support, IP:BOOTP support, IP:RARP support File systems \u0026gt; Network file systems \u0026gt; Root file systems의 NFS 체크 상기 내역까지가 매뉴얼의 내용인데 적용해보면 nfs 버전 문제로 nfs RootFS가 마운트 되지 않는다. bootargs에서 nfs version을 3으로 변경한다. petalinux-config에서 DTG Setting \u0026gt; Kernel Bootargs \u0026gt; generate boot args automatically를 해제 (해제하기전에 설정되어 있는 bootargs를 copy) 위에 복사한 것을 bootargs를 작성하는 란에 붙여넣고 nfsroot부분에 nfsvers=3 추가 ex) earlycon console=ttyPS0,115200 clk_ignore_unused root=/dev/nfs nfsroot=192.168.1.30:/home/minwook/xlx_nfsrfs,tcp,nfsvers=3 ip=dhcp rw MAC 설정 u-boot 부팅 시 마다 아이피가 달라지지 않도록 MAC를 설정한다. petalinux-config 명령의 Subsystem AUTO Hardware Setting \u0026gt; Ethernet Setting \u0026gt; Ethernet MAC address 사실 zcu104의 맥 주소는 부팅 시 eeprom에서 읽어 온다는데 u-boot에서는 안되는 것 같다(사실 잘 모르겠다.) build 및 부팅 준비 u-boot 및 커널 등을 빌드한다. jtag로 부팅 시키기 위해서는 pre-built 폴더에 이미지들이 준비되어 있어야 한다. petalinux-package를 이용해 준비하자. host의 NFS 서버 폴더에 RootFS를 압축 해제 시켜 NFS 부팅을 준비한다. 향후 SD 카드 등에 부트로더/부트 스크립트를 복사할 경우를 대비하여 부팅이미지를 생성하자. 1 2 3 4 5 petalinux-build petalinux-package --prebuilt cd images/linux tar -xzf rootfs.tar.gz -C ${NFS 서버 폴더} petalinux-package --boot --fsbl zynqmp_fsbl.elf --fpga system.bit --pmufw pmufw.elf --atf bl31.elf --u-boot u-boot.elf 빌드가 정상적으로 완료되면 이전에 지정한 host의 tftp 폴더에 build된 image들이 자동으로 복사된다. 향후 u-boot에서 tftp로 커널 등을 로드할 때 tftp 서버의 pxelinux.cfg 폴더 내 어떤 이미지를 로드할 것인지에 대한 설정을 파일에서 읽어온다. pxelinux.cfg 폴더의 default 파일을 보면 tftp 서버에서 kernel, dtb, RootFS를 로드한다는 것을 알 수 있다. 우리는 NFS에서 RootFS를 로드 할 예정이므로 default 파일의 RootFS 로딩 스크립트 부분을 삭제한다. 크로스 컴파일 환경 설정 향후 application의 개발 시 host에서 크로스 컴파일을 진행하고 싶다면 sdk를 만들어 sysroot를 설정하면 된다. 1 2 3 4 petalinux-build --sdk petalinux-package --sysroot -d ${SDK_설치_폴더} unset LD_LIBRARY_PATH source ${SDK_설치_폴더}/environment-setup-cortexa72-cortexa53-xilinx-linux Petalinux Booting jtag boot jtag로 u-boot까지 로딩한다. 보드의 boot-cfg 스위치를 jtag로 맞춘다. SD카드 등이 필요없지만 속도가 느리다. 1 petalinux-boot --jtag --prebuilt 2 --hw_server-url tcp:127.0.0.1:3121 host에 USB를 연결하고 터미널을 오픈 후 하기 명령을 수행하면 부팅이 시작된다.부트 스크립트 로딩 전 대기 카운터에서 엔터를 누르면 u-boot 커맨드 입력이 가능하다. u-boot에서 커널 로딩 u-boot에서 command를 이용하여 tftp서버에서 커널과 dtb를 로드한다.\ndhcp로 target ip 획득 tftp 서버의 ip 및 target의 ip의 환경변수 설정 이부분은 petalinux-config의 u-boot Configuration \u0026gt; u-boot script configuration \u0026gt; Pre bootenv 에서 설정이 가능할 것이라 생각되는데 해보지 않음 tftp 서버에서 설정파일 로드 (pxelinux.cfg/default) tftp 부팅 (이후 RootFS의 로드는 세팅에 따른다.) 1 2 3 4 5 dhcp setenv serverip ${host_ip} setenv ipaddr ${target_ip} pxe get pxe boot SD카드 + NFS RootFS jtag로 부팅하면 편하긴 하지만 느리고 매번 리셋이 필요할 때마다 부팅명령을 다시 넣어줘야 한다. 이를 SD카드로 부팅시켜 해결할 수 있다. 보드의 boot-cfg 스위치를 SD 카드로 변경한다. SD카드가 준비되어 있지 않다면 SD카드를 파티션 설정을 해야 한다. 첫번째 파티션은 부트로더, 부팅스크립트 등을 위한 파티션이며 최소 500MB이며 FAT 파일 형식 두번째 파티션은 RootFS용으로 EXT4 형식이어야 한다. SD카드에 FSBL, U-boot, bitstream인 BOOT.bin 를 넣어 놓고 u-boot 까지 부팅 시킨 후 이후 커널 및 RootFS를 로딩할 수 있다. SD카드에 커널의 내용이 변경되지 않은 경우 부팅스크립트 boot.scr와 커널/디바이스트리 image.ub를 넣어 놓고 자동으로 NFS에서 RootFS를 로딩하게 하는 방법도 가능하다. 참고 자료 https://xilinx.github.io/Embedded-Design-Tutorials/docs/2022.2/build/html/index.html https://github.com/Xilinx/Vitis-Tutorials https://docs.xilinx.com/r/2021.1-English/ug1144-petalinux-tools-reference-guide/Configure-TFTP-Boot ","date":"2023-02-03T00:00:00Z","permalink":"https://muonkmu.github.io/p/mw_project-zcu104-%EA%B0%9C%EB%B0%9C%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95/","title":"[MW_project] zcu104 개발환경 설정"},{"content":"본 챕터에서는 graph-based deep-learnig accelerator 중 Graphcore IPU에 대해 알아본다. Graphcore IPU는 Microsoft와 Dell의 차세대 데이터 센터 딥러닝 가속기로 선정되었다.\nGraphcore Intelligence Processing Unit(IPU) Graphcore IPU는 fine-grained operation을 수행하기 위하여 graph theory를 적용하며 MIMD paralleism을 제공\nFine-Grained : 하나의 작업을 작은 단위의 프로세스로 나눈 뒤 다수의 호출을 통해, 작업 결과를 생성해내는 방식, 반대말은 Coarse-Grained. Intelligence Processor Unit Architecture IPU는 tiles라 불리는 1216 PE로 구성 PE는 256kb local memory를 가지며 레지스터 파일을 제외한 추가 memory storage를 가지지 않음 tiles 간 IPU Exchange라 불리는 on-chip interconnect로 연결되어 있으며 IPU간 연결을 위한 IPU link를 제공 IPU는 6개의 개별 processing thread를 제공하며 각 thread는 별개의 instruction과 excution flow를 제공 각 tile은 static round-robin schedule에 따라 thread 들을 순환한다. Accumulating Matrix Product (AMP) Unit IPU는 pipelined structure AMP를 채택, AMP는 64bit mix-precison 또는 16bit single-point 연산을 클럭 사이클 마다 수행 가능 mix-precison : 훈련 중에 모델에서 16-bit 및 32-bit 부동 소수점 유형을 모두 사용하여 더 빠르게 실행하고 메모리를 적게 사용하는 것, 모델의 특정 부분을 32-bit 유형으로 유지 Memory Architecture PE 당 256kb local memory를 가지며 IPU는 총 304Mb 메모리를 가짐 각 tile은 21bit address space를 가지며 6개의 execution unit과 이를 공유함 Interconnect Architecture IPU 간 연결은 IPU link를 사용하며 2개 IPU 연결은 3개의 IPU link를 사용 (65Gb/s) Host완s PCIE-4로 연결 Bulk Synchronous Parallel (BSP) Model IPU operation은 BSP 모델을 기반으로 하며, BSP 모델은 아래 3개의 Phase Operation으로 구분된다. Computation Phase : 모든 프로세서가 로컬 메모리로 computation을 수행하며 프로세서간 어떤 통신도 없다. Communication Phase : 각 프로세서는 정보를 교환하며 어떤 computation도 없다. Barrier Synchronization : 모든 프로세서는 computation이나 communication 없이 모든 프로세서가 barrier에 도달할 때까지 대기한다. IPU는 computation이 시작되기 전에 instruction이 코어로 전송되는 BSP 모델을 실행한다. core는 computation을 수행하고 이가 끝난 뒤 다른 코어와 communication을 수행한다. 그후 모든 코어는 동기화를 수행한다. 결론 Blaize GSP와 Graphcore IPU는 분산 프로세서를 통해 거대 parallel operation을 처리할 수 있기에 Cloud-base application에 좋은 솔루션이다. 그러나 이들은 power/area 문제로 임베디드 추론 application에는 적합하지 않다. ","date":"2023-01-28T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-2/2/","title":"[AI HW Design] Chap04 Streaming Graph Theory (2/2)"},{"content":"본 챕터에서는 graph-based deep-learnig accelerator에 대해 공부한다 (Blaize GSP, Graphcore IPU). 이들은 Mutiple Instructions Multiple Data(MIMD) 처럼 동작한다.\nBlaize Graph Streaming Processor(GSP) 무언가 칩에 대한 설명보단 Graph Streaming 기본 이론에 대한 내용이 주를 이룬다.\nStream Graph Model Stream Graph는 네트워크 트래픽, 데이터베이스 등에 널리 쓰이는 모델로 dynamic stream data를 처리하기 위해 data stream model(TCS)을 사용 거대 그래프 스트리밍 Transit(T), 큰 데이터 처리 Compute(C), 일시/롱텀 데이터 저장 Store(S) Turnstile 모델이 TCS모델 중에서 데이터 출발/도착과 같은 data behavior을 가장 잘 표현하며 task scheduling에 사용. Depth First Scheduling Approach Blaize GSP는 뉴럴넷모델을 Direct Acyclic Graph(DAG) format (V,E)로 변환 V는 PE vertex, E는 PE간 weighted connection을 위한 edge scheduling을 위해 Depth First Scheduling (DFS)를 사용하며 dynamic graph excution을 가능하게 하고 sparse/conditional graph를 지원한다. (dfs 설명은 유명하니 생략) GSP는 4가지 Parallelism을 달성했다. 자세한 설명은 책 참조 Task parallelism, Thread parallelism, Data parallelism, Instructon parallelism Graph Streaming Processor Architecture GSP는 다음 그림과 같은 구조로 되어 있음 Streaming Processing은 Sequential Processing에 비해 하기와 같은 장점이 있음 (책에 두가지 방법에 대해 비교 그림있음) Small intermediate buffer for local processing Cached data is easily supported Memory bandwidth is reduced to improve the performance with less power Support both task and data-parallel processing Data is sent to the next node when it is ready GSP는 opeartion을 데이터가 준비되는 즉시 기다리지 않고 수행하도록 스케줄링 함으로써 성능을 향상시키고 메모리 access를 감소시킴 ","date":"2023-01-27T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-1/2/","title":"[AI HW Design] Chap04 Streaming Graph Theory (1/2)"},{"content":"Git을 사용하다보면 막히는 부분이 항상 생긴다. 이런 부분에 대해 간단히 정리 해 놓는다.\nGit remote branch 가져오기 Git을 사용하다보면 원격저장소에 있는 branch를 local로 가져와야 경우가 있다. 이럴 때 git checkout -t {저장소 이름} 을 사용하면 된다.\n필요 시 git remote를 갱신 필요 시 remote 브랜치 확인 remote 브랜치 가져오기 1 2 3 git remote update git branch -r git checkout -t {origin/저장소 이름} 만약 브랜치의 이름을 변경하여 가져오고 싶다면 git checkout -b {생성할 branch 이름} {원격 저장소의 branch 이름} 만약 checkout 시 -t 옵션을 제외하면 ‘detached HEAD’ 상태로 소스를 보고 변경 해볼 수도 있지만 변경사항들은 commit, push 할 수 없으며 다른 branch로 checkout하면 사라진다. ","date":"2023-01-20T00:00:00Z","permalink":"https://muonkmu.github.io/p/git-%EC%82%AC%EC%9A%A9-tip-%EC%A0%95%EB%A6%AC/","title":"git 사용 tip 정리"},{"content":"Target Model을 YOLOv3_tiny로 정한 것은 다른 이유가 있는 것은 아니고 간단하고 레퍼런스가 쉽게 구할 수 있어서이다. 사실 프로젝트가 YOLOv3 tini의 경우 매우 가볍기 때문에 가속기로의 의미는 크게 없다고 생각한다. 그러나 YOLO-X 모델 같은 가속기를 구현하기 위해서는 Sparse Matrix operation 등이 적용 가능한 NPU와 같은 구조를 잡는 것이 필요할 것이라 생각되어 미루어 두기로 한다. 우선 간단한 가속기를 구현하는 것에 의미를 둔다.\nYOLO reference YOLO v3 tiny은 YOLO v3에서 FPN 을 덜어내고 경량화 시킨 구조이다. 라즈베리 파이 CPU에서도 돌릴 수 있다고 한다. ( 실제로 돌려보니 정확도가 좀 떨어지는 것 같다. 바운딩 박스도 이상하게 쳐지고) 기본적인 코드는 darknet git에서 구할 수 있다. 사용법 및 설명은 홈페이지에서 볼 수 있다. (https://pjreddie.com/darknet/yolo/) darknet repo pull make(GPU 사용 예정이라면 Makefile 수정) pre-trained 된 weights 다운 test 1 2 3 4 5 git clone https://github.com/pjreddie/darknet cd darknet make wget https://pjreddie.com/media/files/yolov3-tiny.weights ./darknet detect cfg/yolov3-tiny.cfg yolov3-tiny.weights data/dog.jpg yolov3-tiny.cfg 파일을 보면 Model의 구조를 알 수 있다. 각 layer에 대한 설명은 누군가 Darknet을 pytorch로 변환하면서 분석해 놓은 것이 있으니 이를 참조한다. (https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/) YOLO v3 Structure Model을 도식화하면 다음과 같다. 참고 자료 https://wikidocs.net/181704 https://deep-learning-study.tistory.com/411 ","date":"2023-01-16T00:00:00Z","permalink":"https://muonkmu.github.io/p/mw_project-yolo-v3-tiny-%EB%B6%84%EC%84%9D/","title":"[MW_project] YOLO v3 tiny 분석"},{"content":"본 챕터에서는 몇가지 주요한 Paralle Architecture에 대하여 소개한다. 이 페이지에서는 Microsoft Catapult Fabric Accelerator에 대해서 기술한다.\nMicrosoft Catapult Fabric Accelerator 마이크로소프트는 Brainwave Project를 CPU와 FPGA를 쓰는 형태로 변경 48개의 FPGA가 2개의 Half rack (pod)에 그룹화되고 네트워크로 연결 Brainwave는 train된 DNN 모델을 Catapult Fabric이라 불리는 synthesized softcore에 컴파일 후 narrow precision approach를 적용 모델의 파라메터는 softcore 내 상주 System Configuration Catapult Fabric은 synthesized softcore이며 이는 RTL의 recompilation 없이 low level SW library를 통해 reconfigure 될 수 있다. Catapult Fabric은 Shell과 Role 두 파트로 나눌 수 있음 (상세사항은 책을 참조) Shell : 모든 application에서 재사용 가능한 programmable logic (통신/off-chip 등의 인터페이스를 말하는 듯) Roll : Application logic (그림을 보면 Softcore를 지칭하는 듯) Catapult Fabric Architecture Catapult Fabric은 Matrix-Vector Multiplier(MVM), MultiFunction Uint(MFU), Vector Arbitration Network로 구성 MVM : Matrix-Vector 및 Vector-Vector 연산 수행, PRF/VRF/MRF에 ifmap/fmap 저장 VAN : PRF/DRAM/IO-queue 간 데이터 전송 담당 Matrix-Vector Multiplier FP16을 MS-FP8/MS-FP-9으로 변환하여 연산(mantissa가 2~3bit) input data는 VRF, filter weight는 MFR에 저장 3개의 tile engine과 3개의 accumulator에서 병렬 연산을 지원(상세내용을 책을 참조) MVM의 출력은 MFU로 연결, MFU는 vector-vector operation, activation등을 수행 Hierarchical Decode and Dispatch (HDD) Catapult Fabric은 전통적인 scalar processor(Single Instruction Single Data)를 채택 scheduler는 6개의 Decoder가 4-layer구조로 배치 (상세 내용은 책 참조) Sparse Matrix-Vector Multiplication (SMVM) SMVM을 위해 Condensed Interleaved Sparse Representation (CISR) encoding 사용 Compressed Sparse Raw (CSR) 포맷이 가변 row 길이로 인해 Parallel contorl이 어려움을 극복 첫 번째 0이 아닌 요소가 첫 번째 슬롯에 배치, 해당 열 인덱스는 인덱스 배열에서 동일한 순서로 배치, 행 요소가 모두 사용되면 다음 두 행 요소가 빈 슬롯에 할당, 이를 반복 (상세 내용은 책 참조,사실 이해가 잘 안감) ","date":"2023-01-13T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/","title":"[AI HW Design] Chap03 Parallel Architecture (3/3)"},{"content":"현재까지 DL에 대해 공부한 것과 현업에서 배운 것을 섞어보고자 개인 프로젝트를 진행할 예정이다. 실력이 미천하여 성능, 효율성은 미뤄두고 가장 빠르고 쉽게 구현하는 것을 목표로 한다. 생각보다 오래 걸릴 듯 하다.\nGOAL 카메라의 입력을 받아 Real-time으로 Object detection을 수행하는 FPGA 기반 ECU 개발 현재 개발되어 있는 기반 설계 및 IP가 전무하기에 PPA 보다는 빠른 구현에 목표를 둔다.\nSPEC (TBD) target B/D : ZCU104 input : Full HD camera (interface MIPI or USB 중 쉬운거) output : real time image showing a bounding box (interface HDMI) Algorithm : YOLO v3 Tiny (이를 선택한 특별한 이유는 없고 reference 구하기 쉽고 간단해서 이다) miniaml 20 fps Design Flow (TBD) architecture design and spec fix camera interface design output interface design YOLO Core design (HLS + verilog) ","date":"2023-01-13T00:00:00Z","permalink":"https://muonkmu.github.io/p/mw_project-personal-project-propsal/","title":"[MW_project] Personal project propsal"},{"content":"GPU를 이용하여 Deep learning 모델을 구성하고자 하였으나 다른 기술 블로그에서 기술한대로 수행하여도 동작이 되지 않는다. 해당 공식 문서들을 참고하여 설치하는 법을 기술한다.\n목적 GPU 및 pytorch 기반 개발을 위한 PC 개발 환경 구성 GPU 활용을 위해 nvidia driver, cuda, cudnn 설치와 conda 환경에서 pytorch를 설치하는 방법을 다룬다. 환경 OS : ubuntu 20.04 LTS GPU : nvidia 1080ti python 3.8.10 and GCC 9.4.0 설치 절차 우선 모듈의 dependency를 확인해야 한다. 현재 pytorch에서 우분투 20.04를 지원하는 플랫폼은 CUDA 11.7이므로 CUDA 11.7 버전과 이에 적합한 nvidia driver를 설치 해야한다. 필요한 nvidia driver는 사실 cuda를 설치 해보면 dependency 체크를 하면서 필요한 버전을 알려준다.(더 좋은 방법이 있을지도)\nNvidia Driver 설치 nvidia driver 설치 여부 및 현재 설치된 버전을 확인한다. 1 nvidia-smi 필요 시 기존의 nvidia driver를 삭제한다. 1 2 3 sudo apt-get remove --purge \u0026#39;nvidia-.*\u0026#39; sudo apt-get autoremove sudo apt-get autoclean 설치가능한 드라이버를 확인한다. 1 ubuntu-drivers devices 만약 필요한 드라이버 목록에서 없다면 저장소를 추가한다. 1 2 sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update 원하는 드라이버를 설치 후 재부팅 한다. 현재 저자의 환경에서는 5.25가 필요하므로 이 버전을 예로 설명한다. 1 2 sudo apt install nvidia-driver-525 sudo reboot cuda 설치 cuda 홈페이지에서 현재 내 설정에 맞는 runfile을 다운 가능하나 저자는 이상하게 설치가 안되었다. 하기 페이지를 활용하여 네트워크 Repo에서 설치하자 (https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#prepare-ubuntu)\nRemove outdated signing key Install the new cuda-keyring package Install CUDA SDK reboot 1 2 3 4 5 6 sudo apt-key del 7fa2af80 wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb sudo dpkg -i cuda-keyring_1.0-1_all.deb sudo apt update sudo apt-get install cuda-11-7 sudo reboot 환경변수를 등록한다. 1 2 export PATH=/usr/local/cuda-11.7/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-11.7/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} cudnn 설치 cudnn 역시 package 파일로 설치가 잘 안되서 tar 파일로 설치하였다. (https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html)\n필요한 cudnn 라이브러리 tar를 cudnn 홈페이지에서 다운받는다. 파일의 압축을 풀고 cuda 라이브러리에 파일을 복사한다. 1 2 3 4 tar -xvf cudnn-linux-x86_64-8.x.x.x_cudaX.Y-archive.tar.xz sudo cp cudnn-*-archive/include/cudnn*.h /usr/local/cuda/include sudo cp -P cudnn-*-archive/lib/libcudnn* /usr/local/cuda/lib64 sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn* conda 설치 모듈들의 dependency 및 버전 관리를 위해 가상환경인 conda를 사용하기로 하였다.\n자신의 파이썬 환경에 맡는 miniconda 설치 파일을 받고 이를 실행한다. 1 Miniconda3-latest-Linux-x86_64.sh 자신이 사용할 가상환경을 만들고 이를 실행한다. 1 2 conda create -n {my_env} conda activate {my_env} 터미널 실행 시 자동으로 conda 환경이 실행되는 것을 막을려면 다음을 수행한다. 1 conda config --set auto_activate_base false pytorch 설치 conda 환경에서 pytorch 홈페이지를 참고하여 pytorch를 설치한다. 자신이 원하는 구성을 고르면 Run command를 알려준다.\n1 conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia 설치 확인 conda 환경에서 python을 터미널을 실행한 후 pytorch cuda 설정 사용 가능 여부가 True로 출력되면 정상\n1 2 import torch print(torch.cuda.is_available()) ","date":"2023-01-13T00:00:00Z","permalink":"https://muonkmu.github.io/p/dl-%EA%B0%9C%EB%B0%9C%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95-cuda-cudnn-conda-pytorch/","title":"DL 개발환경 설정 (cuda, cudnn, conda, pytorch)"},{"content":"본 강좌에서는 Object Detection의 개념과 이를 위한 YOLO알고리즘의 기초에 대하여 정리한다.\nhttps://www.coursera.org/learn/convolutional-neural-networks 하기 블로그에 더 잘 정리되어 있다..(누군지 존경스럽다.)\nhttps://junstar92.tistory.com/140 Object Localization Classification with localization : object class 뿐 아니라, 알고리즘이 object를 대상으로 bounding box를 표시하는 것을 의미\n이를 위해 이미지를 CNN에 입력 출력으로 class 뿐만 아니라 이미지에 object가 존재할 확률($p_c$), Bounding box의 위치 및 크기를 같이 출력(Bx, By, Bh, Bw) Loss function은 MSE(mean squared error)를 사용한다면 Y 각 요소의 에러의 합과 같다. Pc가 0일 경우 Pc의 에러만 사용한다. $$ L(\\hat{y},y) = \\begin{cases} (\\hat{y}_1-y_1)^2+(\\hat{y}_2-y_2)^2+\\cdots+(\\hat{y}_8-y_8)^2 \u0026amp; \\text{if } y_1=1 \\\\ (\\hat{y}_1-y_1)^2 \u0026amp; \\text{if } y_1=0 \\end{cases} $$\nMSE를 예시로 설명했지만, $c_1$, $c_2$, $c_3$에는 log-likelihood loss와 softmax를 사용하고, bounding box 정보에는 MSE를, 그리고 $p_c$ 에는 Logistic Regression Loss를 사용할 수도 있다.\nLandmark Detection Bounding box가 아닌 일반적인 Face recognition이나 pose detection의 같은 일반적인 경우 이미지의 주요 포인트(landmark)를 X와 Y의 좌표로 나타낼 수 있다. Object Detection Sliding Windows Detection 알고리즘을 사용해서 Object Detection을 위해 ConvNet을 사용하는 방법 알아본다. (CS231n 강의에서는 Sliding window는 하지말라던데 아마 이해를 위해 넣어놓은 것 같다.) 방법은 하기와 같다. object의 클래스를 구분할 수 있는 모델 생성 전체 이미지 중 특정 size의 window를 골라 탐색 window 살짝 옮겨서 반복 더 큰 박스를 이용하여 반복 그러나 이 방법은 computing cost가 높다. 다음 절에서 이를 줄일 수 있는 방법을 알아본다 Convolutional Implementation of Sliding Windows Sliding window 방법은 매우 느린데 이를 해결하기 위해 FC(Full connected) layer를 Convolutional Layer로 튜닝하는 것을 알아보자. 절차는 하기와 같다 FC layer를 이와 같은 output을 낼 수 있는 Filter로 변환 sliding window 시 각각 수행이 아닌 convolution처럼 한번에 연산. 이렇게 하면 중복되는 연산은 공유가 가능하다. 그러나 이 방법은 bounding box의 위치가 정확하지 않다는 단점이 있는데 이를 아래 방법으르 해결한다. Bounding Box Predictions Sliding window 방법은 object가 그 위치에 있지 않거나 일부분만 걸칠 수 있는데 이를 YOLO 알고리즘으로 극복 가능하다. 전체 이미지에 3x3 grid 를 설정(보통은 19x19 사용) 위에서 배운 object localization을 각각의 grid에 적용, 즉 이해한바로는 test set에서 각각의 그리드에 localization방법으로 labeling하고 학습 각 grid에 object가 존재한다면 object의 중간점을 위해서 object를 할당한다. 이때 object의 크기는 1이 넘어갈 수 있다.(gird를 넘어가거나 클 수 있으므로) Bounding box를 설정하는 방법은 여러가지가 있지만(ex. PCA 이용), YOLO논문을 살펴보면 잘 동작할 수 있도록 파라미터화 된 것들이 있다. Intersection Over Union Intersection over union(IoU)은 Object Detection이 잘 동작하는지 판단하기 위한 함수 labeling 된 bounding box와 예측한 bounding box의 전체 넓이와 겹치는 부분 넓이의 비율을 계산 보통 0.5 이상이면 예측한 bounding box의 결과가 옳다고 판단 Non-max Suppression 현재까지 알아본 Object detection의 문제점은 한 Object를 여러번 탐지할 수 있다는 것이다. 즉 한 object가 한 그리드 이상에의 면적을 차지할 경우 이 object의 중심점이 여러 Cell에서 탐지 될 수 있다. 이 경우에 Non-max suppression을 사용하면 알고리즘이 하나의 object를 하나의 cell에서 한번만 탐지할 수 있다. 만약 분류 class 가 1개여서 $p_c$가 class의 확률이라 가정한다. (실제로는 클래스는 여러개) $p_c$를 조사하여 가장 큰 것만 취함 나머지 box와 $p_c$값이 가장 큰 박스와 IoU 조사 IoU가 높은 박스는 제거 만약 class가 여러개라면 class 당 non-max suppression을 수행한다. Anchor Boxes 현재까지 소개한 알고리즘의 문제점 중 하나는 각 grid cell이 오직 하나의 object만 감지할 수 있다는 것이며 이를 anchor box라는 아이디어를 가지고 해결할 수 있다. anchor 박스의 모양을 미리 정의 각각의 anchor box는 각 output을 가지게 한다. anchor box의 선택은 manual로 선택을 할 수도 있고, K-mean알고리즘을 통해서 얻고자하는 유형의 object모양 끼리 그룹화 할 수도 있다. YOLO Algorithm 위의 내용을 모두 종합하여 YOLO object detection algorithm을 정리해보자 이미지의 anchor box와 grid 수를 정하고 이와 같이 labeling된 데이터 셋으로 모델을 학습 상기 모델로 추론을 수행하게 되면 각 grid cell은 anchor box 수만큼의 bounding box를 가질 수 있다. 여기서 낮은 확률을 가지는 예측결과는 제거하고 각 class에 non-max suppression을 적용하여 최종 예측 결과를 얻는다. YOLO 알고리즘은 가장 효과적인 Object Detection 알고리즘 중 하나 ","date":"2023-01-10T00:00:00Z","permalink":"https://muonkmu.github.io/p/cnn-week-03-object-detection/","title":"[CNN] week 03 Object detection"},{"content":"본 챕터에서는 몇가지 주요한 Paralle Architecture에 대하여 소개한다. 이 페이지에서는 NVDLA와 Google TPU에 대해서 기술한다.\nNVIDIA Deep Learning Accelerator (NVDLA) NVDLA는 FPGA로 구성 가능한 추론을 위한 오픈소스 아키텍쳐 (http://nvdla.org) Primitive functional blocks으로 CNN을 지원 (convolution, activation, pooling, normalization) 각 블럭은 next layer의 active와 configuration을 위한 double buffer를 가짐 next layer의 operation은 active operation이 완료되어야 시작 independent mode와 pipeline을 사용하는 fused mode가 있음 Figure. NVDLA core architecture Convolution Operation Direct convolution, Image input convolution, winograd convolution, Batch convolution 지원 (상세내역은 책 참조) Single Data Point Operation(SDP) SDP는 linear functions와 Look-up Table nonlinear functions을 통해 activation과 normalizatin을 지원 (상세내역은 책 참조) Planar Data Operation(PDP) PDP는 maximum/minimum/average pooling을 지원 Multiplane Operation Cross Channel Data Processor(CPD)은 Local Response Normalization(LRN)을 수행 Data Memory and Reshape Operations bridge DMA는 외부 메모리와 메모리 인터페이스간 데이터 전송을 담당 data reshape engine은 data trasnformations, splitting, slicing, merging, contraction, reshape transpose 를 담당 System Configuration NVDLA는 small/large system model로 구현할 수 있음 small system model : IoT 기기와 같이 작은 모델을 위한 모델, 복잡도와 storage를 낮추고 single task를 수행 large system model : mutiple task를 위한 coprocessor와 메모리 인터페이스 추가 External Interface NVDLA는 외부와 통신을 위한 Configuration Space Bus(CSB), Data backbone(DBB), SRAM interface, Interrupt interface를 가짐 (상세내용은 책 참조) Software Design NVDLA SW는 Trained model을 parser/compiler/optimizer를 통해 loadable로 변환 User Mode Driver(UMD)에 의해 Loadalbe이 로딩 되고 Job이 Kernel Mode Driver(KMD)로 제출됨, KMD는 스케줄링 수행 Google Tensor Processing Unit(TPU) 구글은 speech recognition 수요 해결을 위해 TPU v1(stand alone)과 v2/v3(cloud)를 개발 TPU v1은 하기 스펙으로 MLP 0/1, CNN 0/1, RNN 0/1 6가지 neural network application을 수행 가능 256 × 256 eight bits MAC unit 4 Mb on-chip Accumulator Memory (AM) 24 Mb Unified Buffer (UB) – activation memory 8 Gb off-chip weight DRAM memory Two 2133 MHz DDR3 channels TPU는 6가지 neural network application을 수행할 수 있음 Multi-layer perceptron(MLP) 0/1, Convolution Neural Network(CNN) 0/1, Recurrent Neural Network(RNN) 0/1 System Architecture TPU v1은 매트릭스 연산을 Matrix Multiply Unit(MMU)에서 수행 MMU는 256 × 256 eight bits MAC unit이며 16bit 연산을 수행할 경우 성능은 8bit 대비 절반(Sparse matrix 연산을 지원하지 않음) Weight FIFO는 matrix weight를 8Gb DRAM에서 읽어오며 activation, pooling, normalization 후 중간 연산 결과를 24Mb Unified Buffer에 저장 Multipy-Accumulate(MAC) Systolic Array Systolic array는 TPU의 핵심이자 High throughput / low latency를 가진 SIMD pipeline. 책에 별 설명이 없으므로 이에 대한 내용은 더 찾아보는 것이 좋다(다른 많은 곳에 잘 나와 있음) 단점은 전력 소모가 많다는 것(데이터 센터 등에 적합) New Brain Floating-point Format TPU v1은 input data를 FP32에서 INT8로 quantization하여 연산하며 이에 따라 안정성/정확도 문제 발생 이를 위해 IEEE FP16 대신 Brain Floating Point format (BFP16) 사용 BFP16 : Mantissa를 7bit으로 줄이고 exponent를 FP32와 같은 8bit으로 늘림 Sign 1bit, Exponent 8bit, Mantissa 7bit multiplier area와 power를 줄이고 Scaling loss 없이 FP32와 동일한 정확도를 얻음 Performance Comparision roof-line model의 관점에서 볼 때 TPU가 가장 높은 peak performance를 달성했다. roof-line model은 Y축이 성능을 나타내며(평평한 부분이 최고 성능), X축이 byte 당 operation intensity 부가 설명을 하자면 한번에 얼마나 많은 연산을 수행하게 할 때 성능이 어디까지 올라가는지 지표, loof-line은 메모리 Bandwith 때문에 걸림 Cloud TPU configuration TPU v2/v3는 v1에서 DDR을 HBM으로 바꾸고 v1을 Pod로 묶음 Cloud Software Architecture 구글은 cloud computation을 위해 새로운 SW 아키텍쳐를 개발 Model을 TensorFlow를 통해 computational graph로 해석 상세 내용은 책을 참조 ","date":"2022-12-30T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/","title":"[AI HW Design] Chap03 Parallel Architecture (2/3)"},{"content":"IDEC에서 수강한 KiCad 강좌 요약 이다. 디자인 플로우 중 Footprint와 PCB에 대해 정리한다.\nFootprint Footprint design flow footprint 편집기를 연다 실부품 측정 또는 데이터 시트를 참조하여 부품 치수 확인 필요 시 풋프린트 라이브러리 생성 (파일-\u0026gt;새라이브러리) 라이브러리 선택 후 새 풋프린트 생성 하거나 유사한 부품 불러 온 후 다른 이름으로 저장 십자선 커서 이용하여 원점에서 스페이스바를 눌러 원점 위치 선정 실크 레이어에 부품 외형선 그리기 패드와 홀 위치 시킨 후 사이즈 속성 편집 패드 위치에 맞게 번호 편집, 핀번호는 심볼과 일치하도록 할 것 저장 SMD Component Footprint SMD 부품의 경우 패드의 속성을 SMD로 변경 뒷면 실장 component 뒷면에 실장할 경우 레이어를 관련 레이어를 B.* 레이어로 변경해야 한다. F.Cu, F.Silkscreen, F.Courtyard, F.Fab 내용을 B.* 레이어로 이동 PCB design 프로젝트 매니저에서 PCB 편집기 열기 회로도 PCB 전환(F8)을 이용하여 회로도에서 컴포넌트를 로딩 Edge.Cuts layer에서 PCB 외형선을 그리기 외형선 내부에 컴포넌트 배치 및 컴포넌트 레퍼런스/value 위치 조정 필요한 텍스트를 Silkscreen에 부가 트랙 설정 및 배선 (일반적으로 신호선 12mil, 전원선 30mil) 동박면 씌우기 (GND와 연결) DRC 검사 Plot을 통해 거버/드릴링/포지션 파일 생성 및 검사 ","date":"2022-12-28T00:00:00Z","permalink":"https://muonkmu.github.io/p/kicad-footprint-and-pcb/","title":"[KiCad] Footprint and PCB"},{"content":"IDEC에서 수강한 KiCad 강좌 요약 이다. 디자인 플로우 중 Symbol and Schemetic에 대해 정리한다.\nKiCad 개요 회로도 및 PCB가 함께 설계되는 오픈소스 통합 설계도구 거버 /드릴/ 부품위치 파일 생성 및 PCB 계산기, 거버 뷰어, 3D 뷰어, SPICE 시뮬레이터 포함 프로젝트 기반 관리로 한번에 하나의 프로젝트만 열 수 있음 파일구성 *.kicad_pro : 회로도와 pcb간 공유되는 설정이 포함된 프로젝트 파일 *.kicad_sch : 모든 정보와 구성 요소 자체를 포함시키는 회로도 파일 *.kicad_sym : 회로도 심볼 라이브러리 파일로 심볼 요소 설명을 포함 *.kicad_pcb : pcb 보드 파일 *.pretty : 풋프린트 라이브러리 폴더 *.kicad_dru : pcb 사용자 설계 규칙 파일 *.net. : 회로도에 의해 생성되는 넷리스트 파일 KiCad PCB design workflow 프로젝트 생성 회로도 그리기 회로도 심볼을 심볼 라이브러리에서 찾아 지정된 선 연결, 심볼이 없을 경우 새로 심볼을 새로 만듬 각 구성 요소에 대해 풋프린트를 배정하고 풋프린트가 없는 경우 풋프린트를 생성하여 반영 회로도 완성 시 전기 규칙 점검(ERC 수행) pcb 편집기로 전송하여 레이아웃 시작(넷리스트 생성 및 부품 간 선 연결 일치 시킴) 기판 크기(Edge.Cuts) 그리기 및 풋프린트 위치를 선정 배치 배치 후 요소 사이 트랙 연결 트랙은 규정에 따라 전류 용량, 임피던스, 고전압 누화 등을 고려 선폭/선간 설정 (pcb계산기 참조) 트랙은 신호선의 경우 보통 12mil, 6mil 이하로 하면 pcb 제작 단가 상승 레이아웃이 완료되고 설계 규칙 검사(DRC) 및 수정 거버 파일 제작 출력 및 PCB 제작 의회 프로젝트 관리 창 Tip 1 : 프로젝트 생성 시 템플릿을 지정하여 생성 가능 (큰 회사에서 기초 설정 등을 지정한 형식) Tip 2 : 환경 설정에서 텍스트 편집기를 등록하면 텍스트 편집기 사용이 가능하다. Symbol 생성 필요 시 심볼 라이브러리 생성 (파일-\u0026gt;새라이브러리) 라이브러리 선택 후 새 심볼 생성 생성된 심볼에서 레퍼런스, 심볼값을 원하는 위치로 이동 외형선 그리기, 핀 부가, 핀 더블 클릭 하여 속성(이름, 번호, 유형 등) 설정 필요 시 원점 설정 (단축키 space)(심볼 로딩 위치 및 로테이션 시 회전 점) 저장 Tip) 편집 시 원하는 위치에 지정할 수 없을 때 그리드 속성을 편집하여 그리드 간격을 조절하자 회로도 그리기 프로젝트 매니저에서 {프로젝트 이름}.kicad_sch 파일을 연다 심볼을 배치한다 (전원의 경우 pspice 라이브러리는 시뮬레이션 용이니 Power라이브러리 사용) 선을 연결하고 텍스트 위치 조정한다. 레퍼런스 (부품번호, ex. R100) 지정자 채우기 로 레퍼런스 설정 PCB 풋 프린트 배정 ERC 수행/수정 및 BOM 출력 ","date":"2022-12-28T00:00:00Z","permalink":"https://muonkmu.github.io/p/kicad-symbol-and-schemetic/","title":"[KiCad] Symbol and Schemetic"},{"content":"본 chapter에서는 Reinforcement Learning에 대해서 알아보자\nVideo : https://www.youtube.com/watch?v=lvoHnicueoE\u0026list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv\u0026index=15 Slide : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture14.pdf (TODO: 귀차니즘의 압박으로 정리를 안했다.. 근데 강의가 무척 어려워서 잘 이해가 안된다.)\n","date":"2022-12-23T00:00:00Z","permalink":"https://muonkmu.github.io/p/cs231n-chap-14-reinforcement-learning/","title":"[CS231n] Chap 14 Reinforcement Learning"},{"content":"Petalinux Booting and Packaging petalinux Packaging petalinux-package 명령을 이용하여 하기 내역을 수행 할 수 있다. .BIN 또는 .MCS파일을 생성 (\u0026ndash;boot 옵션) BSP (.BSP 파일) 또는 Package image 생성 (\u0026ndash;bsp, \u0026ndash;image 옵션) prebuilt 디렉토리 생성 (\u0026ndash;prebuilt 옵션) Vitis 를 위한 sysroot 설치 (\u0026ndash;sysroot 옵션) petalinux booting QEMU, SD card, Jtag, TFTP, QSPI에 의한 Booting을 지원한다. (jtag Boot는 속도가 느려 잘 사용안함)\nPetalinux Debugging 상세 내용은 교제의 Petalinux Application Debugging 및 LAB5 참조\nPetalinux는 Application Debugging 시 System Debugger(Vitis) 와 GNU Debugger를 지원한다. Vitis는 Target Communication Framework(TCF)와 Xilinx System DBugger(XSDB)를 이용한 Debugging 환경을 제공 일반적인 Linux GNU Debugger 지원 System Debugger 방법 https://xilinx.github.io/Embedded-Design-Tutorials/docs/2022.2/build/html/docs/Introduction/ZynqMPSoC-EDT/ZynqMPSoC-EDT.html 참조 petalinux-config -c rootfs를 이용하여 Root File system에 하기 내역을 포함 시킨다. tcf-agent (default enable) openssh-sftp-server dropbear (default disable) 이미지를 빌드 하고 디버깅하고자 하는 Application을 실행 시킨다. QEMU의 경우 GEM0만 연결되어 있으므로 필요 시 GEM3 등의 Device Tree를 추가하여 빌드한다. vitis를 실행 시키고 *.XSA 파일 등을 이용하여 platform project를 구성한다. platform project에 빈 linux Applicaiton domain을 추가한다. 4)항의 항목내 Debug configuration을 이용하여 Single Application Debug를 추가한다. target 보드의 debug IP/port를 설정하고 파일 패스를 설정한다. GNU Debuger GNU 디버거를 사용하기 위해서는 Root file System에 gdbserver를 포함하여야 한다. Custom HW and Driver Development Xilinx는 Custop IP에 대한 디바이스 제어를 위해 하기의 방법을 제안한다. Linux Device Driver 제작 mmap의 사용 (사용이 쉽다. 인터럽트 핸들링이 안됨) User space I/O (UIO 사용) (간단한 IRQ핸들링이 된다, Latency가 가변적이고 DMA가 지원되지 않는다) Petalinux는 빌드 시 Device Tree Generator가 DTSI/DTS파일을 생성하고 DTB를 만든다 *.XSA 파일을 분석하여 기본적인 DTSI/DTS 파일을 만든다 {project-root}/components/plnx_workspace/device-tree/device-tree에 생성되는 DTSI파일은 다음과 같다 pl.dtsi : memory-mapped PL IP node pcw.dtsi : Dynamic properties of the PS peripheral system-top.dts : boot argument 와 console, memory information zynqmp.dtsi : PS peri and CPU information zynqmp-clk-ccf.dtsi : IP peri를 위한 clock information Custop IP 추가 등 Device tree를 업데이트 하기 위해 하기 DTSI를 업데이트 한다. {project-root}/project-spec/meta-user/recipes-bsp/device-tree/files/system-user.dtsi Custom HW and Petalinux 개발 절차 Custop IP를 개발(RTL 등) 후 Vivado IP Packer를 통하여 IP-XACT Standard Format으로 패키징 한다. Vivado를 이용하여 1)항의 IP와 기타 사용자 IP를 조합하여 *.XSA 파일을 생성한다. petalinux-creat -t project -n {project 이름}를 이용하여 project를 생성하고 *.XSA 파일을 import한다. petalinux-creat -t module -n {driver 이름}을 이용하여 모듈을 생성한다. {project-root}/project-spec/meta-user/recipes-bsp/device-tree/files/system-user.dtsi에 Custom IP에 관련된 Device tree를 업데이트한다. 작성 시 pl.dtsi를 확인하여 module name 및 address 등을 확인한다. 모듈 내부 드라이버 파일을 작성하고 Yocto 레시피를 수정한다. 커널에 로딩할 지 모듈로 rootfs에 등록할지 결정한 후 빌드한다. ","date":"2022-12-22T00:00:00Z","permalink":"https://muonkmu.github.io/p/petalinux-petalinux-advance/","title":"[Petalinux] Petalinux Advance"},{"content":"Petalinux Basic petalinux 정의 Petalinux는 xillinx FPGA를 위한 임베디드 리눅스 개발 툴로 YOCTO 프로젝트 Wrapper이다. Hardware description file(*.XSA) 또는 BSP 파일을 입력으로 리눅스 이미지 생성 Petalinux 프로젝트의 레이아웃은 프로젝트 생성 시, XSA import 시, build 시 추가/달라짐 (교재 p66을 참조 및 https://docs.xilinx.com/r/2021.1-English/ug1144-petalinux-tools-reference-guide/Image-Selector?tocId=nfcK0XF5PXQyI2ebTdA8fA) 기본 명령어 및 Design Flow 상세 내용은 교제의 Petalinux Tool : Design Flow 및 LAB2 참조\n프로젝트 생성\npetalinux-create -t {type} -n {name} \u0026ndash;template {기초 템플릿} 1 petalinux-create -t project -n test_prj --template zynqMP 프로젝트 설정 : Hardware Description 및 boot, rootfs, kernel\npetalinux-config 또는 petalinux-config -c {rootfs/kernel/device-tree/u-boot} 1 2 cd test_prj petalinux-config --get-hw-description={xsa file} --silentconfig 프로젝트 빌드\npetalinux-build 또는 petalinux-build -c {rootfs/kernel/device-tree/u-boot} 생성되는 파일은 하기와 같다 boot.scr: A u-boot boot script image.ub: U-boot wrapped Linux kernel image rootfs.tar.gz: Compressed root file system tar ball 그외 Pakage를 위한 파일 1 petalinux-build 프로젝트 패키징\n.BIN 또는 .MCS 생성 ( = fsbl + ssbl + pmu + bitstream) .BIN 은 다음과 내용을 포함한다. Platform Loader and Manager (PLM) PS Management (PSM) firmware Platform Device Image (PDI) ARM trusted firmware u-boot Device tree blob 1 petalinux-package --boot --fsbl zynqmp_fsbl.elf --u-boot u-boot.elf --pmufw pmufw.elf --fpga system.bit 부트\nSD카드에 이미지 복사(BOOT.BIN, Image, rootfs.cpio.gz.u-boot, boot.scr) 후 보드 부팅 qemu로 에뮬레이션 가능 1 petalinux-boot --qemu --kernel TFTP를 위한 Jtag 부트 1 petalinux-boot --jtag --prebuilt 2 --hw_server-url tcp:127.0.0.1:3121 Application development 상세 내용은 교제의 p133 Petalinux Application Development 및 LAB3 참조\nPetalinux의 project가 생성된 상태에서 petalinux-create를 사용하여 app을 생성\nproject-spec/meta-user/recipes-apps/{app_name}에서 생성된 파일(bb 및 source) 확인 가능 1 petalinux-create -t apps --name helloworld --template c source 및 makefile을 생성 또는 복사한다.\nproject-spec/meta-user/recipes-apps/{app_name}/file에서 수정한다. Yocto Recipe file를 수정한다.\nproject-spec/meta-user/recipes-apps/{app_name}의 {app_name}.bb파일에 관련파일을 등록한다. root filesystem에 등록한다.\npetalinux-config -c rootfs 수행 후 apps 메뉴에서 등록 build 후 /usr/bin에서 app을 확인 가능하다.\n프로젝트 설정 상세 내용은 교제의 p150 Customizing the project 참조 petalinux-config를 이용하여 하기 설정이 가능하다\nfirmware version 정보 root filesystem 종류 : INITRD, INITRAMFS JFFS2, UBI/UBIFS, NFS, EXT4(SD/eMMC\u0026hellip;) U-boot 이미지 저장 위치 : bootenv 조절을 통해 Jtag/DDR, QSPI, NAND의 image offset을 조정할 수 있다. Primary Flash(QSPI?)의 파티션 조절 가능 File system package를 조절하여 Kernel image size 및 Root file system 이미지 사이즈를 줄일 수 있다. TFTP 부팅을 위한 pre-built 이미지 위치를 설정할 수 있다 NFS 또는 SD card를 통한 Root file system 로딩을 설정 할 수 있다. Root file system customize 상세 내용은 교제의 p212 Customizing the Root File System 참조\ncustom applications, libraries, module을 추가하거나 생성 가능 pre-compiled applications, libraries, module을 추가하거나 생성 가능 YOCTO layer, recipes 또는 package 추가 가능 ","date":"2022-12-19T00:00:00Z","permalink":"https://muonkmu.github.io/p/petalinux-petalinux-basic/","title":"[Petalinux] Petalinux Basic"},{"content":"목표 개인 기술 정리를 위한 블로그의 생성 markdown 사용이 편리한 github.io를 이용하기로 결정 빌드가 빠른 HUGO framework을 사용 (github에서는 Jekyll framework가 기본이나 컨텐츠가 쌓이면 빌드가 느려지는 단점이 있음) Hugo theme는 STACK을 사용 개발 환경 Oracle Cloud Arm server Ubuntu 20.40 code-server 사전 준비 GO 설치 Hugo는 GO로 작성되 있으므로 GO를 설치한다.\nref : https://go.dev/doc/install 필요 시 GO의 설치 경로를 PATH에 등록한다. Hugo 설치 리눅스의 경우 패키지 관리자를 이용하여 설치가 가능하나 이 경우 old 버전이 설치된다. STACK 테마의 경우 최신버전과 hugo extension이 필요하므로 Go를 이용하여 설치한다. https://gohugo.io/installation/linux/ 1 go install -tags extended github.com/gohugoio/hugo@latest 필요 시 Hugo의 설치 경로를 PATH에 등록한다. git repo 생성 hosting을 위한 repo를 생성한다. repo의 이름은 {git ID}.github.io 형식 ex) muonkmu.github.io 호스팅 목적이므로 repo는 public hugo 빌드 전 소스를 보관할 repo를 생성한다. 이름은 상관 없음 ex) blog 소스 보관용이므로 public/private은 개인 취향 블로그 작성 및 배포 hugo 프로젝트 생성 및 테마 설정 프로젝트를 생성 후 폴더 이동, 하기 예제의 이름은 hugoBlog로 가정 1 2 hugo new site hugoBlog cd hugoBlog git 초기화 및 테마 설정 하기 예제에서는 Stack 테마 사용 clone으로 테마 소스를 themes폴더에 넣을 수도 있으나 submodule을 추천 1 2 git init git submodule add https://github.com/CaiJimmy/hugo-theme-stack.git themes/hugo-theme-stack config파일 설정 config.toml을 수정, 하기 예제에서는 stack 테마의 예제 파일을 복사/수정 한다. config.yaml의 baseurl, theme, title 등을 수정한다. 1 2 3 rm config.toml cp themes/hugo-theme-stack/exampleSite/config.yaml ./ cp themes/hugo-theme-stack/exampleSite/content ./ 1 2 3 4 5 baseurl: https://muonkmu.github.io/ languageCode: en-us theme: hugo-theme-stack paginate: 7 title: MW Devlog 컨텐츠 작성 및 테스트 categories, post, page 등을 작성한다. 하기 예제에서는 stack 테마의 예제 파일을 복사/수정 한다. content/post 내 예제 파일을 참조하여 post를 작성한다(예제포스트는 지워도 된다.) 1 2 rm -r content cp themes/hugo-theme-stack/exampleSite/content ./ 테스트 서버를 구동하여 동작을 확인한다. 하기 예제에는 orcle 서버에서 개발하는 것을 가정, 내부 바인딩과 포트를 별도로 할당였다(오라클 서버에서 방화벽에 우선적으로 포트을 열어둬야 함) 웹 브라우저로 테스트 서버에 접속해 동작을 확인한다. 1 hugo server -D --bind=0.0.0.0 -p 8070 빌드 및 배포 github repo를 연결한다. 소스 repo에 프로젝트 폴더를 연결 host repo에 public 폴더를 연결 1 2 3 git remote add origin https://github.com/muonkmu/blog.git rm -r public git submodule add -b master https://github.com/muonkmu/muonkmu.github.io.git public 소스를 빌드한다. 하기 예제에서는 stack 테마의 사용 경우이다. 1 hugo -t hugo-theme-stack 빌드 및 소스 파일을 push 한다. 1 2 3 4 5 6 7 8 9 10 cd public git add . git commit -m \u0026#34;first commit\u0026#34; git branch -M main git push origin main cd .. git add . git commit -m \u0026#34;first commit\u0026#34; git branch -M main git push origin main (option)배포에 시 사용할 쉘 스크립트를 작성한다. ex)deploy.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #!/bin/bash hugo -t hugo-theme-stack cd public git add . msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; git push origin main cd .. git add . msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; git push origin main Debug HUGO 받침 분리 표기 문제 사용하던 중 \u0026lsquo;가\u0026rsquo; 받침이 분리되어 표기되는 문제가 발견되었다. ex) \u0026lsquo;각\u0026rsquo; 이 \u0026lsquo;가ㄱ\u0026rsquo; 로 표기 구글링을 해보니 Droid Sans Fallback 폰트의 문제라고 생각되어 관련 폰트를 삭제하여 문제를 해결 ./themes/hugo-theme-stack/assets/scss/variables.scss 의 --sys-font-family, --zh-font-family 변수 내 Droid Sans 관련 폰트를 모두 삭제한다. ","date":"2022-12-13T00:00:00Z","permalink":"https://muonkmu.github.io/p/github-blog-%EB%A7%8C%EB%93%A4%EA%B8%B0/","title":"Github blog 만들기"},{"content":"본 챕터에서는 몇가지 주요한 Paralle Architecture에 대하여 소개한다. 이 페이지에서는 CPU와 GPU에 대해서 우선 기술한다.\nIntel Central Processing Unit (CPU) https://www.intel.com/content/www/us/en/developer/articles/technical/xeon-processor-scalable-family-technical-overview.html CPU는 병렬 프로세싱을 위해 Single Instruction Single Data (SISD) architecture에서 Single Instruction Multiple Data (SIMD)로 진화함. 그러나 이는 딥러닝과 같은 거대 병렬 처리에 적합하지 못하여 2017년 딥러닝 어플리케이션을 위한 Xeon processor scalable family (purley platform) 발표 Purley platform은 하기 특징을 가짐 Skylake mesh architecture 이전 Grantley platform에서는 Last-Level Chache(LLC)등이 Intel Quick Path Interconnect(QPI) ring achitecture로 연결 상기 구조는 코어 증가 시 코어 마다 사용가능한 bandwidth가 줄어들어서 메모리 latency가 증가 Grantley platform에서는 Intel Ultra Path Interconnect(UPI) mesh archictecture로 업그레이드 Comnined Home Agent(CHA)가 통합, 이는 LLC 등의 주소 정보 지도를 작성하며 이는 mesh 연결에서 목적지까지의 라우팅 정보를 제공 Fig1. Intel Xeon processor Scalable family mesh architecture Intel Ultra Path Interconnect UPI는 어드레스를 공유하는 mutiple processor coherent interconnect UPI는 vertical/horizontal path를 통한 한 코어에서 다른 코어로의 최단 경로를 제공 2소켓, 4소켓 링, 8소켓+크로스바 등 다양한 구조 지원 SubNon-Unified Memory Access Clustering 플랫폼은 모든 코어/LLC를 반씩 + 메모리 컨트롤를 1개씩 가진 SNC 0,1 도메인을 가짐 각 도메인은 각 메모리 컨트롤러에 매핑되는 유니크한 LLC 주소를 가지며 이는 LLC access latency를 낮춤 Cache Hierarchy Change 하기 그림과 같이 변경 LLC 및 MLC size 변경으로 hit rate 증가 Figure 11. Generational cache comparison single/Multiple Socket Parallel Processing UPI와 sub-NUMA의 지원으로 딥러닝 worker process들은 코어셋이나 싱글소켓, 다중소켓에 assign 될 수 있음 Advanced vector software extension Intel Advanced Vector Extension 512(Intel AVX-512)가 Vector Neural Network Instruction(VNNI)를 지원하는 AVX-512)_VNNI로 발전 대충 더 빨라지고 8/16/32 FP vector 연산을 지원한다는 듯(자세한 사항은 책 참조) Math Kernel Library for Deep Neural Network(MKL-DNN) Convolution, pooling, activation, batch normalization으로 구성된 최적화된 MKL-DNN 지원 key feature는 prefetching, data reuse, cache blocking, data layout, vectorization, register blocking이며 자세한 사항은 책 참조 NVIDIA Graphics Processing Unit (GPU) GPU 장점 : 효율적인 floating point 연산, high speed memory support Turing architecture를 개발함 (NVLink2를 위한 HBM2 적용, 캐시 구조 변경 등등) Tensor Core Architecture tensor core란 : 행렬연산 및 MAC를 위한 전용 코어 Turing Tensor core는 이전(Pascal)이 matrix row by row만 지원했으나 4X4X4 연산을 지원하도록 변경 INT8, INT4를 지원하며 정확도를 낮추면 연산 속도 증가 Matrix사이즈가 크면 이를 나누어 연산, 다양한 size의 매트릭스 연산에 대응 가능 https://www.nvidia.com/ko-kr/data-center/tensor-cores/ Winograd Transform 곱셈 횟수를 줄일 수 있는 Winograd Transform을 지원 상기 변환에 대한 연산식은 책과 다른 자료를 참조할 것 Simultaneous Multithreading (SMT) SMT의 경우 Matrix는 행렬을 여러 그룹으로 나누고 이를 병렬로 처리 (Single Instruction Multiple Thread, SIMT 방식) 연산 후 하위 그룹을 재그룹 시킴 High Bandwidth Memory (HBM2) Memory Bottleneck해결을 위해 HBM2 적용 (memory die를 TSV로 뚫어서 스택함) HBM2는 GPU와 NVLink2로 연결됨 NVLink2 Configuration NVLink는 엔비디아가 개발한 와이어 기반 통신 프로토콜 시리얼 멀티 레인 근범위 통신 링크 (PCIE의 속도 문제 해결) Turing 아키텍쳐는 sing MIO를 two×8 bidirectional differential pair NVLink2로 대체 CPU/GPU 메모리 간 directly load/store/atomic 가능 (데이터를 GPU메모리에서 바로 읽을 수 있고 CPU cache에 바로 저장 가능) 다양한 구성을 지원한다. (책을 참조하자) ","date":"2022-12-12T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-1/3/","title":"[AI HW Design] Chap03 Parallel Architecture (1/3)"},{"content":"목적 Ubuntu 20.04 LTS 설치 후 나에게 맞는 설정 및 설정 방법 정리 유의사항 설치 시 언어는 영어, 키보드 영어 자판으로 설치를 권장 개인 설정 Nvidia 그래픽 카드 설정 설치 가능한 드라이버 확인 1 ubuntu-drivers devices 권장 드라이버 설치 1 sudo ubuntu-drivers autoinstall 한영키 동작 설정 입력기 설치 : setting → Region and Language → Input Source → Korean(Hangul) 추가 1항의 추가된 항목 설정에서 Hangul Toggle Key를 Hangul만 남김(option) /usr/share/X11/xkb/symbols/altwin 편집 4행의 key \u0026lt;RALT\u0026gt; ... 부분에서 symbols[Gropu1] = [ Alt_R, Meta_R ] 부분을 [ Hangul ] 로 수정한다. VNC 설치 tigerVNC 설치 1 sudo apt-get install tigervnc-standalone-server tigervnc-xorg-extension 비밀번호 설정 1 vncpasswd ~/.vnc/xstartup 작성 1 2 3 4 5 6 #!/bin/sh # Start Gnome 3 Desktop [ -x /etc/vnc/xstartup ] \u0026amp;\u0026amp; exec /etc/vnc/xstartup [ -r $HOME/.Xresources ] \u0026amp;\u0026amp; xrdb $HOME/.Xresources vncconfig -iconic \u0026amp; dbus-launch --exit-with-session gnome-session \u0026amp; vnc 서버 실행 1 vncserver -localhost no vnc 서버 종료 1 vncserver -kill :2 설정변경 : $\u0026gt;sudo vim /etc/vnc.conf 1 2 $geometry = \u0026#34;1920x1080\u0026#34;; $depth = \u0026#34;16\u0026#34;; SSH 설치 서버 설치 1 sudo apt install openssh-server 실행여부 확인 1 sudo systemctl status ssh 서버 실행 1 2 sudo systemctl enable ssh sudo systemctl start ssh xforward 설정 팡일의 /etc/ssh/ssh_config 의 x11Forward no → x11Forward yes로 변경 ssh서버 재실행 및 클라언트 실행 시 -X 옵션 추가 ZSH/om-my-zsh 설치 및 설정 zsh 설치 1 sudo apt-get install zsh 설치확인 1 cat /etc/shells 기본쉘 변경 1 chsh -s $(which zsh) oh-my-zsh 설치(curl설치필요) 1 sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; 테마변경 ~/.zshrc 파일 내 ZSH_THEME=\u0026quot;agnoster\u0026quot; 로 변경 글자깨질 시 Powerline폰트 설치 1 sudo apt-get install fonts-powerline 커맨드라인 컴퓨터 이름 감추기 ~/.zshrc 하단에 하기 내용 추가 1 2 3 4 5 prompt_context() { if [[ \u0026#34;$USER\u0026#34; != \u0026#34;$DEFAULT_USER\u0026#34; || -n \u0026#34;$SSH_CLIENT\u0026#34; ]]; then prompt_segment black default \u0026#34;%(!.%{%F{yellow}%}.)$USER\u0026#34; fi } zsh-autosuggestions 플러그인 설치 1 git clone https://github.com/zsh-users/zsh-autosuggestions.git $ZSH_CUSTOM/plugins/zsh-autosuggestions zsh-syntax-highlighting 플러그인 설치 1 git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $ZSH_CUSTOM/plugins/zsh-syntax-highlighting autojump 설치 1 2 3 git clone https://github.com/wting/autojump.git cd autojump ./install.py 사용법 j [디렉토리 명] 또는 j -s 플러그인 활성화\n~/.zshrc 파일 내 plugins=(git zsh-autosuggestions zsh-syntax-highlighting autojump) 로 변경 줄바꿈 적용(멀티라인 입력)\n~/.oh-my-zsh/themes/agnoster.zsh-theme파일 수정 prompt_hg 하단에 prompt_newline 추가 후 파일 최하단 하기 프롬프트 추가 1 2 3 4 5 6 7 8 9 10 prompt_newline() { if [[ -n $CURRENT_BG ]]; then echo -n \u0026#34;%{%k%F{$CURRENT_BG}%}$SEGMENT_SEPARATOR %{%k%F{blue}%}$SEGMENT_SEPARATOR\u0026#34; else echo -n \u0026#34;%{%k%}\u0026#34; fi echo -n \u0026#34;%{%f%}\u0026#34; CURRENT_BG=\u0026#39;\u0026#39; } (option) TFPT 설치 xilinx petalinux를 사용할 생각이라면 tftp 설치가 필요하다\ntftp 설치 1 2 sudo apt-get update sudo apt-get install tftpd-hpa 서비스 확인 1 sudo service tftpd-hpa status 설정 파일 /etc/default/tftpd-hpa 를 원하는 대로 수정한다. 다른 것은 크게 의미가 없고 up/down 위치인 TFTP_DIRECTORY 정도만 수정 수정 후 디렉토리 권한 설정을 해준다. 1 2 3 4 vim /etc/default/tftpd-hpa sudo mkdir {tftp-dir} sudo chmod 777 {tftp-dir} sudo chown -R tftp:tftp {tftp-dir} 설정 완료 후 재시작 1 sudo service tftpd-hpa restart (option) NFS server 설치 nfs서버 패키지 설치 nfs 서버용 폴더를 만들고 모든 클라이언트 머신이 공유 디렉토리에 액세스하기 위하여 권한 제거 및 파일의 권한 제거 /etc/exports 파일을 편집하여 공유할 폴더를 지정하고 클라언트 및 실행 권한 설정 exportfs로 설정된 폴더 내보내기 nfs_server 재시작 1 2 3 4 5 6 7 sudo apt install nfs-kernel-server mkdir ${공유폴더} sudo chown -R nobody:nogroup ${공유폴더} sudo chmod 777 ${공유폴더} sudo echo \u0026#39;${공유폴더} 192.168.1.1/24(rw,sync,no_root_squash,no_subtree_check)\u0026#39; \u0026gt;\u0026gt; /etc/exports sudo exportfs -a sudo service nfs-kernel-server restart ","date":"2022-12-12T00:00:00Z","permalink":"https://muonkmu.github.io/p/ubuntu-20.04-%EA%B0%9C%EC%9D%B8-%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95/","title":"Ubuntu 20.04 개인 환경 설정"},{"content":"목적 클라우드 서버를 이용하여 원격으로 접속 가능한 개발 서버의 구축 최종 목표 고정 IP를 가진 ubuntu 서버 무료 클라우드 서버 중 오라클이 ARM64-4core/24GB ram/200GB storage VM 머신 제공 (타사 대비 월등히 좋음) 원격 개발을 위한 code-server 설치 서버 구축 클라우드 서버 구축 오라클 클라우드 Free tier 가입 리전은 원하는 곳(춘천이 빠르고 ARM 서버 리소스가 남음) 카드 정보를 기입(실제로 결제가 되지는 않음) 가입 완료 후 하단의 Create a VM instance 시작 instance Name 입력 image는 원하는거 선택, ex) canonical Ubuntu 20.04 shape는 Ampere 선택 core는 4, memory는 24GB 까지 무료 상기 리소스를 나누어 무료 VM를 생성할 수 있다.ex) 2core-12GB 인스턴스 2개 무료 VCN이 없다면 페이지에서 VCN을 생성하여 연결 본인의 PC에서 SSH를 생성하여 Public키를 업로드 한다. http://taewan.kim/oci_docs/98_misc_tips/ssh_key_pairs/ 부트 볼륨 생성 Specify a custom boot volume size을 클릭 후 원하는 볼륨생성 200GB까지 무료이며 상기 리소스를 나누어 무료 VM생성 가능 Create로 생성 해당 리전의 리소스가 부족하여 생성이 안되는 경우가 있다. 상기의 경우 리소스가 풀릴 때 까지 기다리거나 유료계정으로 업그레이드 (승인되는데 시간 걸림) 유료 계정이 되더라도 무료 리소스까지만 쓰면 과금이 되지 않는다. 클라우드 서버 환경 설정 고정 IP 설정 Compute \u0026gt; Instances \u0026gt; Instance Details \u0026gt; Attached VNICs \u0026gt; VNIC Details \u0026gt; IPv4 Addresses 상기 경로에서 NO PUBLIC IP 선택하여 IP 삭제 후 RESERVED PUBLIC IP로 변경 우분터 사용자 계정 생성(option) ssh 로그인 현재 계정 ubuntu 암호 생성 사용자 계정 생성 생성 계정에 sudo 권한 부여 계정 변경 ssh 비번으로 접속 설정 /etc/ssh/sshd_config파일의 PasswordAuthentication 값을 \u0026ldquo;yes\u0026quot;로 변경 클라우드 포트 개방 Networking \u0026gt; Virtual Cloud Networks \u0026gt; {사용중인 VNC} \u0026gt; Security List Details 상기 경로에서 포트 개방 추가 우분투 방화벽 포트 개방 1 sudo iptables -I INPUT 5 -p tcp --dport 8070 -m state --state NEW,ESTABLISHED -j ACCEPT code-server 설치 code-server 다운로드 및 설치 https://coder.com/docs/code-server/latest/install 1 curl -fsSL https://code-server.dev/install.sh | sh 서비스로 실행하기 위해 systemctl로 enable 1 sudo systemctl enable --now code-server@$USER 외부 접속을 위해 .config/code-server/config.yaml파일을 수정한다. 1 2 3 4 bind-addr: 0.0.0.0:{포트번호} auth: password password: {비밀번호} cert: false 서비스를 재시작 후 동작을 확인한다. 1 2 sudo systemctl restart --now code-server@$USER sudo systemctl status code-server@$USER chrome 브라우저에서 접속 시 이미지가 안보일 경우 하기 세팅을 수행 chrome://flags 설정 의 Insecure origins treated as secure Enable 후 http://{접속IP}:{접속Port} 추가 ","date":"2022-12-12T00:00:00Z","permalink":"https://muonkmu.github.io/p/%EC%9B%90%EA%B2%A9-%EA%B0%9C%EB%B0%9C-%EC%84%9C%EB%B2%84-%EA%B5%AC%EC%B6%95/","title":"원격 개발 서버 구축"},{"content":"본 chapter에서는 Gradient를 구하기 위한 Backpropagation을 이해하고 Neural Network의 기본에 대해 설명한다.\nVideo : https://www.youtube.com/watch?v=d14TUNcbn1k Slide : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf (TODO: 귀차니즘의 압박으로 정리를 안해놓았지만 언젠간 해야지)\nBackpropagation Chain rule Sigmoid gate example Patterns in backward flow Gradients add at branches Vectorized operations Neural Network Artificial Neural Network Activation Function Neural networks Architectures ","date":"2022-10-25T00:00:00Z","permalink":"https://muonkmu.github.io/p/cs231n-chap-04-introduction-to-neural-networks/","title":"[CS231n] Chap 04 Introduction to Neural Networks"},{"content":"본 chapter에서는 딥러닝의 기본 개념인 Loss Function, Regularization, Optization(Gradient Descent)에 대해 다룬다\nVideo : https://www.youtube.com/watch?v=h7iBpEHGVNc Slide : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture3.pdf (TODO: 귀차니즘의 압박으로 정리를 안해놓았지만 언젠간 해야지)\nLoss function Regularization Softmax and SVM Optimization Image Feature ","date":"2022-10-17T00:00:00Z","permalink":"https://muonkmu.github.io/p/cs231n-chap-03-loss-function-and-optimization/","title":"[CS231n] Chap 03 Loss Function and Optimization"},{"content":"본 chapter에서는 Computer Vision의 핵심 Task 중 하나인 Image classification에 대해 이해하고 초기의 방법인 K-Nearest Neighbor Algorithm과 Linear Classification에 대하여 다룬다.\nVideo : www.youtube.com/watch?v=OoUX-nOEjG0\u0026list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk\u0026index=2 Slide : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture2.pdf (TODO: 귀차니즘의 압박으로 정리를 안해놓았지만 언젠간 해야지)\nImage Classification 개요 K-Nearest Neighbor Algorithm Linear Classification ","date":"2022-10-08T00:00:00Z","permalink":"https://muonkmu.github.io/p/cs231n-chap-02-image-classification/","title":"[CS231n] Chap 02 Image classification"},{"content":"6개월에 걸쳐 수료를 완료 했다. 3개월 코스라고 하던데\u0026hellip;\nhttps://www.coursera.org/learn/machine-learning homework repo : https://github.com/muonkmu/Coursera_AndrewNg_ML_Program.git (TODO: 귀차니즘의 압박으로 정리를 안해놓았지만 언젠간 해야지)\n","date":"2022-01-25T00:00:00Z","permalink":"https://muonkmu.github.io/p/coursera_ml-course-certificate/","title":"[Coursera_ML] Course certificate"},{"content":"이번 강의에서는 대규모의 대규모의 데이터가 있을 때, 처리하는 알고리즘에 대해서 알아보자.\nhttps://www.coursera.org/learn/machine-learning homework repo : https://github.com/muonkmu/Coursera_AndrewNg_ML_Program.git (TODO: 귀차니즘의 압박으로 정리를 안해놓았지만 언젠간 해야지)\n","date":"2022-01-07T00:00:00Z","permalink":"https://muonkmu.github.io/p/coursera_ml-week_10-gradient-descent-with-large-datasets/","title":"[Coursera_ML] Week_10) Gradient Descent with Large Datasets"}]