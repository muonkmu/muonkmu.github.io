[{"content":"파일 열기 1 2 3 4 5 6 #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/stat.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; int open(const char *name, int flags); int open(const char *name, int flags, mode_t mode) 파일을 열고 파일 디스크립터에 매핑 flags 인자는 O_RDONLY, O_WRONLY, O_RDWR중 하나를 포함해야 함, 그외 O_APPEND, O_NONBLOCK 등이 있음 (책참조) mode에서는 새로운 파일의 권한(ex 0644, S_IRWXU 등이 있음, 책참조) O_WRONLY|O_CREAT|O_TRUNC 조합은 너무 일반적이라 creat()시스템 콜ㅇㄹ 제공 에러 발생 시 -1 리턴 후 errno를 적절한 에러값으로 설정 파일 읽기 1 2 3 #include \u0026lt;unistd.h\u0026gt; ssize_t read(int fd, void *buf, size_t len) fd가 참조하는 파일의 현재 오프셋에서 len byte바이트 만큼 buf에 읽기 현재 읽은 바이트수 반환, 에러시 -1 반환, 0 반환 시 EOF(읽을 데이터 없음)을 나타냄(책에 리턴의 다양한 상황에 대해 나와 있음) 논블럭 읽기 모드에서 읽을 데이터가 없다면 -1반환, errno를 EAGAIN으로 설정(논블럭 모드시 반드시 점검할 것) 최대 읽기 값은 SSIZE_MAX(LONG_MAX, 0x7ffffff) SIZE_MAX는 size_t의 최대값으로 부호가 없는 값, ssize_t는 부호 있는 값을 나타냄 파일 쓰기 1 2 3 #include \u0026lt;unistd.h\u0026gt; ssize_t write(int fd, const void *buf, size_t count) count 바이트 만큼 fd가 참조하는 현재 파일 시작지점이 buf인 내용 기록 현재 쓴 바이트수 반환, 에러시 -1 반환 후 errno를 적당한 값으로 변경 O_APPEND 모드는 파일 오프셋이 항상 파일 끝에 위치하도록 함(멀티 프로세스 관점에서 보면 파일 오프셋을 원자적으로 갱신) 논블럭 쓰기 모드에서 블럭되면 -1반환, errno를 EAGAIN으로 설정 write 명령 시스템콜은 물리적 영역에 바로 쓰는 것이 아닌 버퍼에 복사해 놓음, 이후 이 버퍼를 수집해서 정렬 후 디스크에 씀 동기식 입출력을 지원하기 위해 fsync()와 fdatasync(), sync() 시스템 콜 지원 fsync()는 버퍼의 모든 변경점을 디스크에 씀(메타데이터, 즉 파일생성시간 등 포함) fdatasync()는 메터데이터를 제외한 데이터만 기록 sync()는 인자/반환값이 없이 버퍼의 모든 내용을 기록하도록 요구, 범용성이 높음 open() 호출 시 O_SYNC 플래그 사용하면 모든 파일 입출력 동기화(O_DSYNC, O_RSYNC도 있다, 역할은 찾아볼 것) 직접입출력 open() 시 O_DIRECT 플래그를 사용하면, 캐시/버퍼링/입출력관리 값은 복잡한 계층을 우회하여 직접 입출력 관리, 효과가 미미하다. 파일 닫기 1 2 3 #include \u0026lt;unistd.h\u0026gt; int close(int fd) fd에 연관된 파일과의 매핑해제, 프로세스에서 파일 떼어냄 파일을 닫더라도 버퍼의 내용을 디스크에 강제로 쓰지 않는다. 파일을 닫을 때 커널 내부에서 그 파일을 표현하는 자료구조 해제, 메모리에서 inode 복사본 제거 close의 반환값을 검사하는 것이 좋다. EBADF(fd가 유효하지 않음) 및 EIO 가 중요 파일 탐색 1 2 3 4 #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; off_t lseek(int fd, off_t pos, int origin) 파일의 특정위치로 이동하는 함수, origin 인자 : SEEK_CUR, SEEK_END, SEEK_SET 호출 성공 시 새로운 파일 오프셋을 반환하며 에러가 발생하면 -1을 반환, errno를 설정 lseek(fd, 0, SEEK_CUR)을 이용하면 현재 오프셋을 알아낼 수 있다. 파일의 끝을 넘어 위치를 지정하는 것도 가능, 이의 경우 데이터를 읽으면 EOF 반환, 쓰기 요청시 새로운 공간 생성 후 0으로 채움 이렇게 0으로 채운 공간을 구멍이라고 하는데 이는 물리적 공간을 차지하지 않으며 이를 다루는 과정에서 물리적 입출력 작업이 필요하지 않다. 지정한 위치 읽고 쓰기 1 2 3 4 #include \u0026lt;unistd.h\u0026gt; ssize_t pread(int fd, void *buf, size_t count, off_t pos); ssize_t pwrite(int fd, const void *buf, size_t count, off_t pos); 읽고 쓸 파일 오프셋을 지정하여 read(), write() 수행함, 그러하 하기 차이점 존재 호출 완료 후 파일포인터를 갱신하지 않음 lseek사용 시 발생할 수 있는 경쟁 상태를 회피 가능(멀티 스레드에서 각 스레드가 동시에 offset을 업데이트 하려는 경우) 파일 잘라내기 1 2 3 4 5 #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int ftruncate(int fd, off_t len); int truncate(const char *path, off_t len) 파일을 len 크기로 잘라내는 시스템 콜 호출 성공 0 반환, 에러가 발생하면 -1을 반환, errno를 설정 파일의 크기보다 큰 값으로 잘라내기 가능, 확장된 바이트는 모두 0으로 채워 짐 ","date":"2023-04-22T00:00:00Z","permalink":"https://muonkmu.github.io/p/linux_system_programming-03/","title":"[Linux_System_programming] 03 "},{"content":"Petalinux로 사용하다 보니 여러가지 면에서 불편한다. apt패키지 매너저도 사용하고 싶고 향후 여러 라이브러리의 호한성을 맞추기 위해 rootFS를 ubuntu로 바꾸어 보자\n기본 지식 xilinx는 zynq의 운영체제로 리눅스를 자신의 입맛에 맞게 개조한 petalinux를 제공한다. 좀 더 정확하게 말하면 petalinux란 자신들의 zynq를 위한 리눅스 시스템을 빌드 할 수 있는 YOCTO wrapper 이다. 상기 시스템에서 제공하는 커널 및 rootFS를 그냥 사용해도 되지만 librealsense 등을 포팅하기 힘들고 패키지 매니저 사용들이 불편함으로 ubuntu base를 포팅하자 ubuntu desktop을 사용하여 GUI 환경을 바로 꾸밀 수 도 있지만 desktop 환경은 LXDE/KDE를 직접 설치해도 되므로 우선 CLI 환경만 구성한다. 펌웨어, uboot, 커널, 디바이스 트리는 petalinux에서 제공하는 것을 사용하고 rootFS 만 Ubuntu base로 사용한다. Porting 절차 커널 및 모듈 빌드 xilinx에서 Ubuntu 사용을 위해 권장하는 petalinux의 커널 세팅은 하기와 같다. https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/148668419/Zynq+UltraScale+MPSoC+Ubuntu+VCU+Gstreamer+-+Building+and+Running+Ubuntu+Desktop+from+Sources 근데 PCI 는 왜 disable 하라는지 몰라서 그냥 enable 시켜 놓았다. (아마 제공하는 예제에서는 AP 부분 DP dual lane을 사용하느라 PCIe를 안써서 그런것 같다.) 향후 사용성을 위해 mali driver도 같이 넣기로 하였다. Following are some of the mandatory configurations needed for successful booting of Ubuntu Desktop. Disable initramfs in kernel configuration GUI at ‘General setup -\u0026gt; Initial RAM file system and RAM disk (initramfs/initrd) support’ Following settings are required to enable Input device, multimedia and USB related settings Device Drivers-\u0026gt;Input device support-\u0026gt;Event interface Device Drivers-\u0026gt;Input device support-\u0026gt;Keyboards Device Drivers-\u0026gt;Input device support-\u0026gt;Mouse interface Device Drivers-\u0026gt;Multimedia support-\u0026gt;Media USB Adapters-\u0026gt;USB Video Class (UVC) Device Drivers-\u0026gt;Multimedia support-\u0026gt;Cameras/video grabbers support Device Drivers-\u0026gt;Multimedia support-\u0026gt;V4L platform devices Device Drivers-\u0026gt;USB support and enable all required classes Device Drivers-\u0026gt;HID support-\u0026gt;Generic HID driver Device Drivers-\u0026gt;HID support-\u0026gt;USB HID support-\u0026gt;USB HID transport layer Disabling the PMBUS PMIC so that power demo can use them without any issues Device Drivers-\u0026gt;Hardware Monitoring support-\u0026gt;PMBus support-\u0026gt;Maxim MAX20751 Enable the PHY settings Device Drivers-\u0026gt;PHY Subsystem Device Drivers-\u0026gt;PHY Subsystem-\u0026gt;Xilinx ZynqMP PHY driver Disable the PCI settings Bus Support-\u0026gt;PCI support’ This needs to be disabled for this version Enable the sound related settings: Device Drivers-\u0026gt;Sound card support Device Drivers-\u0026gt;Sound card support-\u0026gt;Advanced Linux Sound Architecture’ enabling ALSA support Kernel hacking \u0026gt; Tracers \u0026gt; Kernel Function Tracer Enable VCU driver Device Drivers-\u0026gt;SOC (System On Chip) Specific Drivers-\u0026gt;Xilinx SoC Drivers-\u0026gt;[ m ] Xilinx VCU logicoreIP Initsupport mali 및 vcu 커널모듈을 만들기 위해 petalinux rootfs 세팅을 다음과 같이 설정한다. Filesystem Packages-\u0026gt;libs-\u0026gt;libmali-xlnx-\u0026gt;[ * ] libmali-xlnx Packages-\u0026gt;libs-\u0026gt;libmali-xlnx-\u0026gt;[ * ] libmali-xlnx-dev Filesystem Packages-\u0026gt;misc-\u0026gt;hdmi-module-\u0026gt;[ * ] kernel-module-hdmi 혹시 mali backend를 x11에서 fb나 wayland로 하고 싶다면 \u0026lt;plnx-proj-root\u0026gt;/project-spec/meta-user/conf/petalinuxbsp.conf에 MALI_BACKEND_DEFAULT = \u0026quot;wayland\u0026quot;를 추가하자 (https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/18841928/Xilinx+Arm+Mali-400+Driver#XilinxArmMali-400Driver-X11backend%3A) petalinux를 빌드한다. 빌드 중에 cpio에서 에러가 나는 경우가 있는데 이는 파일용량이 2GB를 넘어가서이다. 해결방법은 아래와 같다. https://docs.xilinx.com/r/2021.2-English/ug1144-petalinux-tools-reference-guide/do_image_cpio-Function-Failed rootfs 포팅 하기 사이트에 우분투 base의 rootfs를 받을 수 있다. 그러나 이것을 사용하면 사용자 입맛에 맞게 이미지를 만들 수 있으나 설정에 시간이 너무 오래 걸린다. https://cdimage.ubuntu.com/ubuntu-base/releases/focal/release/ 그래서 eewiki에서 제공하는 이미지를 사용하기로 하였다. 이미지를 다운 받아 rootfs 영역에 카피한다.(압축 풀 때 파일 권한이 변경되지 않도록 tar의 p옵션 필수) https://nuclearrambo.com/wordpress/running-ubuntu-20-04-on-zynq-soc/ mali와 vcu 커널 모듈을 rootfs복사한다.(옵션) sudo cp -rfv build/tmp/sysroots-components/xilinx_zcu104/kernel-module-vcu/lib/modules/5.15.19-xilinx-v2022.1/extra/* ${target_root}/root/modules/ sudo cp -rfv build/tmp/sysroots-components/xilinx_zcu104/kernel-module-mali/lib/modules/5.15.19-xilinx-v2022.1/extra/* ${target_root}/root/modules/ sudo cp -rfv build/tmp/sysroots-components/zynqmp/vcu-firmware/lib/firmware/* ${target_root}/lib/firmware/ zynq - ubuntu 기본설정 zynq 보드를 부팅시키고 로그인한다. 네트워크 매니저를 세팅한다. 현재 네트워크 매니저 세팅파일이 없으므로 만들어 준다. 호스트가 ubuntu라면 동일 파일이 있으니 복사해도 된다. 그후 리부팅 1 2 3 4 5 6 7 8 sudo cat \u0026gt; /etc/netplan/01-network-manager-all.yaml \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; network: version: 2 renderer: NetworkManager EOF sudo netplan generate sudo netplan apply sudo reboot 무슨 문제인지 부팅 후에도 dhcp에서 DNS를 받아오지 못한다(원인 파악 중) 다음 방법으로 해결해보자 임시 : echo \u0026ldquo;nameserver ${네임서버주소}\u0026rdquo; | sudo tee /etc/resolv.conf \u0026gt; /dev/null 영구 : /etc/systemd/resolved.conf 파일에 DNS 입력 후 재부팅 업데이트 및 시간대 설정 1 2 sudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade \u0026amp;\u0026amp; sudo apt-get dist-upgrade timedatectl set-timezone Asia/Seoul 참고 자료 https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/148668419/Zynq+UltraScale+MPSoC+Ubuntu+VCU+Gstreamer+-+Building+and+Running+Ubuntu+Desktop+from+Sources https://xilinx.github.io/Embedded-Design-Tutorials/docs/2022.2/build/html/docs/Design_Tutorials/MPSoC_Graphic_Subsystem/README.html https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/18841732/Ubuntu+on+Zynq https://nuclearrambo.com/wordpress/running-ubuntu-20-04-on-zynq-soc/ ","date":"2023-04-19T00:00:00Z","permalink":"https://muonkmu.github.io/p/yolo_acc_prj-06-ubuntu-setting/","title":"[YOLO_Acc_prj] 06 Ubuntu setting"},{"content":"매뉴얼 대로 했는데 커널 패닉이 일어난다. 로그를 보니 rootfs 디바이스가 로드 되지 않는 듯 하다. 일반적인 환경이 아니라 vmware상에서 개발하다보니 생긴 문제이다. 즉 커널 모듈이 vmware의 SCSI 드라이버를 로드하지 못하여 생기는 문제인 것 같다. 하기 두가지 방법으로 해결이 가능하다.\ninitramfs환경으로 부팅 : grub.cfg와 fstab에 파티션 이름이 아닌 UUID를 넣고 initramfs로 부팅하는 방법인데 부팅은 되나 이제껏 구성했던 환경은 아니다. kernel에 필요한 모듈을 넣어 다시 컴파일 하는 방법 필자는 삽질에 삽질을 계속하여 2번째 방법으로 해결하였다.\nKernel rebuild Host 환경 재로그인 필자는 grub을 설치한 후 백업을 하지 않았다. grub 명령어로 host의 우부투를 부팅하자. (부팅 시 grub 부팅 선택 화면에서 c키를 누르면 됨)\n1 2 3 4 set root=(hd0,1) linux /boot/vmlinuz root=/dev/sda1 initrd /boot/initrd.img boot 이후 매뉴얼의 7.3, 7.4절에서 기술한 내용으로 LFS chroot 환경으로 진입한다.\nkernel rebuild 매뉴얼의 10.3절에서 다음의 커널 모듈을 포함하여 커널을 리빌드한다. 사실 몇가지만 필요할텐데 정확하게 필요한게 먼지 몰라서 다 포함 시켰다.\nVMware Balloon Driver VMware VMCI Driver Maintain a devtmpfs filesystem AMD PCnet32 PCI support SCSI device support, SCSI low-level drivers BusLogic SCSI support Fusion MPT ScsiHost drivers for SPI Fusion MPT ScsiHost drivers for FC Fusion MPT ScsiHost drivers for SAS File Systems, Ext4 Journaling file system support 커널이 리빌드 되면 커널 모듈을 인스톨하고 빌드된 커널을 /boot에 복사하자. 이후 리부팅 후 성공\nreference https://blog.andreev.it/2013/09/linux-from-scratch-as-a-virtual-machine-on-vmware-workstation/ https://www.linuxquestions.org/questions/linux-from-scratch-13/my-lfs-does-not-boot-878722/ ","date":"2023-03-28T00:00:00Z","permalink":"https://muonkmu.github.io/p/lfs_prj-05-final-debugging/","title":"[LFS_prj] 05 Final debugging"},{"content":"ZCU104 - See3CAM_CU30를 이용하여 이미지 포맷 변환 및 리사이즈 Pipeline을 구성하였다. Xilinx에서 제공하는 M2M 기본 예제를 이용하였으며 삽질의 연속으로 2주 정도 걸린 것 같다. 결론적으로 Xilinx에서 제공하는 VPSS의 경우 다양한 기능을 지원하여 LUT의 사용량이 많고 M2M 드라이버의 경우 지원이 종료되었다. 향후에는 HLS로 직접 구현해야 할 것으로 생각된다.\n기본 지식 Xilinx IP list Video Processing SubSystem(VPSS) : Color Space Conversion(CSC)와 SCaler(SC) 기능을 지원한다. 기본적으로 Xilinx_Video_Driver와 호환된다. Framebuffer Read/Write : VDMA의 후속(?)으로 이미지 전용 DMA이다. Gstreamer와 호환이 되어 편리하게 이용이 가능하다. Xilinx Linux Driver Mem 2 Mem Composite Video Framework : Framebuffer Read/Write를 이용하여 비디오 파이프라인을 기술해주는 Bridge Driver이다. 현재 지원 종료되었으므로 테스트로만 이용한다.(지원종료된 이유를 확인 중) HW Design 레퍼런스에 나와 있는데로 FB_R -\u0026gt; CSC -\u0026gt; SC -\u0026gt; FB_W 파이프 라인으로 구성한다. 필자가 실수하여 삽질한 부분은 하기와 같다.\n파이프라인의 동작 클럭은 300Mhz로 하였다. 내부 클럭을 이용하므로 clock wizard ip의 입력 패드를 No buffer로 설정 FB_R과 FB_W의 리셋은 드라이버에 의해 구동되므로 PS EMIO로 제어 가능하게 구성한다.(pl reset으로 구동하지 말것) 생각보다 CSC/SC가 리소스를 많이 잡아 먹어서 나중에 HLS로 변경해야 할 듯 하다. SW Design 파이프라인 SW component의 구성을 보면 하기와 같다(필자의 생각이므로 틀릴 수 있음) Deviece Tree Petalinux는 빌드 시 HW Description 파일(.xsa)을 분석하여 PL Side의 Device Tree를 자동으로 생성해준다. \u0026lt;plnx-proj-root\u0026gt;/component/plnx_workspace/device-tree/device-tree/pl.dtsi에서 생성된 Device-tree를 확인할 수 있다 (petalinux-config에서 자동으로 디바이스트리를 생성하도록 설정되어 있어야한다). 우리는 vidoe-M2M 드라이버를 사용해야 하므로 디바이스 트리를 수정해야 하는데 이는 \u0026lt;plnx-proj-root\u0026gt;/project-spec/meta-user/recipes-bsp/device-tree/files/system-user.dtsi에 기술 시 적용된다. 하기 내역을 수정한다.\nFrameBufferRead/Write IP에 의해 브리지 노드가 생성되는데 이 노드를 Disable하고 Properity 및 node를 삭제한다. video_m2m 노드를 생성하고 DMA기술하고 및 포트 연결을 기술한다. CSC/SC 노드의 포트의 연결을 수정하여 video_m2m과 연결될 수 있도록 한다. CSC/SC 노드의 compatible 속성을 xlnx,v-vpss-csc 와 xlnx,v-vpss-sc로 고정한다. 이 v-vpss-*가 비디오 드라이버와 연결되는 드라이버이다. Test 보드가 정상적으로 부팅이 된다면 /dev/video#, /dev/media#, /dev/v4l-subdev#이 등록되었을 것이다. 상기 디바이스가 보이지 않는다면 dmesg 등을 이용해 커널 로그 및 부팅 로그를 확인하여 문제를 분석하자.\nTest는 media-ctl로 CSC/SC를 설정하고 Gstreamer로 스트리밍 파이프라인을 구성하여 결과를 파일로 저장하였다. 카메라 출력은 UYUV/1280x720이며 CSC/SC에서 이를 RGB/640x480으로 변환한다.\n1 2 3 4 5 6 7 media-ctl -d /dev/media0 -V \u0026#34;\\\u0026#34;a0100000.v_proc_ss\\\u0026#34;:0 [fmt:UYVY8_1X16/1280x720 field:none colorspace:rec709]\u0026#34; media-ctl -d /dev/media0 -V \u0026#34;\\\u0026#34;a0100000.v_proc_ss\\\u0026#34;:1 [fmt:RBG888_1X24/1280x720 field:none colorspace:srgb]\u0026#34; media-ctl -d /dev/media0 -V \u0026#34;\\\u0026#34;a0040000.v_proc_ss\\\u0026#34;:0 [fmt:RBG888_1X24/1280x720 field:none colorspace:srgb]\u0026#34; media-ctl -d /dev/media0 -V \u0026#34;\\\u0026#34;a0040000.v_proc_ss\\\u0026#34;:1 [fmt:RBG888_1X24/640x480 field:none colorspace:srgb]\u0026#34; gst-launch-1.0 v4l2src device=/dev/video1 num-buffers=1 ! video/x-raw, width=1280, height=720, format=UYVY ! v4l2convert capture-io-mode=4 output-io-mode=4 ! video/x-raw, width=640, height=480, format=RGB ! filesink location=test.rgb gst-launch-1.0에서 파이프라인 입출력 설정이 한번에 되지 않는 거 같은데 테스트 스크립트임으로 넘어가자..\n상기 저장된 파일은 RGB raw 데이터 파일인데 이를 확인하기 위해 간단한 파이썬 코드를 이용한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import os import cv2 import sys import numpy as np HEIGHT = 480 WIDTH = 640 CHANNEL = 3 if __name__ == \u0026#39;__main__\u0026#39;: #argment number check if (len(sys.argv) \u0026lt; 2) : print(\u0026#34;Arguments is missing\\n\u0026#34;) exit(1) rawRgbFileName = sys.argv[1] rawRgbFd = open(rawRgbFileName, \u0026#39;rb\u0026#39;) cvImage = np.fromfile(rawRgbFd, dtype=np.uint8, count=HEIGHT*WIDTH*CHANNEL).reshape(HEIGHT,WIDTH,CHANNEL) image_rgb = cv2.cvtColor(cvImage, cv2.COLOR_RGB2BGR) rawRgbFd.close # cv2.imshow(\u0026#39;\u0026#39;, cvImage) cv2.imshow(\u0026#39;\u0026#39;, image_rgb) cv2.waitKey() cv2.destroyAllWindows() 참고 자료 M2M reference : https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/80707675/Mem+2+Mem+VPSS-CSC+VPSS-SC+device Petalinux Devie Tree Tip : https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/18842482/Device+Tree+Tips ","date":"2023-03-22T00:00:00Z","permalink":"https://muonkmu.github.io/p/yolo_acc_prj-05-image-format-conversion-and-resize/","title":"[YOLO_Acc_prj] 05 Image format conversion and resize"},{"content":"chroot 환경에서 LFS 시스템 구축 과정을 다룬다.\nInstalling Basic System Software Package install 하기 패키지를 설치한다\nMan-pages-6.03 Iana-Etc-20230202 Glibc-2.37 Zlib-1.2.13 Bzip2-1.0.8 Xz-5.4.1 Zstd-1.5.4 File-5.44 Readline-8.2 M4-1.4.19 Bc-6.2.4 Flex-2.6.4 Tcl-8.6.13 Expect-5.45.4 DejaGNU-1.6.3 Binutils-2.40 GMP-6.2.1 MPFR-4.2.0 MPC-1.3.1 Attr-2.5.1 Acl-2.3.1 Libcap-2.67 Shadow-4.13 GCC-12.2.0 Pkg-config-0.29.2 Ncurses-6.4 Sed-4.9 Psmisc-23.6 Gettext-0.21.1 Bison-3.8.2 Grep-3.8 Bash-5.2.15 Libtool-2.4.7 GDBM-1.23 Gperf-3.1 Expat-2.5.0 Inetutils-2.4 Less-608 Perl-5.36.0 XML::Parser-2.46 Intltool-0.51.0 Autoconf-2.71 Automake-1.16.5 OpenSSL-3.0.8 Kmod-30 Libelf from Elfutils-0.188 Libffi-3.4.4 Python-3.11.2 Wheel-0.38.4 Ninja-1.11.1 Meson-1.0.0 Coreutils-9.1 Check-0.15.2 Diffutils-3.9 Gawk-5.2.1 Findutils-4.9.0 Groff-1.22.4 GRUB-2.06 Gzip-1.12 IPRoute2-6.1.0 Kbd-2.5.1 Libpipeline-1.5.7 Make-4.4 Patch-2.7.6 Tar-1.34 Texinfo-7.0.2 Vim-9.0.1273 MarkupSafe-2.1.2 Jinja2-3.1.2 Systemd-252 D-Bus-1.14.6 Man-DB-2.11.2 Procps-ng-4.0.2 Util-linux-2.38.1 E2fsprogs-1.47.0 Cleaning up 패키지 설치를 위한 컴파일 시 디버그 옵션(-g)가 들어가 있으르모 디버그 심볼이 포함되어 있다. 이를 제거한다면 용량을 줄일 수 있다. 그러나 시스템 소프트웨어에서 디버깅을 하거나 valgrind 또는 GDB를 이용한 회기 테스트 시 디버그 심볼이 필요하다. 매뉴얼을 참조하여 디버깅 심볼을 스트리핑하여 백업하자 /tmp 폴더의 임시파일, libtool archive 파일, 이전 장에서 사용한 컴파일러 등은 필요없으니 매뉴얼을 참조하여 삭제 System Configuration General Network Configuration systemd-networkd에 의해 기본적인 네트워크 구성이 된다. 네트워크 디바이스의 이름을 변경하고 싶거나 정적 IP/DHCP를 구성하고 싶을시 /etc/systemd/netwokr 하위에 관련 설정 파일을 생성 /etc/resolv.conf 파일을 이용해 인터넷 연결 시 필요한 DNS를 설정할 수 있다. /etc/hosts에 호스트 이름을 설정(요즘은 네임서버가 거의 있어서 필요업지만 /etc/hosts를 참조하여 네임서버를 ip로 변환) Managing Devices systemd를 설치했으므로 udev 데몬이 활성화 된다.(udev는 부팅시 sysfs, 내부적으로는 devtmpfs에 검색된 디바이스를 로딩한다.) devtmpfs 인스턴스가 /dev에 마운트 될때, 디바이스 노드가 이름,퍼미션,소유권을 변경하면서 생성, 그 뒤커널은 uevent를 udev데몬(udevd)에 날린다. /etc/udev/rules.d 등 에 기술된 rule을 기반으로 udevd는 디바이스 노드에대한 새로운 심볼릭링크를 생성 만약 중복된 디바이스의 이름을 변경하고 싶다면(ex, usb 카메라와 같은) /etc/udev/rules.d에 룰을 기술 Time zone과 NTP time synq등을 설정한다. 리눅스 콘설 설정파일을 만들고 locale등을 설정한다. readline을 위한 /etc/inputrc를 만들고 /etc/shells 파일을 생성한다. Making the LFS System Bootable /etc/fstab 파일을 만들어 rootfs 와 swap파티션을 마운트 하자 리눅스 커널을 빌드하여 /boot폴더에 복사한다. grub을 설치하고 grub.cfg파일을 만들어 부팅 메뉴를 작성한다.(메뉴얼에서는 백업방법이 나와있다. 왠만하면 따라서 하자) The end chroot환경에서 로그아웃 후 마운트된 디렉토리를 해제하고 리부팅하면 끝이다. 그런데 커널패닉이 발생하니\u0026hellip;.디버깅은 다음장에\u0026hellip;.\n","date":"2023-03-19T00:00:00Z","permalink":"https://muonkmu.github.io/p/lfs_prj-04-building-the-lfs-system/","title":"[LFS_prj] 04 Building the LFS system"},{"content":"LFS 시스템 빌드를 위한 cross tool chain의 설정과정을 다룬다.\nTechical note LFS 시스템의 빌드 단계는 아래의 단계를 따른다\nStage Build Host Target Action 1 pc pc lfs Build cross-compiler cc1 using cc-pc on pc. 2 pc lfs lfs Build compiler cc-lfs using cc1 on pc. 3 lfs lfs lfs Rebuild and test cc-lfs using cc-lfs on lfs 여기서 1단계와 2단계가 cross compiler를 구축하는 단계이고 3단계는 chroot와 cross compiler를 이용한 LFS 시스템을 만드는 단계이다. cross compiler를 만드는 단계가 2단계로 구분되어 있는 이유는 하기와 같이 설명하고 있다.\nC언어는 단순컴파일러가 아니라 표준 라이브러리(glib)도 정의하는데 이는 cc1으로 컴파일되어야 함 그런데 컴파일러는 내부 라이브러리 libgcc를 이용해 컴파일하는데 이는 glib에 연결되어 있어야함 이 상호 의존 문제를 해결하기 위해 먼저 cc1 및 libgcc(스레드,예외처리 일부 기능이 부족한)를 만들고 이를 이용해 glib을 만듬(glib은 성능저하 없음) 그 다음 2단계에서 cc1을 이용해 cc-lfs를 만드며 이때 libgcc 및 llibstdc++도 다시 만듬 그리고 3단계(chroot) 환경에서 기존에 만든 패키지가 있음에도 다시 모든 패키지를 빌드하는데 이는 LSF를 안정적으로 만들어 준다.(2단계에서 만든 패키지들이 몇가지 디펜던시가 부족하므로)\n(이해하기 좀 빡세다\u0026hellip;해보면 이해간다는데 해봐도 머리속이 클리어하지 않다.)\nCompiling a Cross-Toolchain 위의 1단계, cc1을 빌드하고 glib을 만든다. 그리고 cc-lfs도 만들 수 있도록 libstdc++도 추출함\nBinutils-2.40 - Pass 1\nlinker, an assembler, and other tools for handling object files \u0026ndash;prefix=$LFS/tools, \u0026ndash;with-sysroot=$LFS 옵션으로 볼 때 host환경에서 쓴다는 것을 알 수 있다. GCC-12.2.0 - Pass 1\n-prefix=$LFS/tools, \u0026ndash;with-sysroot=$LFS 옵션으로 볼 때 host환경에서 사용 \u0026ndash;disable-shared에서 내부 라이브러리를 정적으로 쓰도록 함, 호스트의 동적 라이브러리로 인한 문제 차단 \u0026ndash;disable-libstdcxx 등 : 위에서 설명한 일부기능이 부족한 cc1이 됨 Linux-6.1.11 API Headers\n리눅스 커널에서 Glibc에 쓰이는 커널 API인 리눅스 API 헤더를 추출 Glibc-2.37\nGlibc 패키지는 메인 C 라이브러리를 포함하고 있다. 이 라이브러리는 메모리 할당, 디렉토리 검색, 파일 열기 및 닫기, 파일 읽기 및 쓰기, 문자열 처리, 패턴 대조, 산술 등의 기본 루틴을 제공한다 \u0026ndash;prefix=/usr 로 볼때 빌드된 것이 LFS 시스템 상에서 사용됨을 알 수 있다. \u0026ndash;host=$LFS_TGT, \u0026ndash;build=$(../scripts/config.guess) 는 이전의 gcc와 binutil등을 이용해서 크로스 컴파일함 Libstdc++ from GCC-12.2.0\nc++을 설치하기 위해서 Libstdc++가 필요한데 Glibc에 디펜던시를 가지므로 gcc 설치 시 설치하지 않았다. Glibc와 같이 \u0026ndash;prefix=/usr, \u0026ndash;host=$LFS_TGT, \u0026ndash;build=$(../scripts/config.guess) 구성을 사용 Cross Compiling Temporary Tools 위 기술노트의 2단계, cc-lfs를 구축한다.\nM4-1.4.19 매크로 처리기 Ncurses-6.4 문자 화면의 터미널 독립적 처리를 위한 라이브러리를 포함(clear, tab 같은) Bash-5.2.15 배쉬 쉘 Coreutils-9.1 기본적인 시스템 특성을 보여주고 설정하기 위한 유틸리티(cat, chmod, cp, dd 등등) Diffutils-3.9 diff 유틸 File-5.44 file 명령어 util Findutils-4.9.0 Findutils 패키지는 파일을 찾는 프로그램을 포함(find 등) Gawk-5.2.1 텍스트 편집에 최적화된 스크립트 언어 Grep-3.8 Gzip-1.12 Make-4.4 Patch-2.7.6 Sed-4.9 Tar-1.34 Xz-5.4.1 Binutils-2.40 - Pass 2 GCC-12.2.0 - Pass 2 Entering Chroot and Building Additional Temporary Tools 위의 빌드를 위한 임시 시스템에서 필요한 몇가지 패키지를 chroot의 분리된 환경에서 설치한다.\n상기 패키지를 host환경이 아닌 LFS 환경에서 설치하는 이유는 나와있지 않다. LFS ver9에서는 chroot로 LFS 환경으로 들어가기 이전에 모든 빌드 시스템 구축을 끝낸다.(이게 더 이해가 쉬운듯) Entering Chroot env and make VFS 현재까지 만들어진 파일 시스템은 lfs 소유인데 이는 다른 유저가 파일을 조작할 권리를 얻을 수 있으므로 소유자를 root로 변환\n1 2 3 4 chown -R root:root $LFS/{usr,lib,var,etc,bin,sbin,tools} case $(uname -m) in x86_64) chown -R root:root $LFS/lib64 ;; esac /dev 등 필요한 가상 커널 파일 시스템을 생성한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 mkdir -pv $LFS/{dev,proc,sys,run} mount -v --bind /dev $LFS/dev mount -v --bind /dev/pts $LFS/dev/pts mount -vt proc proc $LFS/proc mount -vt sysfs sysfs $LFS/sys mount -vt tmpfs tmpfs $LFS/run if [ -h $LFS/dev/shm ]; then mkdir -pv $LFS/$(readlink $LFS/dev/shm) else mount -t tmpfs -o nosuid,nodev tmpfs $LFS/dev/shm fi chroot를 이용하여 LFS환경으로 진입한다\n1 2 3 4 5 6 chroot \u0026#34;$LFS\u0026#34; /usr/bin/env -i \\ HOME=/root \\ TERM=\u0026#34;$TERM\u0026#34; \\ PS1=\u0026#39;(lfs chroot) \\u:\\w\\$ \u0026#39; \\ PATH=/usr/bin:/usr/sbin \\ /bin/bash --login 리눅스 표준 디렉토리 트리를 생성한다(리눅스 파일 시스템 계층 표준 기반)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 mkdir -pv /{boot,home,mnt,opt,srv} mkdir -pv /etc/{opt,sysconfig} mkdir -pv /lib/firmware mkdir -pv /media/{floppy,cdrom} mkdir -pv /usr/{,local/}{include,src} mkdir -pv /usr/local/{bin,lib,sbin} mkdir -pv /usr/{,local/}share/{color,dict,doc,info,locale,man} mkdir -pv /usr/{,local/}share/{misc,terminfo,zoneinfo} mkdir -pv /usr/{,local/}share/man/man{1..8} mkdir -pv /var/{cache,local,log,mail,opt,spool} mkdir -pv /var/lib/{color,misc,locate} ln -sfv /run /var/run ln -sfv /run/lock /var/lock install -dv -m 0750 /root install -dv -m 1777 /tmp /var/tmp 리눅스는 마운트된 파일시스템의 리스트를 /etc/mtab에 관리하고 이를 /proc에 노출시키므로 이를 위한 심볼릭 링크를 생성\nmtab은 mount command에 의해 자동갱신됨, fstab은 부팅 시 마운트할 목록 1 ln -sv /proc/self/mounts /etc/mtab host 파일과 /etc/passwd. /etc/group을 생성한다. 필요 시 테스트를 위한 테스트 그룹도 생성하고 재로그인 한다. 생성된 그룹은 어떤 표준에도 속하지 않는 Udev 구성의 요구사항에 의해 만들어진 그룹이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 cat \u0026gt; /etc/hosts \u0026lt;\u0026lt; EOF 127.0.0.1 localhost $(hostname) ::1 localhost EOF cat \u0026gt; /etc/passwd \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/dev/null:/usr/bin/false daemon:x:6:6:Daemon User:/dev/null:/usr/bin/false messagebus:x:18:18:D-Bus Message Daemon User:/run/dbus:/usr/bin/false systemd-journal-gateway:x:73:73:systemd Journal Gateway:/:/usr/bin/false systemd-journal-remote:x:74:74:systemd Journal Remote:/:/usr/bin/false systemd-journal-upload:x:75:75:systemd Journal Upload:/:/usr/bin/false systemd-network:x:76:76:systemd Network Management:/:/usr/bin/false systemd-resolve:x:77:77:systemd Resolver:/:/usr/bin/false systemd-timesync:x:78:78:systemd Time Synchronization:/:/usr/bin/false systemd-coredump:x:79:79:systemd Core Dumper:/:/usr/bin/false uuidd:x:80:80:UUID Generation Daemon User:/dev/null:/usr/bin/false systemd-oom:x:81:81:systemd Out Of Memory Daemon:/:/usr/bin/false nobody:x:65534:65534:Unprivileged User:/dev/null:/usr/bin/false EOF cat \u0026gt; /etc/group \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; root:x:0: bin:x:1:daemon sys:x:2: kmem:x:3: tape:x:4: tty:x:5: daemon:x:6: floppy:x:7: disk:x:8: lp:x:9: dialout:x:10: audio:x:11: video:x:12: utmp:x:13: usb:x:14: cdrom:x:15: adm:x:16: messagebus:x:18: systemd-journal:x:23: input:x:24: mail:x:34: kvm:x:61: systemd-journal-gateway:x:73: systemd-journal-remote:x:74: systemd-journal-upload:x:75: systemd-network:x:76: systemd-resolve:x:77: systemd-timesync:x:78: systemd-coredump:x:79: uuidd:x:80: systemd-oom:x:81: wheel:x:97: users:x:999: nogroup:x:65534: EOF exec /usr/bin/bash --login 마지막으로 login, agetty 및 init 들은 로그파일이 존재하지 않으면 로그를 남기지 않으므로 파일을 미리 생성해준다.\n1 2 3 4 touch /var/log/{btmp,lastlog,faillog,wtmp} chgrp -v utmp /var/log/lastlog chmod -v 664 /var/log/lastlog chmod -v 600 /var/log/btmp Building Additional Temporary Tools 하기 패키지를 설치한다.\nGettext-0.21.1 다국어화와 현지화를 위한 유틸리티가 포함, 프로그램을 현지 언어를 지원하도록 컴파일 할 수 있음 Bison-3.8.2 파서 생성기가 포함(?) Perl-5.36.0 Python-3.11.2 Texinfo-7.0.2 info 페이지를 읽고, 쓰고, 변환하는 프로그램(man보다 자세하다고 함) Util-linux-2.38.1 파일 시스템, 콘솔, 파티션 및 메시지를 처리하는 유틸리티가 포함된 패키지 Cleaning and Backup 툴체인 셋업이 완료되었으므로 필요하다면 현재구성을 백업하도록 하자. 상세 내용은 메뉴얼 참조\n","date":"2023-03-18T00:00:00Z","permalink":"https://muonkmu.github.io/p/lfs_prj-06-ubutu-porting/","title":"[LFS_prj] 06 Ubutu porting"},{"content":"첫번째로 Host를 셋업하는 과정을 서술한다. 기본적으로 VMware에 우분투 20.04가 설치되었고 파티션이 분할 되었다는 가정하에 진행한다.\nHost 환경설정 LFS를 설치하고 빌드하는데 필요한 host의 환경을 설치하고 필요 package를 설치한다. 하기 링크를 참조한다.\nhttps://www.linuxfromscratch.org/lfs/view/stable-systemd/chapter02/hostreqs.html 상기 링크에는 host의 필요 패키지 버전 및 상태를 체크하는 스크립트도 제공되므로 실행해보도록 하자. 필자의 환경에서 스크립트를 실행 시 하기 문제점이 발견되서 수정하였다.\n/bin/sh의 심볼릭 링크가 dash가 아닌 bash로 설정 sudo dpkg-reconfigure dash 실행 후 no 선택 binutils 등 설치되지 않은 패키지 설치 binutils, bison, gawk, gcc, g++, make, texinfo 우분투에서 설치가능한 패키지의 버전을 확인하고 싶으면 apt-cache policy ${패키지 이름}으로 확인 가능하다. 필자는 패키지 버전을 특별히 버전을 매뉴얼과 맞추지 않았다.(필자는 호환을 보증하지 않는다고 표기된 패키지 버전만 신경씀) Partition Setting \u0026amp; mount 만약 설치시 LFS RootFS 및 SWAP 파티션을 설정하지 않았다면 파티션을 만들어 준다.\nhttps://www.linuxfromscratch.org/lfs/view/stable-systemd/chapter02/creatingfilesystem.html 필자는 하이퍼바이저 위에 우분투20.04를 설치했으므로 이미 Swap 공간이 /swapfile파일에 생성되어 있다. free 명령을 통해 swap공간의 존재를 확인할 수 있다. 그러므로 필자는 메뉴얼과는 다르게 별도 스왑 파티션을 생성하지 않았다. ~만약 swap 용량이 모자란다면 스왑파일 용량을 키우거나 스왑 파티션을 생성하자~ 스왑 파티션이 없으면 나중에 LFS로 부팅했을 때 swap영역이 없으므로 만들어 주는게 좋을 듯 하다. 아님 향후 LFS용 swapfile을 만들자\nLFS RootFS 파티션과 Swap 파티션을 생성한다. RootFS : fdisk 에서 커맨드 n(생성) -\u0026gt; p(primary) 로 생성 (필요시)swap : fdisk 에서 커맨드 n(생성) -\u0026gt; p(primary) 로 생성 후 t(type) -\u0026gt; 코드 82 w로 저장 해당 파일 시스템으로 포맷 1 2 sudo mkfs -v -t ext4 /dev/${RootFS_파티션} (필요시)sudo mkswap /dev/${swap_partition} 전역변수 설정 및 파티션 마운트 \u0026lsquo;mount -l\u0026rsquo; 명령으로 nosuid나 또는 nodev가 아닌지 확인하자. 1 2 3 4 export LFS=/mnt/lfs sudo mkdir -pv $LFS sudo mount -v -t ext4 /dev/sda2 $LFS sudo cat \u0026#34;/dev/sda2 /mnt/lfs ext4 default 1 1\u0026#34; \u0026gt;\u0026gt; /etc/fstab 패키지와 패치 다운로드 기본적인 리눅스 시스템을 구축하기 위해 다운로드 해야 하는 패키지와 패치를 다운로드 한다. (아마 빌드를 위한 sysroot 구축이라 생각되는데?? 아직은 확실하지 않음)\nsource 저장을 위한 폴더 생성 및 권한 설정 패키지 및 패치 다운 패키지와 패치를 다운로드 할때 수동으로 하나씩 받기 힘드므로 리스트 파일을 제공한다. 정상적으로 다운 받았는지 확인하기 위한 md5 체크섬 확인 체크섬 확인을 위한 파일도 제공하므로 이를 이용 1 2 3 4 5 6 7 8 sudo mkdir -v $LFS/sources sudo chmod -v a+wt $LFS/sources wget --directory-prefix=$LFS/sources https://www.linuxfromscratch.org/lfs/view/stable-systemd/wget-list-systemd wget --input-file=wget-list-systemd --continue --directory-prefix=$LFS/sources wget --directory-prefix=$LFS/sources https://www.linuxfromscratch.org/lfs/view/stable-systemd/md5sums pushd $LFS/sources md5sum -c md5sums popd 다운로드한 패키지와 패치의 리스트는 다음 페이지에 나와 있다.\n패키지 : https://www.linuxfromscratch.org/lfs/view/stable-systemd/chapter03/packages.html 패치 : https://www.linuxfromscratch.org/lfs/view/stable-systemd/chapter03/patches.html 최종 준비 크로스 컴파일을 위한 유저 추가 및 bash 환경 설정. 각 절차에서 나타난 옵션에 대한 자세한 설명은 매뉴얼 페이지를 참조한다.\n최소 LFS Filesystem layout 구성 LFS의 빌드를 위한 크로스 컴파일러 생성 시 필요한 최소한의 Filesystem layout을 구축한다.(필자의 추론) 크로스 컴파일용 glib이나 libstdc++등이 여기에 설치되고 LFS에 포함되는 시스템은 상기 폴더에 덮어써진다. Filesystem Layout을 위한 심볼릭 링크도 만들고 그 후 크로스 컴파일러가 설치 될 폴더를 생성한다.(이는 향후 LFS 시스템에 포함되지 않으므로 분리해놓음)\n1 2 3 4 5 6 7 8 sudo mkdir -pv $LFS/{etc,var} $LFS/usr/{bin,lib,sbin} for i in bin lib sbin; do ln -sv usr/$i $LFS/$i done case $(uname -m) in x86_64) mkdir -pv $LFS/lib64 ;; esac sudo mkdir -pv $LFS/tools LFS빌드를 위한 유저 추가 빌드를 위한 환경 구성을 위해 별도의 유저를 추가\nlfs 유저추가 및 비번 설정 생성된 파일 시스템의 소유자를 lfs로 변경 /etc/bash.bashrc 이름 변경 (우분투20.04의 경우 lfs로 로긴 시 해당 파일이 쉘 환경을 변화시키는 것을 막기 위함) lfs로 로긴 1 2 3 4 5 6 7 8 9 sudo groupadd lfs sudo useradd -s /bin/bash -g lfs -m -k /dev/null lfs chown -v lfs $LFS/{usr{,/*},var,etc,tools} chown -vh lfs $LFS/{bin,lib,sbin} case $(uname -m) in x86_64) chown -v lfs $LFS/lib64 ;; esac sudo mv -v /etc/bash.bashrc /etc/bash.bashrc.NOUSE su - lfs lfs 유저 환경설정 lfs 유저를 위환 쉘의 환경을 설정한다. 설정에 대한 상세 내용은 매뉴얼을 참조한다.\n.bash_profile은 Login Shell에서 실행(ex. ssh 로그인 쉘)되며 하기와 같이 설정 1 2 3 cat \u0026gt; ~/.bash_profile \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; exec env -i HOME=$HOME TERM=$TERM PS1=\u0026#39;\\u:\\w\\$ \u0026#39; /bin/bash EOF .bashrc은 Non-Login Shell에서 실행(ex. 로그인된 상태에서 쉘이 실행될 때)되며 하기와 같이 설정 1 2 3 4 5 6 7 8 9 10 11 12 cat \u0026gt; ~/.bashrc \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; set +h umask 022 LFS=/mnt/lfs LC_ALL=POSIX LFS_TGT=$(uname -m)-lfs-linux-gnu PATH=/usr/bin if [ ! -L /bin ]; then PATH=/bin:$PATH; fi PATH=$LFS/tools/bin:$PATH CONFIG_SITE=$LFS/usr/share/config.site export LFS LC_ALL LFS_TGT PATH CONFIG_SITE EOF 프로파일을 적용한다. 1 source ~/.bash_profile ","date":"2023-03-12T00:00:00Z","permalink":"https://muonkmu.github.io/p/lfs_prj-02-host-setting/","title":"[LFS_prj] 02 Host setting"},{"content":"리눅스의 구조를 조금 더 알아보고자 LFS를 진행해보기로 했다. 리눅스를 부트로더부터 쉘까지 구축하는 프로젝트인데 수행하고 나면 리눅스 패키지의 디펜던시를 알 수 있다고 한다. 즉, Roll Your Own(RYO) 기법으로 리눅스를 구축하는 것이다.\n목표 현재 LFS의 버전은 11.3 이며 stable 버전은 init프로그램의 종류에 따라 sysvinit 버전과 systemd 버전으로 나뉜다. 요즘 추세에 따라 LFS v11.3-systemd 를 구성하는 것을 목표로 한다.\n개발 환경 VMware를 환경에서 우분투를 설치하여 진행하였다.\n노트북에 우분투가 깔려 있었지만 LFS 마운트용 파티션 분할이 SSD 파편화 문제로 실패해서.. 다른 글들을 보면 ubuntu LiveCD boot 환경에서 진행할 수 있다고 하는데 필자는 그냥 우분투를 설치하였다. 파티션 구성은 하기와 같다.\nsda1(primary 25Gb) : ubuntu 20.04 sda2(primary 35Gb) : LFS mount 참고 공식 매뉴얼 : https://www.linuxfromscratch.org/lfs/view/stable-systemd/ 번역서(9.1버전) : https://rawcdn.githack.com/NuttyJamie/LinuxFromScratch-for-Korean/88bdabae8abf2fad511b497b0dc676e6ac95b965/9.1/BOOK/HTML/index.html 참고 자료 : http://soopsaram.com/lfs/ ","date":"2023-03-04T00:00:00Z","permalink":"https://muonkmu.github.io/p/lfs_prj-01-lfs-project-propsal/","title":"[LFS_prj] 01 LFS project propsal"},{"content":"SCNN Accelerator SCNN은 sparse encoding scheme을 이용해서 activation / weight sparsity 지원 Planar Tiled-Input Stationary-Cartesian Product-sparse (PT-IS-CP-sparse)라 부르는 새로운 Cartesian product flow를 제안 (activation / weight reuse) SCNN PT-IS-CP-Dense Dataflow PT-IS-CP-Dense dataflow는 convolution nested loop를 어떻게 분해할 것인가에 관한 것\nC X R X S 형태의 K개 filter, batch size N인 C X W X H 형태의 input activation 일 때 Input Stationary (IS) 가 적용되면 loop order는 C→W→H→K→R→S 가 됨 성능향상을 위해 blocking strategy 적용 (K output channel은 $K_c$ 사이즈의 K/$K_c$ output channel group으로 분리) K/$K_c$→C→W→H→$K_c$→R→S intra-PE parallelism을 위해 PE 내부에서 spatial reuse 활용\nfilter weight(F)와 input activation(I)가 각 buffer에서 fetch되고 이는 F X I array 곱셈기로 전송 filter weight와 input activation은 재활용 되며 partial sum은 향후 연산을 위해 메모리 접근 없이 저장됨 intra-PE parallelism을 위해 Spartial tiling 전략이 사용됨\nW X H input activation는 $W_t$ X $H_t$ Planar Tiles(PT)로 나눠져서 PE로 분배됨 또한 mutiple channel processing 지원 (C X $W_t$ X $H_t$이 PE에 할당됨) sliding window operation에서 edge에서 cross-tile dependency가 생기는데 data halo를 이용해 해결\nInput Halos : PE input buffer는 halo을 수용하기 위해 C x Wt x Ht보다 약간 큰 크기로 조정 Output Halos : PE accumulation buffer도 halo을 수용하기 위해 Kc x Wt x Ht보다 약간 큰 크기로 조정. Halo에는 출력 채널 계산이 끝날 때 누적을 완료하기 위해 인접 PE와 통신하는 불완전한 부분 합계가 포함. PT-IS-CP-Dense Dataflow의 최종 수식은 다음과 같다 SCNN PT-IS-CP-Sparse Dataflow PT-IS-CP-Sparse는 PT-IS-CP-Dense dataflow에서 파생되었고 filter weight와 input activation의 sparsity를 지원 filter weight는 Kc X R X S sparse block으로 압축, input activation은 Wt X Ht 사이즈 블럭으로 엔코딩 PE는 nonzero F 와 nonzero I를 곱해서 partial sum은 accumulator buffer에 output index와 저장됨 PT-IS-CP-Sparse는 compressed sparse index input activation / filter weigth / accumulator buffer를 패치 할 수 있도록 수정됨 SCNN Tiled Architecture SCNN은 Tiled architecture로 PT-IS-CP-Sparse를 지원\nPE는 halo를 교환 하기 위해 인접 PE와 연결되며 Layer Sequencer는 PE와 DRAM의 데이터 이동을 제어 Processing Element Architecture PE는 weight buffer, input/output activation RAM (IARAM/OARAM), multiplier array, scatter crossbar, accumulation buffer, Post-Processing Unit (PPU)으로 구성\ninput activation과 filter weight가 PE로 로드되고 multiplier array가 partial sum을 계산 후 acculumlation buffer에 저장 acculumlation buffer는 adder와 output channel entry를 가지고 있으며 double buffers 전략 사용 1개 버퍼는 partial sum을 계산 하고 다른 것은 output을 후처리를 위해 PPU로 전송 PPU는 몇 가지 다른 task를 수행 halo 영역을 위해 인접 PE와 partial sum을 교환 nonlinear activation, pooling, dropout 수행 output activation을 압축하고 ORAM에 씀 Data Compression filter weight와 input/output activation을 압축하기 위해 다른 것과 약간 수정된 엔코딩 방식 사용(책의 그림 참조)\nData vector : 0이 아닌 element 저장 index vector : 0이 아닌 element의 갯수와 이전에 0인 element의 갯수를 저장 SeerNet Accelerator Microsoft SeerNet은 quantizaition convolution을 이용해서 feature map sparsity를 예상하는 방법을 제안\nFeature Map(F)와 filter weight(W)는 Fq와 Wq로 양자화 되고 이를 이용해 quantized low bit inference를 수행하여 binary sparsity mask(M)을 생성 그리고 full precision sparse inference를 수행(앞의 M을 이용하는 듯) Low-Bit Quantization Low-Bit Quantization은 online/offline에서 filter weight를 양자화\noutput feature map의 dimenstion이 H*W 일 때 양자화 복잡도는 1/(HW) online 동작은 낮은 연산 복잡도와 오버헤드로 병렬처리를 제공하고 offline 동작은 추가 저장공간으로 양자화 오버헤드를 제거함 online quantization동안 binary mask 생성을 위한 quantized convolution이 수행되고 이 마스크를 가지고 spase convolution이 수행 됨 Efficient Quantization Full quantization 대신에 layer-by-layer quantization에 집중하고 output feature map을 예측하기 위한 low-bit quantization 적용\nReLU의 경우 output feature map의부호를 찾고 음수을 0으로 출력 시킴 Max pooling의 경우 정확도 없이 output feature map의 가장 큰 값만 찾음 Quantization flow 하기와 같음\nn-1이 양/음의 범위를 모두 커버하는 양자화 레벨 2n-1을 정의 최대 절대값 M을 찾음 양자화 값 x\u0026rsquo; = floor(X/M*2^(n-1)) Quantized Convolution 책에 Quantized Convolution, Quantized ReLU activation, Quantized batch normalization 수식 있음\nInference Acceleration Inference 성능향상을 위해 Intel AVX2 vector 연산 사용\nSparsity-Mask Encoding sparse convolution 성능 향상을 위해 row/column index vector를 이용해 sparsity mask를 엔코딩\nfeature map을 vector format으로 변환(다수의 feature map은 matrix 형태가 됨) column index에는 sparse bit의 column 위치 row index 에는 각 row column의 시작위치가 있음(책에 그림 볼 것) ","date":"2023-03-01T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-2/2/","title":"[AI HW Design] Chap08 Network Sparsity (2/2)"},{"content":"개인 프로젝트를 진행하는데 Xilinx에서 제공하는 레퍼런스의 개발환경이 제각각이다. 우선 기본적으로 IDE 2019.2 버전이 필요한데 현재 데스크탑에 설치되어 있는 개발환경은 2022.1이다. 재 설치를 하기 보다는 Docker로 가상화 공간에 별도의 개발환경을 설치해 보자\nDocker GUI How to use a GUI on the docker Docker에서 개발환경을 가상화 하는 것은 좋은데 Docker는 기본적으로 CLI 환경만 제공한다. 그러나 많은 임베디드 개발환경은 설치 및 실행에서 GUI를 필요로 하기에 Docker에서 GUI를 구성하는 방법은 아래와 같다.\ndesktop 환경이 설치된 Docker에서 VNC 이용 SSH 기반에서 X11 Forwarding을 이용해 HOST에서 GUI를 띄운다. 1번째 방법은 GUI docker image를 다운받아 설정하고 여기에 ssh의 ForwardX11 옵션을 활성화 하면 2번째 방법도 지원이 된다.\nDocker image 빌드 및 실행 선구자가 LXDE 환경의 Docker를 개발하였다. 하기 사이트에서 사용법을 읽어보자.\nhttps://hub.docker.com/r/dorowu/ubuntu-desktop-lxde-vnc/ 위의 이미지를 그대로 사용해도 되지만 시간 설정 등 몇가지 설정이 다르므로 이미지를 빌드하자. 아래는 필자의 개발환경 구성을 위한 dockerfile이다. 필자는 기본쉘로 zsh를 사용하지만 petalinux는 bash 환경에서 실행해야 함을 주의하자.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 #from ubuntu20.04-vnc-dockerfile FROM dorowu/ubuntu-desktop-lxde-vnc:latest # timezone setup ARG DEBIAN_FRONTEND=noninteractive ENV TZ=Asia/Seoul # package add RUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add - \\ \u0026amp;\u0026amp; apt-get update \\ \u0026amp;\u0026amp; apt-get install -y vim git openssh-server zsh fonts-powerline tzdata tio byobu language-pack-en language-pack-ko \\ \u0026amp;\u0026amp; update-locale \\ \u0026amp;\u0026amp; apt autoclean -y \\ \u0026amp;\u0026amp; apt autoremove -y # xilinx user add and password setup, and usb dev group add RUN useradd -m -s /bin/zsh xilinx \\ \u0026amp;\u0026amp; echo \u0026#39;xilinx:xilinx\u0026#39; | chpasswd \\ \u0026amp;\u0026amp; echo \u0026#34;xilinx ALL=(ALL:ALL) ALL\u0026#34; \u0026gt;\u0026gt; /etc/sudoers \\ \u0026amp;\u0026amp; usermod -a -G dialout xilinx \\ \u0026amp;\u0026amp; usermod -a -G plugdev xilinx EXPOSE 80 WORKDIR /home/xilinx ENV HOME=/home/xilinx \\ SHELL=/bin/zsh HEALTHCHECK --interval=30s --timeout=5s CMD curl --fail http://127.0.0.1:6079/api/health ENTRYPOINT [\u0026#34;/startup.sh\u0026#34;] ~ 도커를 실행해보자. 하기 파일은 필자의 실행 스크립트이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 #!/bin/bash CON_UBUNTU_VER=\u0026#34;20.04\u0026#34; CON_NAME=\u0026#34;xilinx-dev\u0026#34; CON_TAG=\u0026#34;2023-3\u0026#34; CON_USR_NAME=\u0026#34;xilinx\u0026#34; CON_USR_PWD=\u0026#34;xilinx\u0026#34; MAP_PORT_SSH=\u0026#34;8091\u0026#34; MAP_PORT_VNC=\u0026#34;8092\u0026#34; MAP_PORT_HTTP=\u0026#34;8093\u0026#34; VNC_PWD=\u0026#34;xilinx\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;######################################################\u0026#34; echo \u0026#34;ubuntu $CON_UBUNTU_VER LXDE Base Docker\u0026#34; echo \u0026#34;$CON_NAME platform : xilinx-ide \u0026#34; echo \u0026#34;Port SSH: $MAP_PORT_SSH, VNC: $MAP_PORT_VNC, HTTP: $MAP_PORT_HTTP\u0026#34; echo \u0026#34;######################################################\u0026#34; echo \u0026#34;\u0026#34; echo -n \u0026#34;Select Command 1)Run-Docker 2)Attach-Bash 3)Remove-Docker : \u0026#34; read selCmd if [ $selCmd = \u0026#34;1\u0026#34; ]; then echo -n \u0026#34;Select Resolution 1)FHD 2)QHD 3)UHD 4)Manual : \u0026#34; read selRes if [ $selRes = \u0026#34;1\u0026#34; ]; then resVal=\u0026#34;1920x1080\u0026#34; elif [ $selRes = \u0026#34;2\u0026#34; ]; then resVal=\u0026#34;2560x1440\u0026#34; elif [ $selRes = \u0026#34;3\u0026#34; ]; then resVal=\u0026#34;3840x2160\u0026#34; elif [ $selRes = \u0026#34;4\u0026#34; ]; then echo -n \u0026#34;Input Resolution ex)1920x1080 : \u0026#34; read resVal else resVal=\u0026#34;1920x1980\u0026#34; echo \u0026#34;Undefined Num, Set resolution FHD\u0026#34; fi sudo docker run -d --privileged \\ -p $MAP_PORT_HTTP:80 -p $MAP_PORT_VNC:5900 -p $MAP_PORT_SSH:22 \\ -e VNC_PASSWORD=$VNC_PWD -e RESOLUTION=$resVal -e USER=$CON_USR_NAME -e PASSWORD=$CON_USR_PWD \\ -v /dev/shm:/dev/shm -v /dev/bus/usb:/dev/bus/usb -v $HOME/Workspace:/home/$CON_USR_NAME/Workspace \\ --name $CON_NAME $CON_NAME:$CON_TAG elif [ $selCmd = \u0026#34;2\u0026#34; ]; then sudo docker exec -it $CON_NAME /bin/zsh -c \u0026#34;su - $CON_USR_NAME\u0026#34; elif [ $selCmd = \u0026#34;3\u0026#34; ]; then sudo docker stop $CON_NAME sudo docker rm $CON_NAME else echo \u0026#34;Input the collect decimal number\u0026#34; fi 이후 Docker의 VNC에 연결하여 진행한다. 기본적으로 필요한 패키지들을 설치하고 만약 .bashrc가 없어서 터미널의 색깔이 이상하다면 해당파일을 복사한다.\n1 cp /etc/skel/.bashrc $HOME Petalinux Install 사전준비 필요 시 /bin/sh가 bash가 아니라면 변경한다. 하기 명령을 수행 후 no을 선택하면 bash로 변경됨 1 sudo dpkg-reconfigure dash 필요 패키지들을 설치한다. https://docs.xilinx.com/v/u/2019.2-English/ug1144-petalinux-tools-reference-guide 1 2 3 sudo dpkg --add-architecture i386 sudo apt update gawk make net-tools libncurses5-dev tftpd zlib1g:i386 libssl-dev flex bison libselinux1 gnupg wget diffstat chrpath socat xterm autoconf libtool tar unzip texinfo zlib1g-dev gcc-multilib build-essential screen pax gzip python2.7 cpio 필요 시 locale에 en_US.utf8을 추가한다. 1 2 sudo apt-get install -y locales sudo locale-gen en_US.utf8 Install Petalinux 설치파일을 다운로드 하고 설치한다. sudo 권한 없이 설치해야하므로 설치폴더의 소유자는 현재 user로 한다.\n1 2 3 4 sudo mkdir /tools/Xilinx/Petalinux/2019.2 sudo chown -R xilinx:xilinx /tools/Xilinx/Petalinux chmod +x ${설치파일} ./${설치파일} /tools/Xilinx/Petalinux/2019.2 Vivado Install 설치파일을 다운로드 하고 설치를 진행한다.(sudo 권한으로 설치?) 필요 시 libtinfo5f를 설치한다. 1 2 3 chmod +x ${설치파일} sudo ./${설치파일} sudo apt-get install libtinfo5 Backup and import 레이어 병합을 위하여 docker의 export 명령으로 출력 시칸다. 1 sudo docker export --output=\u0026#34;${파일이름}\u0026#34; ${컨테이너명} 백업된 파일을 입력시키고 싶다면 다음을 수행한다. 1 sudo docker import -c \u0026#39;EXPOSE 80\u0026#39; -c \u0026#39;WORKDIR /root\u0026#39; -c \u0026#39;ENV HOME=/home/ubuntu SHELL=/bin/bash\u0026#39; -c \u0026#39;HEALTHCHECK --interval=30s --timeout=5s CMD curl --fail http://127.0.0.1:6079/api/health\u0026#39; -c \u0026#39;ENTRYPOINT [\u0026#34;/startup.sh\u0026#34;]\u0026#39; 아카이브.tar 이미지이름:tag Etc ssh 설치 후 자동 실행되지 않는다면 /startup.sh 파일에 ssh 서비스 스타트 명령을 기입한다. 상기 파일의 마지막 줄은 헬스체크 프로세스 시작 명령이므로 이 위에 기입하면 된다. ","date":"2023-02-28T00:00:00Z","permalink":"https://muonkmu.github.io/p/xilinx-ide-installation-based-on-docker/","title":"Xilinx IDE installation based on Docker"},{"content":"8장은 시스템 throughput을 높이기 위해 ineffectual zoro operation을 스킵하는 다양한 구조에 대해 알아본다. (feature map encoding/indexing, weight sharing/pruning, quantized prediction)\nEnergy Efficient Inference Engine (EIE) 스탠포드 대학에서 나온 유명한 논문, 알고리즘(SW) + 가속기(SW) 양쪽에 대해 최적화한 논문으로 알고 있다. 중요도에 비해 설명이 부실한 듯\u0026hellip;\nEIE는 sparse matrix-vector mutiplication을 위한 compressed network model을 지원하고 weight sharing을 다룸 Leading Non-Zero Detection Network, Central Conrol Unit, Processing Element 로 구성 Leading Non-Zero Detection Network (LNZD) LNZD는 input activation으로 부터 nonzero element를 찾아내고 이를 LNZD node로 먹임 node는 nonzero value와 index를 PE로 브로드캐스팅 함 LNZD에 PE는 4개가 연결 Central Control Unit (CCU) CCU는 network segment computation을 제어\nhost와 커뮤니케이션 하고 PE state를 모니터링 함 두가지 동작모드로 나뉨 computing mode : CCU는 LNZD로 부터 nonzero input activation을 받고 이를 PE로 브로드캐스팅, 모든 input channel이 스캔될때 까지 반복됨 I/O mode : PE는 idle, activation과 weight가 DMA에 의해 접근 됨 Processing Element (PE) 요약하면 ifmap 값을 읽어와서 여기 포인터를 이용하여 해당 weight 값과 효율적으로 곱한다는 이야기인듯..\nPE는 activation queue, pointer read unit, sparse matrix unit, arithmetic unit, activation R/W unit으로 구성 computation 동안 CCU는 nonzero input element와 index를 activation queue에 브로드캐스팅 함, PE는 큐가 다차면 input element를 처리(브로드캐스트는 중지) activation queue는 PE가 work backlog를 구축할 수 있도록 해줌(load balancing 문제 해결) pointer read unit은 activation queue의 인덱스를 이용하여 nonzero element의 시작/종료 포인터를 찾음 싱글 사이클에 이를 처리하기 위해 포인터는 LSB와 함께 odd/even SRAM에 저장(의미를 잘 모르겠음) sparse matrix read unit은 sparse matrix memory로 부터 포인터를 이용하여 0이 아닌 값을 읽음(fmap?) arithmetic unit은 activation queue와 sparse matrix memory의 0이 아닌 값 MAC 연산 연속적으로 가산기 사용되는 경우를 위한 bypass 경로 존재 (그림 보자) activation read/write unit의 경우 fully connected layrer를 위한 source/destination activation register를 가지고 있으며 이는 다음 레이어 연산시 교체됨 Deep Compression EIE는 pruning과 weight sharing을 통해 네트워크 압축하기위해 Deep Commpression을 적용, 적용 예는 다음과 같음 (책에 그림 예시를 보자) 4X4 weight matrix라면 16개의 값을 4개의 index(code book)로 만듬 index에 해당하는 weight는 Gradient를 가지고 fine-tunnig됨 MAC 연산은 weight와 input activation vector가 0이 아닌 값에 대해서 수행 EIE는 interleaved Commpressed Sparse Column(CSC) 적용 (Eyeriss와 약간 다르므로 책참조) v는 0이 아닌 값, z는 0이 아닌 값 해당 v 값 앞에 0의 개수 v,z는 하나의 large array pair에 $p_j$(벡터의 시작 포인터), $p_{j+1}$(마지막 항목 다음 번 포인터)와 함께 저장됨 Sparse Matrix Computation 4개의 PE에서 input activation vector(a)는 weight matrix(w)와 곱해짐 a를 스캔해서 0이 아닌 $a_j$는 인덱스 값과 함께 브로드캐스팅 됨 PE는 index에 대응하는 0이 아닌 $w_j$를 곱합 PE는 벡터 v를 $p_j$에서 $p_j+_1$까지만 스캔 (책에 예시 그림 있으니 참조) Cambricon-X Accelerator 병렬연산에서 Nonzero neuron을 선택하기 위해 indexing scheme을 적용 Control Processor (CP), Buffer Controller (BC), Input Neural Buffer (NBin), Output Neural Buffer (NBout), Direct Memory Access (DMA) Module, Computation Unit (CU)으로 구성 중요 element는 BC Tn indexing unit(nonzero neuron을 인덱싱하는 유닛이며 PE와 수가 같다) Computation Unit (CU) CU는 Tn개 PE로 구성되며 모든 PE는 fat tree 형태로 연결 PE는 PE Functional Unit(PEFU)와 Synapse Buffer(SB)로 구성 BC로 부터 neuron을, local BC로 부터 synaps를 읽어서 PEFU에 제공하며 output neuron은 BC에 다시 씀 Tn개 PEFU는 Tm개 곱셈기와 가산기를 가져서 TnXTm 곱셈이 가능 SB는 synapse를 저장하고 메모리 access를 최소화 하기 위해 디자인, 책에서는 하기 예시를 듬(책그림 참조) PE는 4개이고 output neuron 0이 input neruon 2개 연결, output neuron 1이 input neruon 5개 연결 output neuron 0의 weight는 address 0에 output neuron 1의 weight는 address 1/2의 SB에 저장 output neuron 0 계산 시 SB를 1번 읽고 output neuron 1은 두 번 읽음 synapse의 수는 output neuron마다 다를 수 있기 때문에 SB가 비동기적으로 데이터를 로드하여 전체 성능을 향상 Buffer Controller BC는 Indexing Module (IM)과 BC Functional Unit (BCFU) 로 구성 BCFU는 인덱싱을 위해 neuron을 저장 IM은 BC의 nonzero nueron을 구분하고 nonzero indexed nueron만 전송 BC는 input neurons을 NBin에서 PE로 보내거나 BCFU로 제공, PE의 계산결과는 BCFU에 저장 또는 NBout에 쓰여짐 IM에는 두가지 하기 두가지 옵션이 있음 (책에 그림에 잘 나와 있음) direct indexing : nonzero nueron의 여부를 0/1로 표현한 binary string 사용 step indexing : nonzero nueron의 거리를 사용 step indexing이 area와 power를 적게 소모함 ","date":"2023-02-25T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap08-network-sparsity-1/2/","title":"[AI HW Design] Chap08 Network Sparsity (1/2)"},{"content":"6장은 In-Memory Computation, 7장은 Near-Memory Computation 에 대한 내용이다. 현재 개발하고자 하는 가속기와 동떨어지는 내용이라 판단해서 개요만 보고 스킵할 예정이다. 삼성이나 하이닉스를 다녀야 쓸모있지 않을까 싶다.\nIn-Memory Computation 여기서는 메모리와 로직을 stacking 하는 방식의 Processor-In-Memory(PIM)에 대해 설명한다. 다른 방식의 PIM도 있는 걸로 아는데\u0026hellip;\nNeurocube Architecture Nerocube는 parallel neural processing unit과 High Bandwidth Memory(HBM)을 스택한 Hybrid Memory Cube(HMC)를 이용 이는 stacked memrory에서 PE로 직접 데이터 로드가 가능함(레이턴시 감소) Teris Accelerator Teris는 Eyeriss의 3D Memory와 함께 Row Stationary(RS) dataflow 채택 NeuroStream Accelerator NeuroStream은 HMC의 modular extension인 Smart Memory Cube(SMC)를 이용 Near-Memory Computation DiDianNao Supercomputer 대용량 eDRAM을 통해 DianNao의 memory bottleneck을 해결하고자 함 모든 synapse를 수용할 후 있는 거대 storage를 제공하는 Neural Function Unit(NFU)를 지닌 16개의 tile로 구성 NFU는 4개의 eDRAM bank와 time-interleaved 통신(?) 함 (eDRAM의 레이턴시가 크기 때문) Cnvlutin Accelerator DiDianNao에서 파생되었으며 다수의 DiDianNao를 고속 인터페이스로 연결, 거대 parallel mutiplication lane 구조 채택 ","date":"2023-02-21T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap06-07-in/near-memory-computation/","title":"[AI HW Design] Chap06 \u0026 07 In/Near Memory Computation"},{"content":"앞의 포스트가 너무 길어져서 Eyeriss version2 부분은 현 포스트로 나눔\nEyeriss Accelerator Ver.2 irregular data pattern 과 network sparsity 지원을 위한 Eyeriss V2 공개\n새로운 NoC 구조 : data reuse가 적을 땐 external memory에서 더 많은 데이터를 PE로 가져오고 data reuse가 많을 땐 spartial Data를 sharing CSC 엔코딩 적용 RS+ 적용 Eyeriss V1이 GLB\u0026lt;-\u0026gt;PE 연결을 위해 flat multicast NoC를 사용했지만 V2에서는 flexible and mesh NoC 사용, 이 hierarchical mesh는 GLB cluster, Router Cluster, PE cluster로 구성되며 하기 3가지 타입의 데이터 이동 지원\nifmap은 GLB cluster에 로드됨, 이는 GLB 메모리에 저장되고 Router Cluster로 전송 psum은 GLB 메모리에 저장 되고, ofmap은 external memory에 바로 저장 됨 fmap은 Router Cluster로 전송되고 PE spad에 저장 (책에 v1 과 v2 구조에 대한 비교 그림이 있으니 찾아보자) Hierarchical Mesh Network (HM-NoC) 전통적인 Network-on-Chip 구조는 하기와 같으며 장단점을 가지고 있음 (책 그림 참조)\nBroadcast Network : high reuse / low bandwidth Unicast Network : high bandwidth / low reuse All-to-All Network : high reuse, high bandwidth / scale difficulty Eyeriss V2는 RS+를 지원하기 위해 HM-NoC 구조를 제안, all-to-all network에서 파생되었으나 4가지 모드를 가짐\nBroadcast: single input and single weight Unicast: multiple inputs and multiple weights Grouped multicast: shared weights Interleaved multicast: shared inputs HM-NoC는 source, destination, router로 구성되며 design phase에서 cluster로 그룹핑되고 operation mode에서는 고정됨.\nRouter cluster가 다른 cluster와 one-to-one, many-to-many, source/destination 구조로 연결 Router cluster는 4개의 source/destination port를 가지며 4가지 routing mode (broadcast, unicate, grouped/interleaved multicast)를 가짐 책에서는 다음과 같이 예시를 듬\nConvolution layer : ifmap과 fmap이 reuse 되며 grouped multicast 또는 interleaved mode 로 구성 Depth-wise Convolution layer : fmap만 reuse 되며 fmap이 PE로 broadcast, ifmap은 GLB에서 로드 Fully connected layer : ifmap이 모든 PE로 broadcast, fmap은 unicast mode로 로드 Input Activation HM-NoC Router Cluster 안의 3개 ifmap router는 GLB cluster의 ifmap SRAM과 연결 ifmap routerd의 3개의 source/destination port 다른 클러스터와 연결, 1개는 메모리에서 데이터로드, 1개는 PE 연결 책에 그림과 상세 설명이 있으니 참조하자 Filter Weight HM-NoC Router Cluster 안의 각 fmap router는 PE cluster 안의 PE row와 연결 vertical mesh는 사라지고 horizontal mesh 만 데이터 reuse를 위해 남음 책에 그림과 상세 설명이 있으니 참조하자 Partial Sum HM-NoC Router Cluster 안의 4개의 psum router는 GLB cluster의 psum SRAM과 PE cluster의 PE column과 연결됨 horizontal mesh는 사라지고 vertical mesh만 psum accumulation을 위해 남음 책에 그림과 상세 설명이 있으니 참조하자 Compressed Sparse Column (CSC) Format Eyeriss V2는 ifmap과 fmap 둘 다에 CSC 포맷 적용 (zero operation skipping), CSC 포맷의 구조는 다음과 같다\nData vector : 0이 아닌 값 Counter vector : Data vector의 item 기준, 해당 열에서 앞에 있는 0의 값의 갯수 Address vector : 각 열을 기준으로 이전 열까지 0이 아닌 item의 누적 갯수 Eyeriss V2는 PE는 zero operation skip을 위해 7 pipeline stage와 5 spad (ifmap, fmap, psum 저장)로 수정\n첫째로 non-zero data인지 확인하기 위해 address를 검사하고 fmap 로드 전 먼저 ifmap을 로드(zero ifmap skip을 위해) ifmap/fmap이 0이 아니면 계산 pipeline 수행, fmap이 0이면 pipeline disable Row Stationary Plus (RS+) Dataflow PE utilization을 높이기 위해 RS+ dataflow 적용\nmodel dimension을 다른 PE dimension에 매핑하기 위해 데이터를 tiling, spatial fragmentation 함 depth-wise convolution 시 PE utilization이 낮은 문제점 해결 ","date":"2023-02-19T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-3/3/","title":"[AI HW Design] Chap05 Convolution Optimization (3/3)"},{"content":"유명한 MIT의 Eyeriss Accelerator 논문이다. 아직까지 관련 프로젝트가 진행 중인 것으로 보이며 찾아보면 관련 논문에 대하여 리뷰해논 자료가 꽤 많다. 잘 소개된 곳 한 곳 (허락없는 링크..)\nhttps://m.blog.naver.com/PostView.naver?isHttpsRedirect=true\u0026blogId=kangdonghyun\u0026logNo=220990374125 Eyeriss Accelerator Eyeriss Accelerator는 data access를 최소화하기 위한 Row Stationary (RS) Dataflow를 제안하며 다음과 같은 특징을 지님\nSpartial architecture with sequential processing configuration Spartial architecture can exploit high compute parallelism using direct communication between an array of relatively simple processing engines (PEs). Row Stationary (RS) Dataflow 구현 Four level memory hierarchy : PE scratch pad와 inter-PE 통신을 최대한 이용하고 Global Buffer와 외부 메모리의 data transfer를 최소화 point-to-point \u0026amp; multicast Network-on-Chip(NoC) 아키텍쳐 지원 Run-Length Compression (RLC) 포맷 지원 : zero operation 제거 Eyeriss System Architecture Eyeriss는 과 communication link clock 두가지 clock domain을 가짐 data processing core clock : 12X14 PE array, Global Buffer(GLB), RLC codec, ReLu 배치, PE가 local scratchpad를 이용하여 연산하거나 PE가 인접 PE 또는 GLB와 통신 하는것을 가능하게 함 communication link clock Four level memory hierarchy GLB \u0026lt;-\u0026gt; external memory : asynchronous FIFO 이용 PE \u0026lt;-\u0026gt; GLB : NoC 이용 ReLU \u0026lt;-\u0026gt; RLC codec local temporary data storage using scratchpad 분리된 clock으로 PE는 다른 PE와 같은 클럭 시간에 독립적으로 동작가능하고 link clock은 external memory와 64bit 양방향 버스로 data 전송을 제어 Eyeriss Acc는 Convolution network를 레이어 단위로 진행 첫째로 PE array를 레이어 function/size에 맞게 구성하고 매핑 수행 및 전송 패턴을 결정 input feature map 및 filter map은 external memory에서 PE로 로드되고 output feature map은 다시 external memory로 쓰여짐 2D convolution to 1D multiplication convolution을 수행할 때 2D feature/filter map을 1D로 바꾸어 수행해서 PE에 순차적으로 로딩한다는 이야기를 길게 써놓음 (궁금하면 책을 보자) 2D convolution을 1D vector 와 Toeplitz 행렬(대각선의 성분이 모두 같은 매트릭스)의 곱으로 변환된다 책에 어떤 순서로 feature/filter 1D vector가 PE에 로드/계산되는지 그림으로 있다. Stationary Dataflow 칩에 대한 이야기는 아니고 이전의 stationary 전략에 대해 소개한다. (연산을 어떤 데이터를 고정, 이동 시킬지)\nOutput Stationary Patial Sum의 read/write를 local accumulation을 통해 최소화 Weight Stationary filter map을 local buffer에 두고 계속 활용 Input Stationary input feature map을 local buffer에 두고 계속 활용 다른 전략보다 효율이 안좋은데 약점은 다른 전략보다 convolution연산에 더 많은 cycle이 필요 Row Stationary (RS) Dataflow Eyeriss는 1D vector multiplication 수행하는데 RS dataflow 전략을 사용\nfilter map 행은 PE들에서 수평하게 재사용 input feature map 행은 PE들에서 대각적으로 재사용 partial sum 행은 PE들에서 수직적으로 재사용 RS dataflow에서 데이터는 계산 동안 PE에 저장됨(데이터 이동 최소화) time-interleaved approach를 통해 fmap과 ifmap은 같은 clock cycle 내에서 재활용 계산이 완료되면 Psum은 근접 PE들로 이동(다음 계산을 위해서) Filter Reuse fmap이 spad에 로드 되고 고정된다. 다수의 ifmap도 spad에 로드되고 사슬처럼 연걸됨 Input Feature Maps Reuse ifmap이 먼저 PE에 로드 되고 2개의 fmap은 time-interleaved(연산?) 됨 1개의 ifmap으로 2개의 fmap과 1D 연산 수행 이 것은 전체적인 스피드를 올려주지만 fmap과 psum의 time-interleave 연산을 지원하기 위해 큰 spad가 필요 Partial Sums Reuse fmap/ifmap 둘 다 PE에 로드되며 둘 다 time-interleaved 함 psum은 같은 채널 끼리 합쳐짐 fmap/ifmap 둘 다 PE에 로드되어야 하므로 필요한 spad의 용량이 증가 Run-Length Compression (RLC) ReLU 연산 결과 0 값이 많아 지므로 이는 network의 sparsity가 도입됨 zero computation을 피하기 위해 Eyeriss는 64 bit RLC 포맷을 도입 ([5bit]앞에 값이 0인 element 갯수 + [16bit]0이 아닌 값)*3 + [1bit]마지막 item인지 나타내는 flag 첫번째 layer의 ifmap 값을 제회하고 모든 fmap/ifmap은 RLC 포맷으로 external memory에 저장 External Memory에서 입출력 될 때, RLC encoder/decoder를 통과하게 됨 Glabal Buffer (GLB) external memory와 데이터 전송을 위해 GLB 채택 GLB에는 fmap/ifmap/ofmap/psum 이 저장 GLB는 PE가 연산하는 동안 다음 fmap을 preload Processing Element (PE) Architecture PE는 fmap, ifmap, psum을 위한 3가지 타입의 spad를 가짐 datapath는 3가지 pipeline stage에 의해 구성(spad access, fmap/ifmap multiplication, psum accumulation) 16bit 연산을 사용하며 32bit 연산결과는 16bit로 절삭 값이 O인 ifmap이 발견되면 spad에서 fmap 값을 읽는 것과 연산 로직을 끔(전력소모를 줄이기 위해) Network-on-Chip (NoC) NoC는 GLB와 PE 사이의 데이터 이동을 관리, 하기 2개로 구분 Global Input Network (GIN) : GLB \u0026lt;-\u0026gt; PE 간 single cycle multicast 이용 데이터 전송 Y-Bus는 12개의 X-bus와 연결되며 X-bus는 14개의 PE가 연결 top level controller는 \u0026lt;row,col\u0026gt; 태그를 생성하고 Y-bus / PE의 Multicast controller가 tag를 비교하여 데이터 전송 결정 책에 AlexNet의 예시 있음 Global Output Network (GON) : 별다른 설명 없음 ","date":"2023-02-18T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-2/3/","title":"[AI HW Design] Chap05 Convolution Optimization (2/3)"},{"content":"Convolution은 90% 이상의 Computing resource를 사용하며, data access를 줄이기 위해 feature maps reuse / filter weights reuse / partial sum reuse 같은 전략이 사용된다. 이번 챕터에서는 filter decomposition과 Row Stationary(RS) flow를 설명한다.\nDeep Convolution Neural Network Accelerator (DCNN) DCNN은 클라우드 타겟이 아닌 엣지 디바이스 타겟으로 UCLA에서 개발, 다음과 같은 특징을 지님\nStreaming data flow minimizes data access 병렬 컴퓨팅을 위해 bandwidth 향상이 아닌 Interleaving architecture Large-size filter decomposition supports arbitrary convolution window 추가적인 pooling functional unit을 통한 Convolution Unit(CU)의 workload 감소 System Architecture DCNN의 구성은 다음과 같다\nBuffer Bank 중간 데이터 저장 및 외부 메모리와 데이터 교환 목적 Layer Input 용, Layer Output 용 2가지 셋으로 나누어짐 또한 odd/even channel/feature를 위한 Bank A와 Bank B로 나누어짐(Interleaved) Column Buffer Buffer banck의 데이터를 CU engine의 input data type으로 remap Convolution Unint(CU) engine CU engin은 kernel size 3X3까지 지원하는 16개의 Convolution unit으로 구성 16bit fixed-point 연산 local Prefetch unit이 DMA로 부터 weight/bias 값을 주기적으로 업데이트 함 Accumulation (ACCU) buffer convolution 동안 partial sum 연산 또는 Max pooling 연산 수행 Control Command는 외부 메모리에서 128-depth FIFO로 로드 되며 하기 2가지로 분류됨 configure command : network layer를 구성하고 pooling/ReLU function 활성화 excution command : convolution/pooing 초기화 및 필터 decompose 기술 Filter Decomposition 다양한 필터 커널 사이즈 대응을 위해 3X3 CU engine을 이용하여 filter decomposition 기술 이용 커널 사이즈가 3의 배수가 아니면 zero-padding 이용 convolution 후 결과는 one output feature map으로 재결합 됨 상세 사항은 책 참조 Streaming Architecture 데이터의 이동을 최소화하기위해 Filter Weights Reuse와 Input Channel Reuse 사용\nFilter Weights Reuse 3X3 convolution : filter weight는 CU engine에 저장되고 input feature map이 CU engine으로 공급되며 연산이 완료 될 때까지 filter weight는 업데이트 되지 않음 1X1 convolution : CU unit의 9개 곱셈기 중 7개는 off 되고 2개만 odd/even 채널의 partial sum 계산을 위해 사용 Buffer Bank의 output bandwidth를 최대화하기 위해 구조는 하기와 같다(input cannel 및 Column buffer 구조 이야기 같은데) 16개의 row 데이터는 odd/even data set으로 나뉨 2개의 FIFO는 각 데이터 셋에 페어링 됨 (8개의 row 데이터) 8개의 input row data는 10개의 overlapped data로 매핑 Input Channel Reuse 1X1 convolution을 위해 interleaved architecture 사용 (16개 데이터 셋이 odd/even 채널로 2개 데이터셋 구분됨) 2개의 데이터 셋은 다른 filter weight와 곱해져 32개 output이 나옴 출력은 같은 odd/even 채널 끼리 더해짐 위의 내용과 비슷한 것 같은데 filter weight가 이동하고 input 값이 고정이라는 걸 다시 설명한 듯 Pooling pooling function은 average pooling과 max pooing 이 다른 구조로 분리되어 있음\nAverage Pooling Average Pooling function은 Convolution layer에서 Inpu/Output 채널이 같은 사이즈인 CU엔진에 의해 구현 kernel의 사이즈가 pooling window와 일치하는 대응되는 filter weight는 1/K^2으로 되고 나머지는 0으로 된 후 convolution 수행 Max Pooling Max pooling은 ACCU에서 별도 모듈로 구현 Max pooling 모듈은 scratch pad에 있는 8개의 output 값과 연결되며 이는 다른 stride를 지원하기 위해 MUX와 연결 MUX의 출력은 MAX Pooling 계산기로 가는데 이는 3개의 입력과 1개의 output feedback 입력을 받아 계산하며 인풋이 없어질때까지 연산 반복 Convolution Unit(CU) Engine 3X3 CU engine은 9개의 PE(input feature와 filter weight를 곱함)와 1개의 ADDER로 구성 다른 커널 사이즈 지원을 위해 PE는 On/Off 기능을 가짐 상세 내용은 책 참조 (그림과 3X3일 때, 1X1일때 예시 있음) Accumulation (ACCU) Buffer ACCU는 scratch pad에 partial sums 과 stores output feature maps을 저장 ACCU는 partial product accumulation을 위한 Ping-pong buffer, Max pooling을 위한 temporary storage, readout block으로 구성 (책에 그림 참조) Convolution이 진행되는 동안 1개 버퍼는 덧셈만 하고 다른 하나의 버퍼는 Max pooling을 진행, convolution이 완료된후 각 버퍼의 연결이 switch 됨 Model Compression Model Compression을 하기 위해 training 과정에서 모델을 pruning하고 filter weights를 codebook으로 quantization 함. ","date":"2023-02-12T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/","title":"[AI HW Design] Chap05 Convolution Optimization (1/3)"},{"content":"Target Board는 현재 소유하고 있는 ZCU104를 사용하기로 하고 EVB의 번들 카메라인 See3CAM_CU30를 사용하기로 하였다. 출력은 보드에 DP/HDMI가 있는데 DP는 PS 영역이며 HDMI는 사용자가 PL영역에 구성해야 한다. 그래서 DP로 결정. 선정 사유는 역시 Reference를 구하기 쉽다는것에 있다. 상기 ZCU104 + See3CAM_CU30의 reference design은 xilinx의 Embedded-Reference-Platforms 또는 Zynq-UltraScale-MPSoC-VCU-TRD-2022.1에서 확인할 수 있으나 필자는 봐도 어떻게 구성되어 있는지 잘 모르겠다\u0026hellip;\n기본 지식 See3CAM_CU30은 USB3.0 카메라이므로 리눅스에서 Usb Video Clss (UVC)를 gadget을 사용하여 연결한다. UVC : 웹캠이나 캠코더 같은 비디오 스트리밍이 가능한 장치를 기술하는 USB device class Video4Linux2(v4l2) 비디오 캡쳐 시스템을 위한 디바이스 드라이버의 모음이자 표준 API MIPI/USB camera 카메라등을 지원하는 것으로 봐선 UVC 위에서 표준 추상화 계층을 제공하는 것 같다. 출력은 기본적으로 Frame buffer 및 X11 + DRM KMS 구조를 지닌다. petalinux config 이전 포스트인 zcu104 개발환경 설정에서 다음 드라이버 및 프로그램을 설치한다.\nkernel 커널은 하기 모듈이 필요하다. BSP를 사용했다면 거의 바꿀 것 없지만 petalinux-config -c kernel로 다음 기능을 확인하자.\n카메라 입력 : USB gadget driver, web camera/video driver 모니터 출력 : xilinx DRM KMS driver, frame buffer driver 1 2 3 4 5 6 7 8 9 10 11 12 13 CONFIG_MEDIA_CAMERA_SUPPORT CONFIG_MEDIA_CONTROLLER CONFIG_VIDEO_V4L2_SUBDEV_API CONFIG_VIDEO_ADV_DEBUG CONFIG_MEDIA_USB_SUPPORT CONFIG_USB_VIDEO_CLASS CONFIG_USB_VIDEO_CLASS_INPUT_EVDEV CONFIG_V4L_PLATFORM_DRIVERS CONFIG_VIDEO_XILINX 및 그 외 CONFIG_DRM_XLNX 및 그 외 (필요한지??) CONFIG_USB 및 기타 가젯 필요한거 CONFIG_USB_GADGET_XILINX CONFIG_USB_CONFIGFS 및 그외 (필요한지 잘 모르겠음) RootFS petalinux-config -c rootfs RootFS에는 gstreamer/opencv/x11/v4lutil/gcc 패키지그룹, gstreamer 라이브러리, vim, python3 등을 설치한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 CONFIG_packagegroup-petalinux-gstreamer CONFIG_packagegroup-petalinux-opencv CONFIG_packagegroup-petalinux-x11 CONFIG_packagegroup-petalinux-v4lutils CONFIG_packagegroup-core-buildessential CONFIG_vim CONFIG_python3 CONFIG_python3-shell (?) CONFIG_python3-threading (?) CONFIG_python3-multiprocessing (?) CONFIG_gstreamer1.0 CONFIG_gstreamer1.0-plugins-base CONFIG_gstreamer1.0-plugins-good 카메라 및 프래임 버퍼 테스트 상기 설정으로 빌드 및 부팅 후 USB 캠을 연결한다. 그 후 아래 명령어로 카메라의 정보를 확인 할 수 있다.\nv4l2-ctl --list-devices : 연결된 디바이스 확인 v4l2-ctl -d ${디바이스번호} --all : 카메라 capability 등 모든 정보의 출력 v4l2-ctl -d ${디바이스번호} --list-formats-ext : 지원하는 포멧 확인. DP 포트와 모니터를 연결하면 Frame buffer /dev/fb# 이 생성됨을 확인할 수 있다. fbset명령으로 정보를 조해할 수 있다.\npython기반으로 opencv를 이용해서 카메라의 영상을 캡쳐 및 Framebuffer로 출력해 보자. 하기 코드는 테스트 용으로 카메라 및 프레임 버퍼의 설정 부분은 제외했으므로 출력이 이상할 수 있으니 필요하면 자신의 환경에 맞게 고쳐야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import os import cv2 import time capture = cv2.VideoCapture(0) time.sleep(0.1) # (success, reference) = capture.read() # cv2.imwrite(\u0026#39;${이미지 저장 경로}/${저장 이름}\u0026#39;,reference) while 1 : (ret, capFrame) = capture.read() frame16 = cv2.cvtColor(capFrame, cv2.COLOR_BGR2BGR565) fbframe = cv2.resize(frame16, (1920,1080)) with open(\u0026#39;/dev/fb0\u0026#39;, \u0026#39;rb+\u0026#39;) as buf: buf.write(fbframe) capture.release() cv2.destroyAllWindows() 참고 자료 https://github.com/Xilinx/Embedded-Reference-Platforms-User-Guide/tree/2019.2 https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/2322268161/Zynq+UltraScale+MPSoC+VCU+TRD+2022.1 https://www.hackster.io/whitney-knitter/using-a-usb-web-camera-with-the-minized-5783b1 https://www.e-consystems.com/blog/camera/products/getting-started-with-xilinx-zynq-ultrascale-mpsoc-zcu104-evaluation-kit-and-see3cam_cu30_chl_tc_bx/ https://m.blog.naver.com/overcrash3/120105061216?referrerCode=1 ","date":"2023-02-08T00:00:00Z","permalink":"https://muonkmu.github.io/p/yolo_acc_prj-04-camera-%EC%9E%85%EB%A0%A5-%EB%B0%8F-dp-%EC%B6%9C%EB%A0%A5-%EA%B5%AC%EC%84%B1/","title":"[YOLO_Acc_prj] 04 Camera 입력 및 DP 출력 구성"},{"content":"zcu104 petalinux를 포팅하기 위한 일주일 간 삽질의 기록이다. xilinx에서 제공하는 training reference를 따라하면 간단하지만 이는 SD카드에 커널과 루트파일 시스템을 삽입하는 방법이다. 실제 개발의 편의성을 위해 TFTP 및 NFS를 이용하여 부팅하는 방법을 다룬다.\n목표 하기 boot config를 지원하는 petalinux의 포팅 방법 설명 (vivado/petalinux 2022.1 기반)\njtag로 u-boot까지 다운로드 및 부팅 -\u0026gt; u-boot에서 tftp/pxe로 리눅스 kernel 및 device-tree 로드 명령-\u0026gt; 커널에 의한 NFS로 RootFS 로드 SD카드로 u-boot까지 다운로드 및 부팅 -\u0026gt; u-boot에서 tftp/pxe로 리눅스 kernel 및 device-tree 로드 명령-\u0026gt; 커널에 의한 NFS로 RootFS 로드 SD카드로 u-boot, 리눅스 kernel, device-tree 로드 -\u0026gt; 커널에 의한 NFS로 RootFS 로드 Hardware description config 우선 베이스 프로젝트는 리눅스 포팅이 목적이므로 PS영역만 셋업한다. xilinx에서 제공하는 training reference를 그대로 따라해도 무방하다. (https://xilinx.github.io/Embedded-Design-Tutorials/docs/2022.2/build/html/docs/Introduction/ZynqMPSoC-EDT/3-system-configuration.html) zcu104_bsp에 사용되는 hw config을 보고 싶다면 후에 기술할 bsp에 기반한 petalinux 프로젝트 hardware 폴더에 관련 프로젝트가 들어있다. vivado project follow vivado에서 zcu104보드 프로젝트를 만든다. Project is an extensible Vitis platform 은 vitis에서 xrt 라이브러리 등을 사용할 때 필요하다. 현 프로젝트에서는 미선택 board 세팅에서 ZCU104를 선택 create block design을 선택하여 디자인 블럭 생성 zynq_mpsoc ip 를 추가하고 borad preset을 적용한다. 지금은 PL 영역이 필요없으로 AXI_HPM/HP 포트를 미사용으로 설정 Validate Design 으로 디자인 검증 후 Create HDL Wrapper 로 래퍼를 생성한다. Generate Block Design을 실행 후 bit stream (*.bit) 을 생성한다. 현재는 pl 영역의 디자인이 없으므로 bit-stream을 생성하지 않아도 무관한다. Export Hardware로 xsa 파일을 생성한다. 현재는 pl부분의 디자인이 없으므로 bit-stream을 포함하지 않아도 되며 포함하여도 상관없다. Petalinux porting project create 처음에는 기초부터 시작하고자 base template 프로젝트로 시작하였지만 포팅 시 부팅이 잘 안되었다. 1 2 petalinux-create -t project --name ${프로젝트 이름} --template zynqMP petalinux-config --get-hw-description=${xsa파일} --silentconfig 그래서 xilinx에서 제공하는 bsp 기반으로 프로젝트를 만들었다. 1 2 petalinux-create -t project --name ${프로젝트 이름} -s ${bsp파일} petalinux-config --get-hw-description=${xsa파일} --silentconfig 위의 두 프로젝트의 폴더/파일을 비교해보면 BSP를 위해 커널등이 어떻게 설정되어 있는지 알 수 있다. bsp 기반으로 만들어진 프로젝트의 경우 README에 BSP가 어떤 설정을 가지고 만들어져 있는지 나와 있다. 위의 기본 템플릿 프로젝트와 파일과 비교해서 보면 몇가지 설정에 대한 설명이 누락되었음을 알 수 있다. tftp boot config 이전 post인 ubuntu 환경 설정을 내용을 참고하여 host에 tftp 서버를 설정한다. petalinux-config 명령을 실행하여 Image Packaging Configuration \u0026gt; Copy final images to tftpboot에 host tftp서버 폴더를 지정한다. 만약 향후에 RootFS를 INITRAMFS으로 할려고 할 시 built-in FIT image를 위한 임시 ram 사이즈가 작아서 부팅 시 \u0026ldquo;There\u0026rsquo;s no \u0026lsquo;/dev\u0026rsquo; on rootfs\u0026rdquo; 에러가 난다. 이럴 경우 petalinux-config의 Image packaging configuration \u0026gt; INITRAMFS/INITRD Image name 을 petalinux-image-minimal로 변경 한다. NFS rootFS config 이전 post인 ubuntu 환경 설정을 내용을 참고하여 host에 NFS 서버를 설정한다. petalinux-config 명령을 실행하여 Image Packaging Configuration 에서 하기 내역을 설정 Root File System Type에서 NFS를 선택 Location of NFS root directory에 host nfs 폴더를 지정 NFS Server IP address 에서 host ip를 지정 petalinux-config -c kernel에서 하기 내역이 설정 되어 있는지 확인 Networking support \u0026gt; IP: kernel level configuration 의 IP:DHCP support, IP:BOOTP support, IP:RARP support File systems \u0026gt; Network file systems \u0026gt; Root file systems의 NFS 체크 상기 내역까지가 매뉴얼의 내용인데 적용해보면 nfs 버전 문제로 nfs RootFS가 마운트 되지 않는다. bootargs에서 nfs version을 3으로 변경한다. petalinux-config에서 DTG Setting \u0026gt; Kernel Bootargs \u0026gt; generate boot args automatically를 해제 (해제하기전에 설정되어 있는 bootargs를 copy) 위에 복사한 것을 bootargs를 작성하는 란에 붙여넣고 nfsroot부분에 nfsvers=3 추가 ex) earlycon console=ttyPS0,115200 clk_ignore_unused root=/dev/nfs nfsroot=192.168.1.30:/home/minwook/xlx_nfsrfs,tcp,nfsvers=3 ip=dhcp rw MAC 설정 u-boot 부팅 시 마다 아이피가 달라지지 않도록 MAC를 설정한다. petalinux-config 명령의 Subsystem AUTO Hardware Setting \u0026gt; Ethernet Setting \u0026gt; Ethernet MAC address 사실 zcu104의 맥 주소는 부팅 시 eeprom에서 읽어 온다는데 u-boot에서는 안되는 것 같다(사실 잘 모르겠다.) build 및 부팅 준비 u-boot 및 커널 등을 빌드한다. jtag로 부팅 시키기 위해서는 pre-built 폴더에 이미지들이 준비되어 있어야 한다. petalinux-package를 이용해 준비하자. host의 NFS 서버 폴더에 RootFS를 압축 해제 시켜 NFS 부팅을 준비한다. 향후 SD 카드 등에 부트로더/부트 스크립트를 복사할 경우를 대비하여 부팅이미지를 생성하자. 1 2 3 4 5 petalinux-build petalinux-package --prebuilt cd images/linux tar -xzf rootfs.tar.gz -C ${NFS 서버 폴더} petalinux-package --boot --fsbl zynqmp_fsbl.elf --fpga system.bit --pmufw pmufw.elf --atf bl31.elf --u-boot u-boot.elf 빌드가 정상적으로 완료되면 이전에 지정한 host의 tftp 폴더에 build된 image들이 자동으로 복사된다. 향후 u-boot에서 tftp로 커널 등을 로드할 때 tftp 서버의 pxelinux.cfg 폴더 내 어떤 이미지를 로드할 것인지에 대한 설정을 파일에서 읽어온다. pxelinux.cfg 폴더의 default 파일을 보면 tftp 서버에서 kernel, dtb, RootFS를 로드한다는 것을 알 수 있다. 우리는 NFS에서 RootFS를 로드 할 예정이므로 default 파일의 RootFS 로딩 스크립트 부분을 삭제한다. 크로스 컴파일 환경 설정 향후 application의 개발 시 host에서 크로스 컴파일을 진행하고 싶다면 sdk를 만들어 sysroot를 설정하면 된다. 1 2 3 4 petalinux-build --sdk petalinux-package --sysroot -d ${SDK_설치_폴더} unset LD_LIBRARY_PATH source ${SDK_설치_폴더}/environment-setup-cortexa72-cortexa53-xilinx-linux Petalinux Booting jtag boot jtag로 u-boot까지 로딩한다. 보드의 boot-cfg 스위치를 jtag로 맞춘다. SD카드 등이 필요없지만 속도가 느리다. host에 USB를 연결하고 터미널을 오픈 후 하기 명령을 수행하면 부팅이 시작된다.부트 스크립트 로딩 전 대기 카운터에서 엔터를 누르면 u-boot 커맨드 입력이 가능하다. 만약 rlwrap: warning 이 발행한다면 현재 $TERM에 rlwrap이 없는 것이므로 쉘의 $TERM의 값을 xterm으로 변경한다. 1 2 export TERM=xterm petalinux-boot --jtag --prebuilt 2 --hw_server-url tcp:127.0.0.1:3121 u-boot에서 커널 로딩 u-boot에서 command를 이용하여 tftp서버에서 커널과 dtb를 로드한다.\ndhcp로 target ip 획득 tftp 서버의 ip 및 target의 ip의 환경변수 설정 이부분은 petalinux-config의 u-boot Configuration \u0026gt; u-boot script configuration \u0026gt; Pre bootenv 에서 설정이 가능할 것이라 생각되는데 해보지 않음 tftp 서버에서 설정파일 로드 (pxelinux.cfg/default) tftp 부팅 (이후 RootFS의 로드는 세팅에 따른다.) 1 2 3 4 5 dhcp setenv serverip ${host_ip} setenv ipaddr ${target_ip} pxe get pxe boot SD카드 + NFS RootFS jtag로 부팅하면 편하긴 하지만 느리고 매번 리셋이 필요할 때마다 부팅명령을 다시 넣어줘야 한다. 이를 SD카드로 부팅시켜 해결할 수 있다. 보드의 boot-cfg 스위치를 SD 카드로 변경한다. SD카드가 준비되어 있지 않다면 SD카드를 파티션 설정을 해야 한다. 첫번째 파티션은 부트로더, 부팅스크립트 등을 위한 파티션이며 최소 500MB이며 FAT 파일 형식 두번째 파티션은 RootFS용으로 EXT4 형식이어야 한다. SD카드에 FSBL, U-boot, bitstream인 BOOT.bin 를 넣어 놓고 u-boot 까지 부팅 시킨 후 이후 커널 및 RootFS를 로딩할 수 있다. SD카드에 커널의 내용이 변경되지 않은 경우 부팅스크립트 boot.scr와 커널/디바이스트리 image.ub를 넣어 놓고 자동으로 NFS에서 RootFS를 로딩하게 하는 방법도 가능하다. 참고 자료 https://xilinx.github.io/Embedded-Design-Tutorials/docs/2022.2/build/html/index.html https://github.com/Xilinx/Vitis-Tutorials https://docs.xilinx.com/r/2021.1-English/ug1144-petalinux-tools-reference-guide/Configure-TFTP-Boot ","date":"2023-02-03T00:00:00Z","permalink":"https://muonkmu.github.io/p/yolo_acc_prj-03-zcu104-%EA%B0%9C%EB%B0%9C%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95/","title":"[YOLO_Acc_prj] 03 zcu104 개발환경 설정"},{"content":"본 챕터에서는 graph-based deep-learnig accelerator 중 Graphcore IPU에 대해 알아본다. Graphcore IPU는 Microsoft와 Dell의 차세대 데이터 센터 딥러닝 가속기로 선정되었다.\nGraphcore Intelligence Processing Unit(IPU) Graphcore IPU는 fine-grained operation을 수행하기 위하여 graph theory를 적용하며 MIMD paralleism을 제공\nFine-Grained : 하나의 작업을 작은 단위의 프로세스로 나눈 뒤 다수의 호출을 통해, 작업 결과를 생성해내는 방식, 반대말은 Coarse-Grained. Intelligence Processor Unit Architecture IPU는 tiles라 불리는 1216 PE로 구성 PE는 256kb local memory를 가지며 레지스터 파일을 제외한 추가 memory storage를 가지지 않음 tiles 간 IPU Exchange라 불리는 on-chip interconnect로 연결되어 있으며 IPU간 연결을 위한 IPU link를 제공 IPU는 6개의 개별 processing thread를 제공하며 각 thread는 별개의 instruction과 excution flow를 제공 각 tile은 static round-robin schedule에 따라 thread 들을 순환한다. Accumulating Matrix Product (AMP) Unit IPU는 pipelined structure AMP를 채택, AMP는 64bit mix-precison 또는 16bit single-point 연산을 클럭 사이클 마다 수행 가능 mix-precison : 훈련 중에 모델에서 16-bit 및 32-bit 부동 소수점 유형을 모두 사용하여 더 빠르게 실행하고 메모리를 적게 사용하는 것, 모델의 특정 부분을 32-bit 유형으로 유지 Memory Architecture PE 당 256kb local memory를 가지며 IPU는 총 304Mb 메모리를 가짐 각 tile은 21bit address space를 가지며 6개의 execution unit과 이를 공유함 Interconnect Architecture IPU 간 연결은 IPU link를 사용하며 2개 IPU 연결은 3개의 IPU link를 사용 (65Gb/s) Host완s PCIE-4로 연결 Bulk Synchronous Parallel (BSP) Model IPU operation은 BSP 모델을 기반으로 하며, BSP 모델은 아래 3개의 Phase Operation으로 구분된다. Computation Phase : 모든 프로세서가 로컬 메모리로 computation을 수행하며 프로세서간 어떤 통신도 없다. Communication Phase : 각 프로세서는 정보를 교환하며 어떤 computation도 없다. Barrier Synchronization : 모든 프로세서는 computation이나 communication 없이 모든 프로세서가 barrier에 도달할 때까지 대기한다. IPU는 computation이 시작되기 전에 instruction이 코어로 전송되는 BSP 모델을 실행한다. core는 computation을 수행하고 이가 끝난 뒤 다른 코어와 communication을 수행한다. 그후 모든 코어는 동기화를 수행한다. 결론 Blaize GSP와 Graphcore IPU는 분산 프로세서를 통해 거대 parallel operation을 처리할 수 있기에 Cloud-base application에 좋은 솔루션이다. 그러나 이들은 power/area 문제로 임베디드 추론 application에는 적합하지 않다. ","date":"2023-01-28T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-2/2/","title":"[AI HW Design] Chap04 Streaming Graph Theory (2/2)"},{"content":"본 챕터에서는 graph-based deep-learnig accelerator에 대해 공부한다 (Blaize GSP, Graphcore IPU). 이들은 Mutiple Instructions Multiple Data(MIMD) 처럼 동작한다.\nBlaize Graph Streaming Processor(GSP) 무언가 칩에 대한 설명보단 Graph Streaming 기본 이론에 대한 내용이 주를 이룬다.\nStream Graph Model Stream Graph는 네트워크 트래픽, 데이터베이스 등에 널리 쓰이는 모델로 dynamic stream data를 처리하기 위해 data stream model(TCS)을 사용 거대 그래프 스트리밍 Transit(T), 큰 데이터 처리 Compute(C), 일시/롱텀 데이터 저장 Store(S) Turnstile 모델이 TCS모델 중에서 데이터 출발/도착과 같은 data behavior을 가장 잘 표현하며 task scheduling에 사용. Depth First Scheduling Approach Blaize GSP는 뉴럴넷모델을 Direct Acyclic Graph(DAG) format (V,E)로 변환 V는 PE vertex, E는 PE간 weighted connection을 위한 edge scheduling을 위해 Depth First Scheduling (DFS)를 사용하며 dynamic graph excution을 가능하게 하고 sparse/conditional graph를 지원한다. (dfs 설명은 유명하니 생략) GSP는 4가지 Parallelism을 달성했다. 자세한 설명은 책 참조 Task parallelism, Thread parallelism, Data parallelism, Instructon parallelism Graph Streaming Processor Architecture GSP는 다음 그림과 같은 구조로 되어 있음 Streaming Processing은 Sequential Processing에 비해 하기와 같은 장점이 있음 (책에 두가지 방법에 대해 비교 그림있음) Small intermediate buffer for local processing Cached data is easily supported Memory bandwidth is reduced to improve the performance with less power Support both task and data-parallel processing Data is sent to the next node when it is ready GSP는 opeartion을 데이터가 준비되는 즉시 기다리지 않고 수행하도록 스케줄링 함으로써 성능을 향상시키고 메모리 access를 감소시킴 ","date":"2023-01-27T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap04-streaming-graph-theory-1/2/","title":"[AI HW Design] Chap04 Streaming Graph Theory (1/2)"},{"content":"Git을 사용하다보면 막히는 부분이 항상 생긴다. 이런 부분에 대해 간단히 정리 해 놓는다.\nGit remote branch 가져오기 Git을 사용하다보면 원격저장소에 있는 branch를 local로 가져와야 경우가 있다. 이럴 때 git checkout -t {저장소 이름} 을 사용하면 된다.\n필요 시 git remote를 갱신 필요 시 remote 브랜치 확인 remote 브랜치 가져오기 1 2 3 git remote update git branch -r git checkout -t {origin/저장소 이름} 만약 브랜치의 이름을 변경하여 가져오고 싶다면 git checkout -b {생성할 branch 이름} {원격 저장소의 branch 이름} 만약 checkout 시 -t 옵션을 제외하면 ‘detached HEAD’ 상태로 소스를 보고 변경 해볼 수도 있지만 변경사항들은 commit, push 할 수 없으며 다른 branch로 checkout하면 사라진다. ","date":"2023-01-20T00:00:00Z","permalink":"https://muonkmu.github.io/p/git-%EC%82%AC%EC%9A%A9-tip-%EC%A0%95%EB%A6%AC/","title":"git 사용 tip 정리"},{"content":"Target Model을 YOLOv3_tiny로 정한 것은 다른 이유가 있는 것은 아니고 간단하고 레퍼런스가 쉽게 구할 수 있어서이다. 사실 프로젝트가 YOLOv3 tini의 경우 매우 가볍기 때문에 가속기로의 의미는 크게 없다고 생각한다. 그러나 YOLO-X 모델 같은 가속기를 구현하기 위해서는 Sparse Matrix operation 등이 적용 가능한 NPU와 같은 구조를 잡는 것이 필요할 것이라 생각되어 미루어 두기로 한다. 우선 간단한 가속기를 구현하는 것에 의미를 둔다.\nYOLO reference YOLO v3 tiny은 YOLO v3에서 FPN 을 덜어내고 경량화 시킨 구조이다. 라즈베리 파이 CPU에서도 돌릴 수 있다고 한다. ( 실제로 돌려보니 정확도가 좀 떨어지는 것 같다. 바운딩 박스도 이상하게 쳐지고) 기본적인 코드는 darknet git에서 구할 수 있다. 사용법 및 설명은 홈페이지에서 볼 수 있다. (https://pjreddie.com/darknet/yolo/) darknet repo pull make(GPU 사용 예정이라면 Makefile 수정) pre-trained 된 weights 다운 test 1 2 3 4 5 git clone https://github.com/pjreddie/darknet cd darknet make wget https://pjreddie.com/media/files/yolov3-tiny.weights ./darknet detect cfg/yolov3-tiny.cfg yolov3-tiny.weights data/dog.jpg yolov3-tiny.cfg 파일을 보면 Model의 구조를 알 수 있다. 각 layer에 대한 설명은 누군가 Darknet을 pytorch로 변환하면서 분석해 놓은 것이 있으니 이를 참조한다. (https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/) YOLO v3 Structure Model을 도식화하면 다음과 같다. 참고 자료 https://wikidocs.net/181704 https://deep-learning-study.tistory.com/411 ","date":"2023-01-16T00:00:00Z","permalink":"https://muonkmu.github.io/p/yolo_acc_prj-02-yolo-v3-tiny-%EB%B6%84%EC%84%9D/","title":"[YOLO_Acc_prj] 02 YOLO v3 tiny 분석"},{"content":"본 챕터에서는 몇가지 주요한 Paralle Architecture에 대하여 소개한다. 이 페이지에서는 Microsoft Catapult Fabric Accelerator에 대해서 기술한다.\nMicrosoft Catapult Fabric Accelerator 마이크로소프트는 Brainwave Project를 CPU와 FPGA를 쓰는 형태로 변경 48개의 FPGA가 2개의 Half rack (pod)에 그룹화되고 네트워크로 연결 Brainwave는 train된 DNN 모델을 Catapult Fabric이라 불리는 synthesized softcore에 컴파일 후 narrow precision approach를 적용 모델의 파라메터는 softcore 내 상주 System Configuration Catapult Fabric은 synthesized softcore이며 이는 RTL의 recompilation 없이 low level SW library를 통해 reconfigure 될 수 있다. Catapult Fabric은 Shell과 Role 두 파트로 나눌 수 있음 (상세사항은 책을 참조) Shell : 모든 application에서 재사용 가능한 programmable logic (통신/off-chip 등의 인터페이스를 말하는 듯) Roll : Application logic (그림을 보면 Softcore를 지칭하는 듯) Catapult Fabric Architecture Catapult Fabric은 Matrix-Vector Multiplier(MVM), MultiFunction Uint(MFU), Vector Arbitration Network로 구성 MVM : Matrix-Vector 및 Vector-Vector 연산 수행, PRF/VRF/MRF에 ifmap/fmap 저장 VAN : PRF/DRAM/IO-queue 간 데이터 전송 담당 Matrix-Vector Multiplier FP16을 MS-FP8/MS-FP-9으로 변환하여 연산(mantissa가 2~3bit) input data는 VRF, filter weight는 MFR에 저장 3개의 tile engine과 3개의 accumulator에서 병렬 연산을 지원(상세내용을 책을 참조) MVM의 출력은 MFU로 연결, MFU는 vector-vector operation, activation등을 수행 Hierarchical Decode and Dispatch (HDD) Catapult Fabric은 전통적인 scalar processor(Single Instruction Single Data)를 채택 scheduler는 6개의 Decoder가 4-layer구조로 배치 (상세 내용은 책 참조) Sparse Matrix-Vector Multiplication (SMVM) SMVM을 위해 Condensed Interleaved Sparse Representation (CISR) encoding 사용 Compressed Sparse Raw (CSR) 포맷이 가변 row 길이로 인해 Parallel contorl이 어려움을 극복 첫 번째 0이 아닌 요소가 첫 번째 슬롯에 배치, 해당 열 인덱스는 인덱스 배열에서 동일한 순서로 배치, 행 요소가 모두 사용되면 다음 두 행 요소가 빈 슬롯에 할당, 이를 반복 (상세 내용은 책 참조,사실 이해가 잘 안감) ","date":"2023-01-13T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-3/3/","title":"[AI HW Design] Chap03 Parallel Architecture (3/3)"},{"content":"현재까지 DL에 대해 공부한 것과 현업에서 배운 것을 섞어보고자 개인 프로젝트를 진행할 예정이다. 실력이 미천하여 성능, 효율성은 미뤄두고 가장 빠르고 쉽게 구현하는 것을 목표로 한다. 생각보다 오래 걸릴 듯 하다.\nGOAL 카메라의 입력을 받아 Real-time으로 Object detection을 수행하는 FPGA 기반 ECU 개발 현재 개발되어 있는 기반 설계 및 IP가 전무하기에 PPA 보다는 빠른 구현에 목표를 둔다.\nSPEC (TBD) target B/D : ZCU104 input : Full HD camera (interface MIPI or USB 중 쉬운거) output : real time image showing a bounding box (interface HDMI) Algorithm : YOLO v3 Tiny (이를 선택한 특별한 이유는 없고 reference 구하기 쉽고 간단해서 이다) miniaml 20 fps Design Flow (TBD) architecture design and spec fix camera interface design output interface design YOLO Core design (HLS + verilog) ","date":"2023-01-13T00:00:00Z","permalink":"https://muonkmu.github.io/p/yolo_acc_prj-01-yolov3-acc-personal-project-propsal/","title":"[YOLO_Acc_prj] 01 YOLOv3 Acc Personal project propsal"},{"content":"GPU를 이용하여 Deep learning 모델을 구성하고자 하였으나 다른 기술 블로그에서 기술한대로 수행하여도 동작이 되지 않는다. 해당 공식 문서들을 참고하여 설치하는 법을 기술한다.\n목적 GPU 및 pytorch 기반 개발을 위한 PC 개발 환경 구성 GPU 활용을 위해 nvidia driver, cuda, cudnn 설치와 conda 환경에서 pytorch를 설치하는 방법을 다룬다. 환경 OS : ubuntu 20.04 LTS GPU : nvidia 1080ti python 3.8.10 and GCC 9.4.0 설치 절차 우선 모듈의 dependency를 확인해야 한다. 현재 pytorch에서 우분투 20.04를 지원하는 플랫폼은 CUDA 11.7이므로 CUDA 11.7 버전과 이에 적합한 nvidia driver를 설치 해야한다. 필요한 nvidia driver는 사실 cuda를 설치 해보면 dependency 체크를 하면서 필요한 버전을 알려준다.(더 좋은 방법이 있을지도)\nNvidia Driver 설치 nvidia driver 설치 여부 및 현재 설치된 버전을 확인한다. 1 nvidia-smi 필요 시 기존의 nvidia driver를 삭제한다. 1 2 3 sudo apt-get remove --purge \u0026#39;nvidia-.*\u0026#39; sudo apt-get autoremove sudo apt-get autoclean 설치가능한 드라이버를 확인한다. 1 ubuntu-drivers devices 만약 필요한 드라이버 목록에서 없다면 저장소를 추가한다. 1 2 sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update 원하는 드라이버를 설치 후 재부팅 한다. 현재 저자의 환경에서는 5.25가 필요하므로 이 버전을 예로 설명한다. 1 2 sudo apt install nvidia-driver-525 sudo reboot cuda 설치 cuda 홈페이지에서 현재 내 설정에 맞는 runfile을 다운 가능하나 저자는 이상하게 설치가 안되었다. 하기 페이지를 활용하여 네트워크 Repo에서 설치하자 (https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#prepare-ubuntu)\nRemove outdated signing key Install the new cuda-keyring package Install CUDA SDK reboot 1 2 3 4 5 6 sudo apt-key del 7fa2af80 wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb sudo dpkg -i cuda-keyring_1.0-1_all.deb sudo apt update sudo apt-get install cuda-11-7 sudo reboot 환경변수를 등록한다. 1 2 export PATH=/usr/local/cuda-11.7/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-11.7/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} cudnn 설치 cudnn 역시 package 파일로 설치가 잘 안되서 tar 파일로 설치하였다. (https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html)\n필요한 cudnn 라이브러리 tar를 cudnn 홈페이지에서 다운받는다. 파일의 압축을 풀고 cuda 라이브러리에 파일을 복사한다. 1 2 3 4 tar -xvf cudnn-linux-x86_64-8.x.x.x_cudaX.Y-archive.tar.xz sudo cp cudnn-*-archive/include/cudnn*.h /usr/local/cuda/include sudo cp -P cudnn-*-archive/lib/libcudnn* /usr/local/cuda/lib64 sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn* conda 설치 모듈들의 dependency 및 버전 관리를 위해 가상환경인 conda를 사용하기로 하였다.\n자신의 파이썬 환경에 맡는 miniconda 설치 파일을 받고 이를 실행한다. 1 Miniconda3-latest-Linux-x86_64.sh 자신이 사용할 가상환경을 만들고 이를 실행한다. 1 2 conda create -n {my_env} conda activate {my_env} 터미널 실행 시 자동으로 conda 환경이 실행되는 것을 막을려면 다음을 수행한다. 1 conda config --set auto_activate_base false pytorch 설치 conda 환경에서 pytorch 홈페이지를 참고하여 pytorch를 설치한다. 자신이 원하는 구성을 고르면 Run command를 알려준다.\n1 conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia 설치 확인 conda 환경에서 python을 터미널을 실행한 후 pytorch cuda 설정 사용 가능 여부가 True로 출력되면 정상\n1 2 import torch print(torch.cuda.is_available()) ","date":"2023-01-13T00:00:00Z","permalink":"https://muonkmu.github.io/p/dl-%EA%B0%9C%EB%B0%9C%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95-cuda-cudnn-conda-pytorch/","title":"DL 개발환경 설정 (cuda, cudnn, conda, pytorch)"},{"content":"본 강좌에서는 Object Detection의 개념과 이를 위한 YOLO알고리즘의 기초에 대하여 정리한다.\nhttps://www.coursera.org/learn/convolutional-neural-networks 하기 블로그에 더 잘 정리되어 있다..(누군지 존경스럽다.)\nhttps://junstar92.tistory.com/140 Object Localization Classification with localization : object class 뿐 아니라, 알고리즘이 object를 대상으로 bounding box를 표시하는 것을 의미\n이를 위해 이미지를 CNN에 입력 출력으로 class 뿐만 아니라 이미지에 object가 존재할 확률($p_c$), Bounding box의 위치 및 크기를 같이 출력(Bx, By, Bh, Bw) Loss function은 MSE(mean squared error)를 사용한다면 Y 각 요소의 에러의 합과 같다. Pc가 0일 경우 Pc의 에러만 사용한다. $$ L(\\hat{y},y) = \\begin{cases} (\\hat{y}_1-y_1)^2+(\\hat{y}_2-y_2)^2+\\cdots+(\\hat{y}_8-y_8)^2 \u0026amp; \\text{if } y_1=1 \\\\ (\\hat{y}_1-y_1)^2 \u0026amp; \\text{if } y_1=0 \\end{cases} $$\nMSE를 예시로 설명했지만, $c_1$, $c_2$, $c_3$에는 log-likelihood loss와 softmax를 사용하고, bounding box 정보에는 MSE를, 그리고 $p_c$ 에는 Logistic Regression Loss를 사용할 수도 있다.\nLandmark Detection Bounding box가 아닌 일반적인 Face recognition이나 pose detection의 같은 일반적인 경우 이미지의 주요 포인트(landmark)를 X와 Y의 좌표로 나타낼 수 있다. Object Detection Sliding Windows Detection 알고리즘을 사용해서 Object Detection을 위해 ConvNet을 사용하는 방법 알아본다. (CS231n 강의에서는 Sliding window는 하지말라던데 아마 이해를 위해 넣어놓은 것 같다.) 방법은 하기와 같다. object의 클래스를 구분할 수 있는 모델 생성 전체 이미지 중 특정 size의 window를 골라 탐색 window 살짝 옮겨서 반복 더 큰 박스를 이용하여 반복 그러나 이 방법은 computing cost가 높다. 다음 절에서 이를 줄일 수 있는 방법을 알아본다 Convolutional Implementation of Sliding Windows Sliding window 방법은 매우 느린데 이를 해결하기 위해 FC(Full connected) layer를 Convolutional Layer로 튜닝하는 것을 알아보자. 절차는 하기와 같다 FC layer를 이와 같은 output을 낼 수 있는 Filter로 변환 sliding window 시 각각 수행이 아닌 convolution처럼 한번에 연산. 이렇게 하면 중복되는 연산은 공유가 가능하다. 그러나 이 방법은 bounding box의 위치가 정확하지 않다는 단점이 있는데 이를 아래 방법으르 해결한다. Bounding Box Predictions Sliding window 방법은 object가 그 위치에 있지 않거나 일부분만 걸칠 수 있는데 이를 YOLO 알고리즘으로 극복 가능하다. 전체 이미지에 3x3 grid 를 설정(보통은 19x19 사용) 위에서 배운 object localization을 각각의 grid에 적용, 즉 이해한바로는 test set에서 각각의 그리드에 localization방법으로 labeling하고 학습 각 grid에 object가 존재한다면 object의 중간점을 위해서 object를 할당한다. 이때 object의 크기는 1이 넘어갈 수 있다.(gird를 넘어가거나 클 수 있으므로) Bounding box를 설정하는 방법은 여러가지가 있지만(ex. PCA 이용), YOLO논문을 살펴보면 잘 동작할 수 있도록 파라미터화 된 것들이 있다. Intersection Over Union Intersection over union(IoU)은 Object Detection이 잘 동작하는지 판단하기 위한 함수 labeling 된 bounding box와 예측한 bounding box의 전체 넓이와 겹치는 부분 넓이의 비율을 계산 보통 0.5 이상이면 예측한 bounding box의 결과가 옳다고 판단 Non-max Suppression 현재까지 알아본 Object detection의 문제점은 한 Object를 여러번 탐지할 수 있다는 것이다. 즉 한 object가 한 그리드 이상에의 면적을 차지할 경우 이 object의 중심점이 여러 Cell에서 탐지 될 수 있다. 이 경우에 Non-max suppression을 사용하면 알고리즘이 하나의 object를 하나의 cell에서 한번만 탐지할 수 있다. 만약 분류 class 가 1개여서 $p_c$가 class의 확률이라 가정한다. (실제로는 클래스는 여러개) $p_c$를 조사하여 가장 큰 것만 취함 나머지 box와 $p_c$값이 가장 큰 박스와 IoU 조사 IoU가 높은 박스는 제거 만약 class가 여러개라면 class 당 non-max suppression을 수행한다. Anchor Boxes 현재까지 소개한 알고리즘의 문제점 중 하나는 각 grid cell이 오직 하나의 object만 감지할 수 있다는 것이며 이를 anchor box라는 아이디어를 가지고 해결할 수 있다. anchor 박스의 모양을 미리 정의 각각의 anchor box는 각 output을 가지게 한다. anchor box의 선택은 manual로 선택을 할 수도 있고, K-mean알고리즘을 통해서 얻고자하는 유형의 object모양 끼리 그룹화 할 수도 있다. YOLO Algorithm 위의 내용을 모두 종합하여 YOLO object detection algorithm을 정리해보자 이미지의 anchor box와 grid 수를 정하고 이와 같이 labeling된 데이터 셋으로 모델을 학습 상기 모델로 추론을 수행하게 되면 각 grid cell은 anchor box 수만큼의 bounding box를 가질 수 있다. 여기서 낮은 확률을 가지는 예측결과는 제거하고 각 class에 non-max suppression을 적용하여 최종 예측 결과를 얻는다. YOLO 알고리즘은 가장 효과적인 Object Detection 알고리즘 중 하나 ","date":"2023-01-10T00:00:00Z","permalink":"https://muonkmu.github.io/p/cnn-week-03-object-detection/","title":"[CNN] week 03 Object detection"},{"content":"본 챕터에서는 몇가지 주요한 Paralle Architecture에 대하여 소개한다. 이 페이지에서는 NVDLA와 Google TPU에 대해서 기술한다.\nNVIDIA Deep Learning Accelerator (NVDLA) NVDLA는 FPGA로 구성 가능한 추론을 위한 오픈소스 아키텍쳐 (http://nvdla.org) Primitive functional blocks으로 CNN을 지원 (convolution, activation, pooling, normalization) 각 블럭은 next layer의 active와 configuration을 위한 double buffer를 가짐 next layer의 operation은 active operation이 완료되어야 시작 independent mode와 pipeline을 사용하는 fused mode가 있음 Figure. NVDLA core architecture Convolution Operation Direct convolution, Image input convolution, winograd convolution, Batch convolution 지원 (상세내역은 책 참조) Single Data Point Operation(SDP) SDP는 linear functions와 Look-up Table nonlinear functions을 통해 activation과 normalizatin을 지원 (상세내역은 책 참조) Planar Data Operation(PDP) PDP는 maximum/minimum/average pooling을 지원 Multiplane Operation Cross Channel Data Processor(CPD)은 Local Response Normalization(LRN)을 수행 Data Memory and Reshape Operations bridge DMA는 외부 메모리와 메모리 인터페이스간 데이터 전송을 담당 data reshape engine은 data trasnformations, splitting, slicing, merging, contraction, reshape transpose 를 담당 System Configuration NVDLA는 small/large system model로 구현할 수 있음 small system model : IoT 기기와 같이 작은 모델을 위한 모델, 복잡도와 storage를 낮추고 single task를 수행 large system model : mutiple task를 위한 coprocessor와 메모리 인터페이스 추가 External Interface NVDLA는 외부와 통신을 위한 Configuration Space Bus(CSB), Data backbone(DBB), SRAM interface, Interrupt interface를 가짐 (상세내용은 책 참조) Software Design NVDLA SW는 Trained model을 parser/compiler/optimizer를 통해 loadable로 변환 User Mode Driver(UMD)에 의해 Loadalbe이 로딩 되고 Job이 Kernel Mode Driver(KMD)로 제출됨, KMD는 스케줄링 수행 Google Tensor Processing Unit(TPU) 구글은 speech recognition 수요 해결을 위해 TPU v1(stand alone)과 v2/v3(cloud)를 개발 TPU v1은 하기 스펙으로 MLP 0/1, CNN 0/1, RNN 0/1 6가지 neural network application을 수행 가능 256 × 256 eight bits MAC unit 4 Mb on-chip Accumulator Memory (AM) 24 Mb Unified Buffer (UB) – activation memory 8 Gb off-chip weight DRAM memory Two 2133 MHz DDR3 channels TPU는 6가지 neural network application을 수행할 수 있음 Multi-layer perceptron(MLP) 0/1, Convolution Neural Network(CNN) 0/1, Recurrent Neural Network(RNN) 0/1 System Architecture TPU v1은 매트릭스 연산을 Matrix Multiply Unit(MMU)에서 수행 MMU는 256 × 256 eight bits MAC unit이며 16bit 연산을 수행할 경우 성능은 8bit 대비 절반(Sparse matrix 연산을 지원하지 않음) Weight FIFO는 matrix weight를 8Gb DRAM에서 읽어오며 activation, pooling, normalization 후 중간 연산 결과를 24Mb Unified Buffer에 저장 Multipy-Accumulate(MAC) Systolic Array Systolic array는 TPU의 핵심이자 High throughput / low latency를 가진 SIMD pipeline. 책에 별 설명이 없으므로 이에 대한 내용은 더 찾아보는 것이 좋다(다른 많은 곳에 잘 나와 있음) 단점은 전력 소모가 많다는 것(데이터 센터 등에 적합) New Brain Floating-point Format TPU v1은 input data를 FP32에서 INT8로 quantization하여 연산하며 이에 따라 안정성/정확도 문제 발생 이를 위해 IEEE FP16 대신 Brain Floating Point format (BFP16) 사용 BFP16 : Mantissa를 7bit으로 줄이고 exponent를 FP32와 같은 8bit으로 늘림 Sign 1bit, Exponent 8bit, Mantissa 7bit multiplier area와 power를 줄이고 Scaling loss 없이 FP32와 동일한 정확도를 얻음 Performance Comparision roof-line model의 관점에서 볼 때 TPU가 가장 높은 peak performance를 달성했다. roof-line model은 Y축이 성능을 나타내며(평평한 부분이 최고 성능), X축이 byte 당 operation intensity 부가 설명을 하자면 한번에 얼마나 많은 연산을 수행하게 할 때 성능이 어디까지 올라가는지 지표, loof-line은 메모리 Bandwith 때문에 걸림 Cloud TPU configuration TPU v2/v3는 v1에서 DDR을 HBM으로 바꾸고 v1을 Pod로 묶음 Cloud Software Architecture 구글은 cloud computation을 위해 새로운 SW 아키텍쳐를 개발 Model을 TensorFlow를 통해 computational graph로 해석 상세 내용은 책을 참조 ","date":"2022-12-30T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/","title":"[AI HW Design] Chap03 Parallel Architecture (2/3)"},{"content":"IDEC에서 수강한 KiCad 강좌 요약 이다. 디자인 플로우 중 Symbol and Schemetic에 대해 정리한다.\nKiCad 개요 회로도 및 PCB가 함께 설계되는 오픈소스 통합 설계도구 거버 /드릴/ 부품위치 파일 생성 및 PCB 계산기, 거버 뷰어, 3D 뷰어, SPICE 시뮬레이터 포함 프로젝트 기반 관리로 한번에 하나의 프로젝트만 열 수 있음 파일구성 *.kicad_pro : 회로도와 pcb간 공유되는 설정이 포함된 프로젝트 파일 *.kicad_sch : 모든 정보와 구성 요소 자체를 포함시키는 회로도 파일 *.kicad_sym : 회로도 심볼 라이브러리 파일로 심볼 요소 설명을 포함 *.kicad_pcb : pcb 보드 파일 *.pretty : 풋프린트 라이브러리 폴더 *.kicad_dru : pcb 사용자 설계 규칙 파일 *.net. : 회로도에 의해 생성되는 넷리스트 파일 KiCad PCB design workflow 프로젝트 생성 회로도 그리기 회로도 심볼을 심볼 라이브러리에서 찾아 지정된 선 연결, 심볼이 없을 경우 새로 심볼을 새로 만듬 각 구성 요소에 대해 풋프린트를 배정하고 풋프린트가 없는 경우 풋프린트를 생성하여 반영 회로도 완성 시 전기 규칙 점검(ERC 수행) pcb 편집기로 전송하여 레이아웃 시작(넷리스트 생성 및 부품 간 선 연결 일치 시킴) 기판 크기(Edge.Cuts) 그리기 및 풋프린트 위치를 선정 배치 배치 후 요소 사이 트랙 연결 트랙은 규정에 따라 전류 용량, 임피던스, 고전압 누화 등을 고려 선폭/선간 설정 (pcb계산기 참조) 트랙은 신호선의 경우 보통 12mil, 6mil 이하로 하면 pcb 제작 단가 상승 레이아웃이 완료되고 설계 규칙 검사(DRC) 및 수정 거버 파일 제작 출력 및 PCB 제작 의회 프로젝트 관리 창 Tip 1 : 프로젝트 생성 시 템플릿을 지정하여 생성 가능 (큰 회사에서 기초 설정 등을 지정한 형식) Tip 2 : 환경 설정에서 텍스트 편집기를 등록하면 텍스트 편집기 사용이 가능하다. Symbol 생성 필요 시 심볼 라이브러리 생성 (파일-\u0026gt;새라이브러리) 라이브러리 선택 후 새 심볼 생성 생성된 심볼에서 레퍼런스, 심볼값을 원하는 위치로 이동 외형선 그리기, 핀 부가, 핀 더블 클릭 하여 속성(이름, 번호, 유형 등) 설정 필요 시 원점 설정 (단축키 space)(심볼 로딩 위치 및 로테이션 시 회전 점) 저장 Tip) 편집 시 원하는 위치에 지정할 수 없을 때 그리드 속성을 편집하여 그리드 간격을 조절하자 회로도 그리기 프로젝트 매니저에서 {프로젝트 이름}.kicad_sch 파일을 연다 심볼을 배치한다 (전원의 경우 pspice 라이브러리는 시뮬레이션 용이니 Power라이브러리 사용) 선을 연결하고 텍스트 위치 조정한다. 레퍼런스 (부품번호, ex. R100) 지정자 채우기 로 레퍼런스 설정 PCB 풋 프린트 배정 ERC 수행/수정 및 BOM 출력 ","date":"2022-12-28T00:00:00Z","permalink":"https://muonkmu.github.io/p/kicad-01-symbol-and-schemetic/","title":"[KiCad] 01 Symbol and Schemetic"},{"content":"IDEC에서 수강한 KiCad 강좌 요약 이다. 디자인 플로우 중 Footprint와 PCB에 대해 정리한다.\nFootprint Footprint design flow footprint 편집기를 연다 실부품 측정 또는 데이터 시트를 참조하여 부품 치수 확인 필요 시 풋프린트 라이브러리 생성 (파일-\u0026gt;새라이브러리) 라이브러리 선택 후 새 풋프린트 생성 하거나 유사한 부품 불러 온 후 다른 이름으로 저장 십자선 커서 이용하여 원점에서 스페이스바를 눌러 원점 위치 선정 실크 레이어에 부품 외형선 그리기 패드와 홀 위치 시킨 후 사이즈 속성 편집 패드 위치에 맞게 번호 편집, 핀번호는 심볼과 일치하도록 할 것 저장 SMD Component Footprint SMD 부품의 경우 패드의 속성을 SMD로 변경 뒷면 실장 component 뒷면에 실장할 경우 레이어를 관련 레이어를 B.* 레이어로 변경해야 한다. F.Cu, F.Silkscreen, F.Courtyard, F.Fab 내용을 B.* 레이어로 이동 PCB design 프로젝트 매니저에서 PCB 편집기 열기 회로도 PCB 전환(F8)을 이용하여 회로도에서 컴포넌트를 로딩 Edge.Cuts layer에서 PCB 외형선을 그리기 외형선 내부에 컴포넌트 배치 및 컴포넌트 레퍼런스/value 위치 조정 필요한 텍스트를 Silkscreen에 부가 트랙 설정 및 배선 (일반적으로 신호선 12mil, 전원선 30mil) 동박면 씌우기 (GND와 연결) DRC 검사 Plot을 통해 거버/드릴링/포지션 파일 생성 및 검사 ","date":"2022-12-28T00:00:00Z","permalink":"https://muonkmu.github.io/p/kicad-02-footprint-and-pcb/","title":"[KiCad] 02 Footprint and PCB"},{"content":"본 chapter에서는 Reinforcement Learning에 대해서 알아보자\nVideo : https://www.youtube.com/watch?v=lvoHnicueoE\u0026list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv\u0026index=15 Slide : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture14.pdf (TODO: 귀차니즘의 압박으로 정리를 안했다.. 근데 강의가 무척 어려워서 잘 이해가 안된다.)\n","date":"2022-12-23T00:00:00Z","permalink":"https://muonkmu.github.io/p/cs231n-chap-14-reinforcement-learning/","title":"[CS231n] Chap 14 Reinforcement Learning"},{"content":"Petalinux Booting and Packaging petalinux Packaging petalinux-package 명령을 이용하여 하기 내역을 수행 할 수 있다. .BIN 또는 .MCS파일을 생성 (\u0026ndash;boot 옵션) BSP (.BSP 파일) 또는 Package image 생성 (\u0026ndash;bsp, \u0026ndash;image 옵션) prebuilt 디렉토리 생성 (\u0026ndash;prebuilt 옵션) Vitis 를 위한 sysroot 설치 (\u0026ndash;sysroot 옵션) petalinux booting QEMU, SD card, Jtag, TFTP, QSPI에 의한 Booting을 지원한다. (jtag Boot는 속도가 느려 잘 사용안함)\nPetalinux Debugging 상세 내용은 교제의 Petalinux Application Debugging 및 LAB5 참조\nPetalinux는 Application Debugging 시 System Debugger(Vitis) 와 GNU Debugger를 지원한다. Vitis는 Target Communication Framework(TCF)와 Xilinx System DBugger(XSDB)를 이용한 Debugging 환경을 제공 일반적인 Linux GNU Debugger 지원 System Debugger 방법 https://xilinx.github.io/Embedded-Design-Tutorials/docs/2022.2/build/html/docs/Introduction/ZynqMPSoC-EDT/ZynqMPSoC-EDT.html 참조 petalinux-config -c rootfs를 이용하여 Root File system에 하기 내역을 포함 시킨다. tcf-agent (default enable) openssh-sftp-server dropbear (default disable) 이미지를 빌드 하고 디버깅하고자 하는 Application을 실행 시킨다. QEMU의 경우 GEM0만 연결되어 있으므로 필요 시 GEM3 등의 Device Tree를 추가하여 빌드한다. vitis를 실행 시키고 *.XSA 파일 등을 이용하여 platform project를 구성한다. platform project에 빈 linux Applicaiton domain을 추가한다. 4)항의 항목내 Debug configuration을 이용하여 Single Application Debug를 추가한다. target 보드의 debug IP/port를 설정하고 파일 패스를 설정한다. GNU Debuger GNU 디버거를 사용하기 위해서는 Root file System에 gdbserver를 포함하여야 한다. Custom HW and Driver Development Xilinx는 Custop IP에 대한 디바이스 제어를 위해 하기의 방법을 제안한다. Linux Device Driver 제작 mmap의 사용 (사용이 쉽다. 인터럽트 핸들링이 안됨) User space I/O (UIO 사용) (간단한 IRQ핸들링이 된다, Latency가 가변적이고 DMA가 지원되지 않는다) Petalinux는 빌드 시 Device Tree Generator가 DTSI/DTS파일을 생성하고 DTB를 만든다 *.XSA 파일을 분석하여 기본적인 DTSI/DTS 파일을 만든다 {project-root}/components/plnx_workspace/device-tree/device-tree에 생성되는 DTSI파일은 다음과 같다 pl.dtsi : memory-mapped PL IP node pcw.dtsi : Dynamic properties of the PS peripheral system-top.dts : boot argument 와 console, memory information zynqmp.dtsi : PS peri and CPU information zynqmp-clk-ccf.dtsi : IP peri를 위한 clock information Custop IP 추가 등 Device tree를 업데이트 하기 위해 하기 DTSI를 업데이트 한다. {project-root}/project-spec/meta-user/recipes-bsp/device-tree/files/system-user.dtsi Custom HW and Petalinux 개발 절차 Custop IP를 개발(RTL 등) 후 Vivado IP Packer를 통하여 IP-XACT Standard Format으로 패키징 한다. Vivado를 이용하여 1)항의 IP와 기타 사용자 IP를 조합하여 *.XSA 파일을 생성한다. petalinux-creat -t project -n {project 이름}를 이용하여 project를 생성하고 *.XSA 파일을 import한다. petalinux-creat -t module -n {driver 이름}을 이용하여 모듈을 생성한다. {project-root}/project-spec/meta-user/recipes-bsp/device-tree/files/system-user.dtsi에 Custom IP에 관련된 Device tree를 업데이트한다. 작성 시 pl.dtsi를 확인하여 module name 및 address 등을 확인한다. 모듈 내부 드라이버 파일을 작성하고 Yocto 레시피를 수정한다. 커널에 로딩할 지 모듈로 rootfs에 등록할지 결정한 후 빌드한다. ","date":"2022-12-22T00:00:00Z","permalink":"https://muonkmu.github.io/p/petalinux-petalinux-advance/","title":"[Petalinux] Petalinux Advance"},{"content":"HLS ","date":"2022-12-19T00:00:00Z","permalink":"https://muonkmu.github.io/p/hls-petalinux-basic/","title":"[HLS] Petalinux Basic"},{"content":"Petalinux Basic petalinux 정의 Petalinux는 xillinx FPGA를 위한 임베디드 리눅스 개발 툴로 YOCTO 프로젝트 Wrapper이다. Hardware description file(*.XSA) 또는 BSP 파일을 입력으로 리눅스 이미지 생성 Petalinux 프로젝트의 레이아웃은 프로젝트 생성 시, XSA import 시, build 시 추가/달라짐 (교재 p66을 참조 및 https://docs.xilinx.com/r/2021.1-English/ug1144-petalinux-tools-reference-guide/Image-Selector?tocId=nfcK0XF5PXQyI2ebTdA8fA) 기본 명령어 및 Design Flow 상세 내용은 교제의 Petalinux Tool : Design Flow 및 LAB2 참조\n프로젝트 생성\npetalinux-create -t {type} -n {name} \u0026ndash;template {기초 템플릿} 1 petalinux-create -t project -n test_prj --template zynqMP 프로젝트 설정 : Hardware Description 및 boot, rootfs, kernel\npetalinux-config 또는 petalinux-config -c {rootfs/kernel/device-tree/u-boot} 1 2 cd test_prj petalinux-config --get-hw-description={xsa file} --silentconfig 프로젝트 빌드\npetalinux-build 또는 petalinux-build -c {rootfs/kernel/device-tree/u-boot} 생성되는 파일은 하기와 같다 boot.scr: A u-boot boot script image.ub: U-boot wrapped Linux kernel image rootfs.tar.gz: Compressed root file system tar ball 그외 Pakage를 위한 파일 1 petalinux-build 프로젝트 패키징\n.BIN 또는 .MCS 생성 ( = fsbl + ssbl + pmu + bitstream) .BIN 은 다음과 내용을 포함한다. Platform Loader and Manager (PLM) PS Management (PSM) firmware Platform Device Image (PDI) ARM trusted firmware u-boot Device tree blob 1 petalinux-package --boot --fsbl zynqmp_fsbl.elf --u-boot u-boot.elf --pmufw pmufw.elf --fpga system.bit 부트\nSD카드에 이미지 복사(BOOT.BIN, Image, rootfs.cpio.gz.u-boot, boot.scr) 후 보드 부팅 qemu로 에뮬레이션 가능 1 petalinux-boot --qemu --kernel TFTP를 위한 Jtag 부트 1 petalinux-boot --jtag --prebuilt 2 --hw_server-url tcp:127.0.0.1:3121 Application development 상세 내용은 교제의 p133 Petalinux Application Development 및 LAB3 참조\nPetalinux의 project가 생성된 상태에서 petalinux-create를 사용하여 app을 생성\nproject-spec/meta-user/recipes-apps/{app_name}에서 생성된 파일(bb 및 source) 확인 가능 1 petalinux-create -t apps --name helloworld --template c source 및 makefile을 생성 또는 복사한다.\nproject-spec/meta-user/recipes-apps/{app_name}/file에서 수정한다. Yocto Recipe file를 수정한다.\nproject-spec/meta-user/recipes-apps/{app_name}의 {app_name}.bb파일에 관련파일을 등록한다. root filesystem에 등록한다.\npetalinux-config -c rootfs 수행 후 apps 메뉴에서 등록 build 후 /usr/bin에서 app을 확인 가능하다.\n프로젝트 설정 상세 내용은 교제의 p150 Customizing the project 참조 petalinux-config를 이용하여 하기 설정이 가능하다\nfirmware version 정보 root filesystem 종류 : INITRD, INITRAMFS JFFS2, UBI/UBIFS, NFS, EXT4(SD/eMMC\u0026hellip;) U-boot 이미지 저장 위치 : bootenv 조절을 통해 Jtag/DDR, QSPI, NAND의 image offset을 조정할 수 있다. Primary Flash(QSPI?)의 파티션 조절 가능 File system package를 조절하여 Kernel image size 및 Root file system 이미지 사이즈를 줄일 수 있다. TFTP 부팅을 위한 pre-built 이미지 위치를 설정할 수 있다 NFS 또는 SD card를 통한 Root file system 로딩을 설정 할 수 있다. Root file system customize 상세 내용은 교제의 p212 Customizing the Root File System 참조\ncustom applications, libraries, module을 추가하거나 생성 가능 pre-compiled applications, libraries, module을 추가하거나 생성 가능 YOCTO layer, recipes 또는 package 추가 가능 ","date":"2022-12-19T00:00:00Z","permalink":"https://muonkmu.github.io/p/petalinux-petalinux-basic/","title":"[Petalinux] Petalinux Basic"},{"content":"목표 개인 기술 정리를 위한 블로그의 생성 markdown 사용이 편리한 github.io를 이용하기로 결정 빌드가 빠른 HUGO framework을 사용 (github에서는 Jekyll framework가 기본이나 컨텐츠가 쌓이면 빌드가 느려지는 단점이 있음) Hugo theme는 STACK을 사용 개발 환경 Oracle Cloud Arm server Ubuntu 20.40 code-server 사전 준비 GO 설치 Hugo는 GO로 작성되 있으므로 GO를 설치한다.\nref : https://go.dev/doc/install 필요 시 GO의 설치 경로를 PATH에 등록한다. Hugo 설치 리눅스의 경우 패키지 관리자를 이용하여 설치가 가능하나 이 경우 old 버전이 설치된다. STACK 테마의 경우 최신버전과 hugo extension이 필요하므로 Go를 이용하여 설치한다. https://gohugo.io/installation/linux/ 1 go install -tags extended github.com/gohugoio/hugo@latest 필요 시 Hugo의 설치 경로를 PATH에 등록한다. git repo 생성 hosting을 위한 repo를 생성한다. repo의 이름은 {git ID}.github.io 형식 ex) muonkmu.github.io 호스팅 목적이므로 repo는 public hugo 빌드 전 소스를 보관할 repo를 생성한다. 이름은 상관 없음 ex) blog 소스 보관용이므로 public/private은 개인 취향 블로그 작성 및 배포 hugo 프로젝트 생성 및 테마 설정 프로젝트를 생성 후 폴더 이동, 하기 예제의 이름은 hugoBlog로 가정 1 2 hugo new site hugoBlog cd hugoBlog git 초기화 및 테마 설정 하기 예제에서는 Stack 테마 사용 clone으로 테마 소스를 themes폴더에 넣을 수도 있으나 submodule을 추천 1 2 git init git submodule add https://github.com/CaiJimmy/hugo-theme-stack.git themes/hugo-theme-stack config파일 설정 config.toml을 수정, 하기 예제에서는 stack 테마의 예제 파일을 복사/수정 한다. config.yaml의 baseurl, theme, title 등을 수정한다. 1 2 3 rm config.toml cp themes/hugo-theme-stack/exampleSite/config.yaml ./ cp themes/hugo-theme-stack/exampleSite/content ./ 1 2 3 4 5 baseurl: https://muonkmu.github.io/ languageCode: en-us theme: hugo-theme-stack paginate: 7 title: MW Devlog 컨텐츠 작성 및 테스트 categories, post, page 등을 작성한다. 하기 예제에서는 stack 테마의 예제 파일을 복사/수정 한다. content/post 내 예제 파일을 참조하여 post를 작성한다(예제포스트는 지워도 된다.) 1 2 rm -r content cp themes/hugo-theme-stack/exampleSite/content ./ 테스트 서버를 구동하여 동작을 확인한다. 하기 예제에는 orcle 서버에서 개발하는 것을 가정, 내부 바인딩과 포트를 별도로 할당였다(오라클 서버에서 방화벽에 우선적으로 포트을 열어둬야 함) 웹 브라우저로 테스트 서버에 접속해 동작을 확인한다. 1 hugo server -D --bind=0.0.0.0 -p 8070 빌드 및 배포 github repo를 연결한다. 소스 repo에 프로젝트 폴더를 연결 host repo에 public 폴더를 연결 1 2 3 git remote add origin https://github.com/muonkmu/blog.git rm -r public git submodule add -b master https://github.com/muonkmu/muonkmu.github.io.git public 소스를 빌드한다. 하기 예제에서는 stack 테마의 사용 경우이다. 1 hugo -t hugo-theme-stack 빌드 및 소스 파일을 push 한다. 1 2 3 4 5 6 7 8 9 10 cd public git add . git commit -m \u0026#34;first commit\u0026#34; git branch -M main git push origin main cd .. git add . git commit -m \u0026#34;first commit\u0026#34; git branch -M main git push origin main (option)배포에 시 사용할 쉘 스크립트를 작성한다. ex)deploy.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #!/bin/bash hugo -t hugo-theme-stack cd public git add . msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; git push origin main cd .. git add . msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; git push origin main Debug HUGO 받침 분리 표기 문제 사용하던 중 \u0026lsquo;가\u0026rsquo; 받침이 분리되어 표기되는 문제가 발견되었다. ex) \u0026lsquo;각\u0026rsquo; 이 \u0026lsquo;가ㄱ\u0026rsquo; 로 표기 구글링을 해보니 Droid Sans Fallback 폰트의 문제라고 생각되어 관련 폰트를 삭제하여 문제를 해결 ./themes/hugo-theme-stack/assets/scss/variables.scss 의 --sys-font-family, --zh-font-family 변수 내 Droid Sans 관련 폰트를 모두 삭제한다. ","date":"2022-12-13T00:00:00Z","permalink":"https://muonkmu.github.io/p/github-blog-%EB%A7%8C%EB%93%A4%EA%B8%B0/","title":"Github blog 만들기"},{"content":"본 챕터에서는 몇가지 주요한 Paralle Architecture에 대하여 소개한다. 이 페이지에서는 CPU와 GPU에 대해서 우선 기술한다.\nIntel Central Processing Unit (CPU) https://www.intel.com/content/www/us/en/developer/articles/technical/xeon-processor-scalable-family-technical-overview.html CPU는 병렬 프로세싱을 위해 Single Instruction Single Data (SISD) architecture에서 Single Instruction Multiple Data (SIMD)로 진화함. 그러나 이는 딥러닝과 같은 거대 병렬 처리에 적합하지 못하여 2017년 딥러닝 어플리케이션을 위한 Xeon processor scalable family (purley platform) 발표 Purley platform은 하기 특징을 가짐 Skylake mesh architecture 이전 Grantley platform에서는 Last-Level Chache(LLC)등이 Intel Quick Path Interconnect(QPI) ring achitecture로 연결 상기 구조는 코어 증가 시 코어 마다 사용가능한 bandwidth가 줄어들어서 메모리 latency가 증가 Grantley platform에서는 Intel Ultra Path Interconnect(UPI) mesh archictecture로 업그레이드 Comnined Home Agent(CHA)가 통합, 이는 LLC 등의 주소 정보 지도를 작성하며 이는 mesh 연결에서 목적지까지의 라우팅 정보를 제공 Fig1. Intel Xeon processor Scalable family mesh architecture Intel Ultra Path Interconnect UPI는 어드레스를 공유하는 mutiple processor coherent interconnect UPI는 vertical/horizontal path를 통한 한 코어에서 다른 코어로의 최단 경로를 제공 2소켓, 4소켓 링, 8소켓+크로스바 등 다양한 구조 지원 SubNon-Unified Memory Access Clustering 플랫폼은 모든 코어/LLC를 반씩 + 메모리 컨트롤를 1개씩 가진 SNC 0,1 도메인을 가짐 각 도메인은 각 메모리 컨트롤러에 매핑되는 유니크한 LLC 주소를 가지며 이는 LLC access latency를 낮춤 Cache Hierarchy Change 하기 그림과 같이 변경 LLC 및 MLC size 변경으로 hit rate 증가 Figure 11. Generational cache comparison single/Multiple Socket Parallel Processing UPI와 sub-NUMA의 지원으로 딥러닝 worker process들은 코어셋이나 싱글소켓, 다중소켓에 assign 될 수 있음 Advanced vector software extension Intel Advanced Vector Extension 512(Intel AVX-512)가 Vector Neural Network Instruction(VNNI)를 지원하는 AVX-512)_VNNI로 발전 대충 더 빨라지고 8/16/32 FP vector 연산을 지원한다는 듯(자세한 사항은 책 참조) Math Kernel Library for Deep Neural Network(MKL-DNN) Convolution, pooling, activation, batch normalization으로 구성된 최적화된 MKL-DNN 지원 key feature는 prefetching, data reuse, cache blocking, data layout, vectorization, register blocking이며 자세한 사항은 책 참조 NVIDIA Graphics Processing Unit (GPU) GPU 장점 : 효율적인 floating point 연산, high speed memory support Turing architecture를 개발함 (NVLink2를 위한 HBM2 적용, 캐시 구조 변경 등등) Tensor Core Architecture tensor core란 : 행렬연산 및 MAC를 위한 전용 코어 Turing Tensor core는 이전(Pascal)이 matrix row by row만 지원했으나 4X4X4 연산을 지원하도록 변경 INT8, INT4를 지원하며 정확도를 낮추면 연산 속도 증가 Matrix사이즈가 크면 이를 나누어 연산, 다양한 size의 매트릭스 연산에 대응 가능 https://www.nvidia.com/ko-kr/data-center/tensor-cores/ Winograd Transform 곱셈 횟수를 줄일 수 있는 Winograd Transform을 지원 상기 변환에 대한 연산식은 책과 다른 자료를 참조할 것 Simultaneous Multithreading (SMT) SMT의 경우 Matrix는 행렬을 여러 그룹으로 나누고 이를 병렬로 처리 (Single Instruction Multiple Thread, SIMT 방식) 연산 후 하위 그룹을 재그룹 시킴 High Bandwidth Memory (HBM2) Memory Bottleneck해결을 위해 HBM2 적용 (memory die를 TSV로 뚫어서 스택함) HBM2는 GPU와 NVLink2로 연결됨 NVLink2 Configuration NVLink는 엔비디아가 개발한 와이어 기반 통신 프로토콜 시리얼 멀티 레인 근범위 통신 링크 (PCIE의 속도 문제 해결) Turing 아키텍쳐는 sing MIO를 two×8 bidirectional differential pair NVLink2로 대체 CPU/GPU 메모리 간 directly load/store/atomic 가능 (데이터를 GPU메모리에서 바로 읽을 수 있고 CPU cache에 바로 저장 가능) 다양한 구성을 지원한다. (책을 참조하자) ","date":"2022-12-12T00:00:00Z","permalink":"https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-1/3/","title":"[AI HW Design] Chap03 Parallel Architecture (1/3)"},{"content":"목적 Ubuntu 20.04 LTS 설치 후 나에게 맞는 설정 및 설정 방법 정리 유의사항 설치 시 언어는 영어, 키보드 영어 자판으로 설치를 권장 개인 설정 Nvidia 그래픽 카드 설정 설치 가능한 드라이버 확인 1 ubuntu-drivers devices 권장 드라이버 설치 1 sudo ubuntu-drivers autoinstall 한영키 동작 설정 입력기 설치 : setting → Region and Language → Input Source → Korean(Hangul) 추가 1항의 추가된 항목 설정에서 Hangul Toggle Key를 Hangul만 남김(option) /usr/share/X11/xkb/symbols/altwin 편집 4행의 key \u0026lt;RALT\u0026gt; ... 부분에서 symbols[Gropu1] = [ Alt_R, Meta_R ] 부분을 [ Hangul ] 로 수정한다. VNC 설치 tigerVNC 설치 1 sudo apt-get install tigervnc-standalone-server tigervnc-xorg-extension 비밀번호 설정 1 vncpasswd ~/.vnc/xstartup 작성 1 2 3 4 5 6 #!/bin/sh # Start Gnome 3 Desktop [ -x /etc/vnc/xstartup ] \u0026amp;\u0026amp; exec /etc/vnc/xstartup [ -r $HOME/.Xresources ] \u0026amp;\u0026amp; xrdb $HOME/.Xresources vncconfig -iconic \u0026amp; dbus-launch --exit-with-session gnome-session \u0026amp; vnc 서버 실행 1 vncserver -localhost no vnc 서버 종료 1 vncserver -kill :2 설정변경 : $\u0026gt;sudo vim /etc/vnc.conf 1 2 $geometry = \u0026#34;1920x1080\u0026#34;; $depth = \u0026#34;16\u0026#34;; SSH 설치 서버 설치 1 sudo apt install openssh-server 실행여부 확인 1 sudo systemctl status ssh 서버 실행 1 2 sudo systemctl enable ssh sudo systemctl start ssh xforward 설정 팡일의 /etc/ssh/ssh_config 의 x11Forward no → x11Forward yes로 변경 ssh서버 재실행 및 클라언트 실행 시 -X 옵션 추가 ZSH/om-my-zsh 설치 및 설정 zsh 설치 1 sudo apt-get install zsh 설치확인 1 cat /etc/shells 기본쉘 변경 1 chsh -s $(which zsh) oh-my-zsh 설치(curl설치필요) 1 sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; 테마변경 ~/.zshrc 파일 내 ZSH_THEME=\u0026quot;agnoster\u0026quot; 로 변경 글자깨질 시 Powerline폰트 설치 1 sudo apt-get install fonts-powerline 커맨드라인 컴퓨터 이름 감추기 ~/.zshrc 하단에 하기 내용 추가 1 2 3 4 5 prompt_context() { if [[ \u0026#34;$USER\u0026#34; != \u0026#34;$DEFAULT_USER\u0026#34; || -n \u0026#34;$SSH_CLIENT\u0026#34; ]]; then prompt_segment black default \u0026#34;%(!.%{%F{yellow}%}.)$USER\u0026#34; fi } zsh-autosuggestions 플러그인 설치 1 git clone https://github.com/zsh-users/zsh-autosuggestions.git $ZSH_CUSTOM/plugins/zsh-autosuggestions zsh-syntax-highlighting 플러그인 설치 1 git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $ZSH_CUSTOM/plugins/zsh-syntax-highlighting autojump 설치 1 2 3 git clone https://github.com/wting/autojump.git cd autojump ./install.py 사용법 j [디렉토리 명] 또는 j -s 플러그인 활성화\n~/.zshrc 파일 내 plugins=(git zsh-autosuggestions zsh-syntax-highlighting autojump) 로 변경 줄바꿈 적용(멀티라인 입력)\n~/.oh-my-zsh/themes/agnoster.zsh-theme파일 수정 prompt_hg 하단에 prompt_newline 추가 후 파일 최하단 하기 프롬프트 추가 1 2 3 4 5 6 7 8 9 10 prompt_newline() { if [[ -n $CURRENT_BG ]]; then echo -n \u0026#34;%{%k%F{$CURRENT_BG}%}$SEGMENT_SEPARATOR %{%k%F{blue}%}$SEGMENT_SEPARATOR\u0026#34; else echo -n \u0026#34;%{%k%}\u0026#34; fi echo -n \u0026#34;%{%f%}\u0026#34; CURRENT_BG=\u0026#39;\u0026#39; } (option) TFPT 설치 xilinx petalinux를 사용할 생각이라면 tftp 설치가 필요하다\ntftp 설치 1 2 sudo apt-get update sudo apt-get install tftpd-hpa 서비스 확인 1 sudo service tftpd-hpa status 설정 파일 /etc/default/tftpd-hpa 를 원하는 대로 수정한다. 다른 것은 크게 의미가 없고 up/down 위치인 TFTP_DIRECTORY 정도만 수정 수정 후 디렉토리 권한 설정을 해준다. 1 2 3 4 vim /etc/default/tftpd-hpa sudo mkdir {tftp-dir} sudo chmod 777 {tftp-dir} sudo chown -R tftp:tftp {tftp-dir} 설정 완료 후 재시작 1 sudo service tftpd-hpa restart (option) NFS server 설치 nfs서버 패키지 설치 nfs 서버용 폴더를 만들고 모든 클라이언트 머신이 공유 디렉토리에 액세스하기 위하여 권한 제거 및 파일의 권한 제거 /etc/exports 파일을 편집하여 공유할 폴더를 지정하고 클라언트 및 실행 권한 설정 exportfs로 설정된 폴더 내보내기 nfs_server 재시작 1 2 3 4 5 6 7 sudo apt install nfs-kernel-server mkdir ${공유폴더} sudo chown -R nobody:nogroup ${공유폴더} sudo chmod 777 ${공유폴더} sudo echo \u0026#39;${공유폴더} 192.168.1.1/24(rw,sync,no_root_squash,no_subtree_check)\u0026#39; \u0026gt;\u0026gt; /etc/exports sudo exportfs -a sudo service nfs-kernel-server restart ","date":"2022-12-12T00:00:00Z","permalink":"https://muonkmu.github.io/p/ubuntu-20.04-%EA%B0%9C%EC%9D%B8-%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95/","title":"Ubuntu 20.04 개인 환경 설정"},{"content":"목적 클라우드 서버를 이용하여 원격으로 접속 가능한 개발 서버의 구축 최종 목표 고정 IP를 가진 ubuntu 서버 무료 클라우드 서버 중 오라클이 ARM64-4core/24GB ram/200GB storage VM 머신 제공 (타사 대비 월등히 좋음) 원격 개발을 위한 code-server 설치 서버 구축 클라우드 서버 구축 오라클 클라우드 Free tier 가입 리전은 원하는 곳(춘천이 빠르고 ARM 서버 리소스가 남음) 카드 정보를 기입(실제로 결제가 되지는 않음) 가입 완료 후 하단의 Create a VM instance 시작 instance Name 입력 image는 원하는거 선택, ex) canonical Ubuntu 20.04 shape는 Ampere 선택 core는 4, memory는 24GB 까지 무료 상기 리소스를 나누어 무료 VM를 생성할 수 있다.ex) 2core-12GB 인스턴스 2개 무료 VCN이 없다면 페이지에서 VCN을 생성하여 연결 본인의 PC에서 SSH를 생성하여 Public키를 업로드 한다. http://taewan.kim/oci_docs/98_misc_tips/ssh_key_pairs/ 부트 볼륨 생성 Specify a custom boot volume size을 클릭 후 원하는 볼륨생성 200GB까지 무료이며 상기 리소스를 나누어 무료 VM생성 가능 Create로 생성 해당 리전의 리소스가 부족하여 생성이 안되는 경우가 있다. 상기의 경우 리소스가 풀릴 때 까지 기다리거나 유료계정으로 업그레이드 (승인되는데 시간 걸림) 유료 계정이 되더라도 무료 리소스까지만 쓰면 과금이 되지 않는다. 클라우드 서버 환경 설정 고정 IP 설정 Compute \u0026gt; Instances \u0026gt; Instance Details \u0026gt; Attached VNICs \u0026gt; VNIC Details \u0026gt; IPv4 Addresses 상기 경로에서 NO PUBLIC IP 선택하여 IP 삭제 후 RESERVED PUBLIC IP로 변경 우분터 사용자 계정 생성(option) ssh 로그인 현재 계정 ubuntu 암호 생성 사용자 계정 생성 생성 계정에 sudo 권한 부여 계정 변경 ssh 비번으로 접속 설정 /etc/ssh/sshd_config파일의 PasswordAuthentication 값을 \u0026ldquo;yes\u0026quot;로 변경 클라우드 포트 개방 Networking \u0026gt; Virtual Cloud Networks \u0026gt; {사용중인 VNC} \u0026gt; Security List Details 상기 경로에서 포트 개방 추가 우분투 방화벽 포트 개방 1 sudo iptables -I INPUT 5 -p tcp --dport 8070 -m state --state NEW,ESTABLISHED -j ACCEPT code-server 설치 code-server 다운로드 및 설치 https://coder.com/docs/code-server/latest/install 1 curl -fsSL https://code-server.dev/install.sh | sh 서비스로 실행하기 위해 systemctl로 enable 1 sudo systemctl enable --now code-server@$USER 외부 접속을 위해 .config/code-server/config.yaml파일을 수정한다. 1 2 3 4 bind-addr: 0.0.0.0:{포트번호} auth: password password: {비밀번호} cert: false 서비스를 재시작 후 동작을 확인한다. 1 2 sudo systemctl restart --now code-server@$USER sudo systemctl status code-server@$USER chrome 브라우저에서 접속 시 이미지가 안보일 경우 하기 세팅을 수행 chrome://flags 설정 의 Insecure origins treated as secure Enable 후 http://{접속IP}:{접속Port} 추가 ","date":"2022-12-12T00:00:00Z","permalink":"https://muonkmu.github.io/p/%EC%9B%90%EA%B2%A9-%EA%B0%9C%EB%B0%9C-%EC%84%9C%EB%B2%84-%EA%B5%AC%EC%B6%95/","title":"원격 개발 서버 구축"},{"content":"본 chapter에서는 Gradient를 구하기 위한 Backpropagation을 이해하고 Neural Network의 기본에 대해 설명한다.\nVideo : https://www.youtube.com/watch?v=d14TUNcbn1k Slide : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf (TODO: 귀차니즘의 압박으로 정리를 안해놓았지만 언젠간 해야지)\nBackpropagation Chain rule Sigmoid gate example Patterns in backward flow Gradients add at branches Vectorized operations Neural Network Artificial Neural Network Activation Function Neural networks Architectures ","date":"2022-10-25T00:00:00Z","permalink":"https://muonkmu.github.io/p/cs231n-chap-04-introduction-to-neural-networks/","title":"[CS231n] Chap 04 Introduction to Neural Networks"},{"content":"본 chapter에서는 딥러닝의 기본 개념인 Loss Function, Regularization, Optization(Gradient Descent)에 대해 다룬다\nVideo : https://www.youtube.com/watch?v=h7iBpEHGVNc Slide : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture3.pdf (TODO: 귀차니즘의 압박으로 정리를 안해놓았지만 언젠간 해야지)\nLoss function Regularization Softmax and SVM Optimization Image Feature ","date":"2022-10-17T00:00:00Z","permalink":"https://muonkmu.github.io/p/cs231n-chap-03-loss-function-and-optimization/","title":"[CS231n] Chap 03 Loss Function and Optimization"},{"content":"본 chapter에서는 Computer Vision의 핵심 Task 중 하나인 Image classification에 대해 이해하고 초기의 방법인 K-Nearest Neighbor Algorithm과 Linear Classification에 대하여 다룬다.\nVideo : www.youtube.com/watch?v=OoUX-nOEjG0\u0026list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk\u0026index=2 Slide : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture2.pdf (TODO: 귀차니즘의 압박으로 정리를 안해놓았지만 언젠간 해야지)\nImage Classification 개요 K-Nearest Neighbor Algorithm Linear Classification ","date":"2022-10-08T00:00:00Z","permalink":"https://muonkmu.github.io/p/cs231n-chap-02-image-classification/","title":"[CS231n] Chap 02 Image classification"},{"content":"6개월에 걸쳐 수료를 완료 했다. 3개월 코스라고 하던데\u0026hellip;\nhttps://www.coursera.org/learn/machine-learning homework repo : https://github.com/muonkmu/Coursera_AndrewNg_ML_Program.git (TODO: 귀차니즘의 압박으로 정리를 안해놓았지만 언젠간 해야지)\n","date":"2022-01-25T00:00:00Z","permalink":"https://muonkmu.github.io/p/coursera_ml-course-certificate/","title":"[Coursera_ML] Course certificate"},{"content":"이번 강의에서는 대규모의 대규모의 데이터가 있을 때, 처리하는 알고리즘에 대해서 알아보자.\nhttps://www.coursera.org/learn/machine-learning homework repo : https://github.com/muonkmu/Coursera_AndrewNg_ML_Program.git (TODO: 귀차니즘의 압박으로 정리를 안해놓았지만 언젠간 해야지)\n","date":"2022-01-07T00:00:00Z","permalink":"https://muonkmu.github.io/p/coursera_ml-week_10-gradient-descent-with-large-datasets/","title":"[Coursera_ML] Week_10) Gradient Descent with Large Datasets"}]