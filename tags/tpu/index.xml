<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>TPU on MW Devlog</title>
        <link>https://muonkmu.github.io/tags/tpu/</link>
        <description>Recent content in TPU on MW Devlog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Fri, 30 Dec 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://muonkmu.github.io/tags/tpu/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[AI HW Design] Chap03 Parallel Architecture (2/3)</title>
        <link>https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/</link>
        <pubDate>Fri, 30 Dec 2022 00:00:00 +0000</pubDate>
        
        <guid>https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/</guid>
        <description>&lt;p&gt;본 챕터에서는 몇가지 주요한 Paralle Architecture에 대하여 소개한다.
이 페이지에서는 NVDLA와 Google TPU에 대해서 기술한다.&lt;/p&gt;
&lt;h2 id=&#34;nvidia-deep-learning-accelerator-nvdla&#34;&gt;NVIDIA Deep Learning Accelerator (NVDLA)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA는 FPGA로 구성 가능한 추론을 위한 오픈소스 아키텍쳐 (&lt;a class=&#34;link&#34; href=&#34;http://nvdla.org&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://nvdla.org&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Primitive functional blocks으로 CNN을 지원 (convolution, activation, pooling, normalization)&lt;/li&gt;
&lt;li&gt;각 블럭은 next layer의 active와 configuration을 위한 double buffer를 가짐&lt;/li&gt;
&lt;li&gt;next layer의 operation은 active operation이 완료되어야 시작&lt;/li&gt;
&lt;li&gt;independent mode와 pipeline을 사용하는 fused mode가 있음
&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/NVDLA_core_architecture.png&#34;
	width=&#34;1142&#34;
	height=&#34;950&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/NVDLA_core_architecture_hu2cca1af52d961361ac30bee8eb97a9b7_113988_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/NVDLA_core_architecture_hu2cca1af52d961361ac30bee8eb97a9b7_113988_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;NVDLA core architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;120&#34;
		data-flex-basis=&#34;288px&#34;
	
&gt;
&lt;em&gt;Figure. NVDLA core architecture&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convolution-operation&#34;&gt;Convolution Operation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Direct convolution, Image input convolution, winograd convolution, Batch convolution 지원 (상세내역은 책 참조)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;single-data-point-operationsdp&#34;&gt;Single Data Point Operation(SDP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SDP는 linear functions와 Look-up Table nonlinear functions을 통해 activation과 normalizatin을 지원 (상세내역은 책 참조)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;planar-data-operationpdp&#34;&gt;Planar Data Operation(PDP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;PDP는 maximum/minimum/average pooling을 지원&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multiplane-operation&#34;&gt;Multiplane Operation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cross Channel Data Processor(CPD)은 Local Response Normalization(LRN)을 수행&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-memory-and-reshape-operations&#34;&gt;Data Memory and Reshape Operations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;bridge DMA는 외부 메모리와 메모리 인터페이스간 데이터 전송을 담당&lt;/li&gt;
&lt;li&gt;data reshape engine은 data trasnformations, splitting, slicing, merging, contraction, reshape transpose 를 담당&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;system-configuration&#34;&gt;System Configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA는 small/large system model로 구현할 수 있음
&lt;ul&gt;
&lt;li&gt;small system model : IoT 기기와 같이 작은 모델을 위한 모델, 복잡도와 storage를 낮추고 single task를 수행&lt;/li&gt;
&lt;li&gt;large system model : mutiple task를 위한 coprocessor와 메모리 인터페이스 추가&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;external-interface&#34;&gt;External Interface&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA는 외부와 통신을 위한 Configuration Space Bus(CSB), Data backbone(DBB), SRAM interface, Interrupt interface를 가짐 (상세내용은 책 참조)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;software-design&#34;&gt;Software Design&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA SW는 Trained model을 parser/compiler/optimizer를 통해 loadable로 변환&lt;/li&gt;
&lt;li&gt;User Mode Driver(UMD)에 의해 Loadalbe이 로딩 되고 Job이 Kernel Mode Driver(KMD)로 제출됨, KMD는 스케줄링 수행&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;google-tensor-processing-unittpu&#34;&gt;Google Tensor Processing Unit(TPU)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;구글은 speech recognition 수요 해결을 위해 TPU v1(stand alone)과 v2/v3(cloud)를 개발&lt;/li&gt;
&lt;li&gt;TPU v1은 하기 스펙으로 MLP 0/1, CNN 0/1, RNN 0/1 6가지 neural network application을 수행 가능
&lt;ul&gt;
&lt;li&gt;256 × 256 eight bits MAC unit&lt;/li&gt;
&lt;li&gt;4 Mb on-chip Accumulator Memory (AM)&lt;/li&gt;
&lt;li&gt;24 Mb Unified Buffer (UB) – activation memory&lt;/li&gt;
&lt;li&gt;8 Gb off-chip weight DRAM memory&lt;/li&gt;
&lt;li&gt;Two 2133 MHz DDR3 channels&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TPU는 6가지 neural network application을 수행할 수 있음
&lt;ul&gt;
&lt;li&gt;Multi-layer perceptron(MLP) 0/1, Convolution Neural Network(CNN) 0/1, Recurrent Neural Network(RNN) 0/1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;system-architecture&#34;&gt;System Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;TPU v1은 매트릭스 연산을 Matrix Multiply Unit(MMU)에서 수행&lt;/li&gt;
&lt;li&gt;MMU는 256 × 256 eight bits MAC unit이며 16bit 연산을 수행할 경우 성능은 8bit 대비 절반(Sparse matrix 연산을 지원하지 않음)&lt;/li&gt;
&lt;li&gt;Weight FIFO는 matrix weight를 8Gb DRAM에서 읽어오며 activation, pooling, normalization 후 중간 연산 결과를 24Mb Unified Buffer에 저장&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/tpu-15dly1.max-500x500.PNG&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;TPU System Architecture&#34;
	
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multipy-accumulatemac-systolic-array&#34;&gt;Multipy-Accumulate(MAC) Systolic Array&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Systolic array는 TPU의 핵심이자 High throughput / low latency를 가진 SIMD pipeline.&lt;/li&gt;
&lt;li&gt;책에 별 설명이 없으므로 이에 대한 내용은 더 찾아보는 것이 좋다(다른 많은 곳에 잘 나와 있음)&lt;/li&gt;
&lt;li&gt;단점은 전력 소모가 많다는 것(데이터 센터 등에 적합)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;new-brain-floating-point-format&#34;&gt;New Brain Floating-point Format&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;TPU v1은 input data를 FP32에서 INT8로 quantization하여 연산하며 이에 따라 안정성/정확도 문제 발생&lt;/li&gt;
&lt;li&gt;이를 위해 IEEE FP16 대신 Brain Floating Point format (BFP16) 사용
&lt;ul&gt;
&lt;li&gt;BFP16 : Mantissa를 7bit으로 줄이고 exponent를 FP32와 같은 8bit으로 늘림&lt;/li&gt;
&lt;li&gt;Sign 1bit, Exponent 8bit, Mantissa 7bit&lt;/li&gt;
&lt;li&gt;multiplier area와 power를 줄이고 Scaling loss 없이 FP32와 동일한 정확도를 얻음&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;performance-comparision&#34;&gt;Performance Comparision&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;roof-line model의 관점에서 볼 때 TPU가 가장 높은 peak performance를 달성했다.&lt;/li&gt;
&lt;li&gt;roof-line model은 Y축이 성능을 나타내며(평평한 부분이 최고 성능), X축이 byte 당 operation intensity&lt;/li&gt;
&lt;li&gt;부가 설명을 하자면 한번에 얼마나 많은 연산을 수행하게 할 때 성능이 어디까지 올라가는지 지표, loof-line은 메모리 Bandwith 때문에 걸림&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cloud-tpu-configuration&#34;&gt;Cloud TPU configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;TPU v2/v3는 v1에서 DDR을 HBM으로 바꾸고 v1을 Pod로 묶음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cloud-software-architecture&#34;&gt;Cloud Software Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;구글은 cloud computation을 위해 새로운 SW 아키텍쳐를 개발&lt;/li&gt;
&lt;li&gt;Model을 TensorFlow를 통해 computational graph로 해석&lt;/li&gt;
&lt;li&gt;상세 내용은 책을 참조&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
