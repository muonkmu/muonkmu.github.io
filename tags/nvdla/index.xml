<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>NVDLA on MW Devlog</title>
        <link>https://muonkmu.github.io/tags/nvdla/</link>
        <description>Recent content in NVDLA on MW Devlog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sat, 16 Dec 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://muonkmu.github.io/tags/nvdla/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[NVDLA] NVDLA Primer</title>
        <link>https://muonkmu.github.io/p/nvdla-nvdla-primer/</link>
        <pubDate>Sat, 16 Dec 2023 00:00:00 +0000</pubDate>
        
        <guid>https://muonkmu.github.io/p/nvdla-nvdla-primer/</guid>
        <description>&lt;h2 id=&#34;abtract&#34;&gt;Abtract&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;딥러닝 추론을 위한 대부분의 컴퓨팅 작업은 convolutions, activations, pooling, normalization로 그룹화 될 수 있음, 이는 예측가능한 메모리 접근 패턴과 병렬화 하기 적합한 특성을 지님&lt;/li&gt;
&lt;li&gt;NVDLA 간단하고 유연하며 견고한 추론 가속화 솔루션을 제공(다양한 성능 수준을 제공,)&lt;/li&gt;
&lt;li&gt;NVDLA는 HW/SW 스택을 제공
&lt;ul&gt;
&lt;li&gt;HW : Verilog model(합성/시뮬레이션용 RTL), TLM SystemC(소프트웨어 개발 /시스템통합 / 검증용)&lt;/li&gt;
&lt;li&gt;SW : On-device SW stack(동작 바이너리?), training intrastructure(신규모델 개발용), parser(컴파일러 역활)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;accelerating-deep-learning-inference-using-nvdla&#34;&gt;Accelerating Deep Learning Inference using NVDLA&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA는 모듈러 구조이며 각 모듈은 독립적으로 구성 및 확장 가능하다(scalalbe, CONV core를 더 넣거나 average engine을 뺄 수 있다.)
&lt;ul&gt;
&lt;li&gt;Convolution Core : convolution engine&lt;/li&gt;
&lt;li&gt;Single Data Processor : Activation function을 위한 single point lookup engine&lt;/li&gt;
&lt;li&gt;Planar Data Processor : pooling을 위한 planar averaging engine&lt;/li&gt;
&lt;li&gt;Channel Data Processor : Normalization을 위한 Multi-channel averaging engine&lt;/li&gt;
&lt;li&gt;Dedicated Memory and Data Reshape Engine : tensor reshape 와 copy를 위한 mem-to-mem 가속기(DMA?)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;각 유닛의 스케줄링은 co-processor나 CPU에 의해 동작(fine-grained scheduling)
&lt;ul&gt;
&lt;li&gt;NVDLA 모듈 내 co-processor가 있는 걸 headed,  CPU가 스케쥴링 하는 걸 Headless라고 하는 듯 (현재는 릴리즈 된건 headless 만)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NVDLA 컨트롤 채널은 AXI bus를 사용(레지스터, 인터럽트 포함), primary memory IF는 시스템 DRAM 연결용, HBM 연결을 위한 2nd-mem-IF를 optional로 제공&lt;/li&gt;
&lt;li&gt;일반적 인터페이싱 플로우는 Command-excute-interrupt 구조
&lt;ul&gt;
&lt;li&gt;CPU나 co-processor가 layer의 hardware configuration 정보를 active command와 전송(이는 데이터 디펜던시가 없다면 multiple layer를 병렬로 처리 가능)&lt;/li&gt;
&lt;li&gt;모든 엔진은 config register를 위한 더블 버퍼를 가지고 있어서 active layer가 끝나는 직시 second layer가 실행 가능(다음 레이어 정보를 미리 로드할 수 있다는 듯)&lt;/li&gt;
&lt;li&gt;엔진 동작이 끝나면 인터럽트로 알림&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NVDLA는 두가지 모델을 제공(제안?)&lt;/li&gt;
&lt;li&gt;small system 모델은 headless로 저사양, large system model은 co-processor/sram-IF가 추가된 고성능 IoT용(sram은 NVDLA의 캐시로 사용)
&lt;ul&gt;
&lt;li&gt;사이트에 더 상세한 설명(그러나 일반적인 내용)이 기술되어 있음&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;http://nvdla.org/_images/nvdla-primer-system-comparison.svg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;hardware-architecture&#34;&gt;Hardware Architecture&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA는 두 가지 operation 모드로 프로그램 될 수 있음
&lt;ul&gt;
&lt;li&gt;Independent : 각 블럭은 독립적으로 동작, 각 블럭은 지정된 메모리 영역 읽으면서 동작 시작, 끝나면 지정된 메모리 영역에 데이터를 씀&lt;/li&gt;
&lt;li&gt;Fused : Independent와 비슷하나 몇몇 블럭이 파이프라인(FIFO)을 통해 통합될 수 있음&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;http://nvdla.org/_images/nvdla-primer-core-diagram1.svg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;connection&#34;&gt;Connection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA는 다른 시스템과 연결을 위한 3가지 인터페이스를 제공
&lt;ul&gt;
&lt;li&gt;Configureation Space Bus(CSB) : NVDLA config register 접근을 위한 32비트 버스(low BW/Power), AMBA나 OCP 등으로 쉽게 변환 가능&lt;/li&gt;
&lt;li&gt;Interrupt : 1bit level driven, task의 완료나 에러를 통지&lt;/li&gt;
&lt;li&gt;Data Backbone (DBB) : 메인 시스템 메모리 연결을 위한 유사 AXI 버스, address-size / bus-width / burst-size 등을 조절 가능&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;DBB는 On-Chip SRAM 등을 연결하기 위한 optional 인터페이스를 구현할 수 있음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Convolution : &lt;code&gt;tf.nn.conv2d&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;input &amp;ldquo;feature&amp;quot;와 &amp;ldquo;weight&amp;quot;의 convolution engine, 다양한 사이즈에 매핑 시키기 위해서 파라메터를 expose함(무슨 의미?)&lt;/li&gt;
&lt;li&gt;일반적 콘볼루션을 위한 optimiztion 포함, sparse weight compression / Winograd Convolution / Batch convolution 지원&lt;/li&gt;
&lt;li&gt;weight 와 input feature 저장을 위한 convolution buffer(RAM)을 내장&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Single Data Point Processor : &lt;code&gt;tf.nn.batch_normaliztion&lt;/code&gt;, &lt;code&gt;tf.nn.bias_add&lt;/code&gt;, &lt;code&gt;tf.nn.elu&lt;/code&gt;, &lt;code&gt;tf.nn.relu&lt;/code&gt;, &lt;code&gt;tf.sigmoid&lt;/code&gt;, &lt;code&gt;tf.tanh&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Convloution 이후에 쓰임,&lt;/li&gt;
&lt;li&gt;non-linear function 구현을 위한 lookup table을 가짐, linear-funtion은 bias와 scaling 지원&lt;/li&gt;
&lt;li&gt;위의 두 함수의 조합으로 대부분의 activation function 및 element-wise 연산을 지원&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Planar Data Processor : &lt;code&gt;tf.nn.avg_pool&lt;/code&gt;, &lt;code&gt;tf.nn.max_pool&lt;/code&gt;, &lt;code&gt;tf.nn.pool&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;runtime 중 다른 사이즈의  pool group config 지원, max/avg/min pool 지원&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cross-channel Data Processor : &lt;code&gt;tf.nn.local_response_nomaliztion&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;local response nomalization(LRN)을 위한 유닛&lt;/li&gt;
&lt;li&gt;LRN : Alexnet에 사용된 것으로 activation map의 같은 위치에 있는 픽셀끼리 정규화, 요즘은 Batch normalization을 써서 잘안쓴다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Data Reshape Engine : &lt;code&gt;tf.nn.conv2d_transpos&lt;/code&gt;, &lt;code&gt;tf.concat&lt;/code&gt;, &lt;code&gt;tf.slice&lt;/code&gt;, &lt;code&gt;tf.transpose&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;data format transformation(slicing, merging, reshape-transpose 같은)을 수행&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Bridge DMA
&lt;ul&gt;
&lt;li&gt;System DRAM과 high-performance memory interface 간 data copy / move engine&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;configurability&#34;&gt;Configurability&lt;/h3&gt;
&lt;p&gt;(NVDLA는 광범위한 하드웨어 파라메터를 제공)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data type : int4/8/16/32, fp16/32/64 지원 (선택가능한 데이터 타입은 바이너리를 포함?)&lt;/li&gt;
&lt;li&gt;Input image memory formats: planar/semi-planar image와 packaged memory format을 지원
&lt;ul&gt;
&lt;li&gt;RGB 이미지가 RGB RGB RGB 형태이면 packaged, RRR BBB GGG 형태이면 planar&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Weight compression : sparsely storing convolution weight를 on/off 가능&lt;/li&gt;
&lt;li&gt;Winograd convolution : winograd convolution 기능을 on/off 가능&lt;/li&gt;
&lt;li&gt;Batched Convolution : Batched convolution(메모리 밴드위스를 줄이기 위한) 기능 on/off 가능&lt;/li&gt;
&lt;li&gt;Convolution buffer size : buffer bank 2~32, 각 bank의 size 4/8KiB 선택 가능&lt;/li&gt;
&lt;li&gt;MAC array size : MAC은 C(width dimension, 8~64) * K(depth dimension, 4~64) 형태, 조절 가능&lt;/li&gt;
&lt;li&gt;Second memory interface : high-speed access를 위한 추가 메모리 인터페이스 포함 여부 선택 가능&lt;/li&gt;
&lt;li&gt;Non-linear activation function : 비선형 함수(sigmoid, tanh등)을 위한 lookup table 삭제 가능&lt;/li&gt;
&lt;li&gt;Active engine size : 한 사이클당 생산되는 activation output이 1~16까지 조절 가능&lt;/li&gt;
&lt;li&gt;Bridge DMA : 삭제 가능&lt;/li&gt;
&lt;li&gt;Data reshape engine : 삭제 가능&lt;/li&gt;
&lt;li&gt;Pooling engine presence : pooling engine 삭제 가능&lt;/li&gt;
&lt;li&gt;Pooling engine size : 한 사이클당 1~4개의 출력을 생성하도록 조절 가능&lt;/li&gt;
&lt;li&gt;Local response nomalization engine presence : 삭제가능&lt;/li&gt;
&lt;li&gt;Local response nomalization engine size : 한 사이클당 1~4개의 출력을 생성하도록 조절 가능&lt;/li&gt;
&lt;li&gt;Memory interface bit width : 외부 메모리 인터페이스 width에 따라 조절 가능&lt;/li&gt;
&lt;li&gt;Memory read latency tolerance : memory latency time(read request 부터 return까지 cycle) 조절 가능 (read DMA의 latency buffer size에 영향)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;software-design&#34;&gt;Software Design&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA는 on-device sw stack, full training infrastructure를 포함하는 sw 생태계를 지원&lt;/li&gt;
&lt;li&gt;compilation tool(model conversion)과 runtime environement(runtime sw load/excute) 그룹으로 나뉨&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;compilation-tools--model-creation-and-compilation&#34;&gt;Compilation Tools : Model Creation and Compilation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;copilation tool은 compiler와 parse를 포함
&lt;ul&gt;
&lt;li&gt;compiler : NVDLA config에 따른 최적화된 HW layer sequence를 생성&lt;/li&gt;
&lt;li&gt;parser : caffe model을 읽어서 &amp;ldquo;intermediate representation&amp;quot;을 생성&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;compiler는 intermediate representation과 NVDLA HW config를 입력받아 HW layer network를 생성&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;runtime-environment--model-inference-on-device&#34;&gt;Runtime Environment : Model Inference on Device&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA RTE는 running model을 포함하며 2개의 layer로 구분됨
&lt;ul&gt;
&lt;li&gt;User Mdoe Driver(UMD) : compiler는 NVDLA loaderble(binary?)를 생성하는데 이를 로드하고 Kernel mode 드라이버에 interfernce job을 제공&lt;/li&gt;
&lt;li&gt;Kernel Mode Driver(KMD) : 펌웨어와 드라이버로 구성되며 layer를 스케줄링하고 register를 설정&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;HW의 NVDLA function block처럼 SW 관점에서는 모듈을 &amp;ldquo;layer&amp;quot;로 불림(UMD, KMD), layer에는 layer간 dependency, tensor, config에 대한 정보를 포함&lt;/li&gt;
&lt;li&gt;각 layer는 dependancy graph를 통해 연결(KMD가 각 operation을 위해 이를 이용)&lt;/li&gt;
&lt;li&gt;UMD : image load, input/output memory binding, inference run을 위한 API, 리눅스에서는 ioctl()일 수 있음&lt;/li&gt;
&lt;li&gt;KMD : interferece job을 수신 및 선택(multi-processing일때)하여 core engine scheduler에 전송&lt;/li&gt;
&lt;li&gt;core engine scheduler는 인터럽트를 핸들링/ function block 스케줄링 / dependency 업데이트를 함, 스케쥴러는 dependency graph를 이용하여 subsequent layer가 스케쥴링 준비가 되었는지 결정&lt;/li&gt;
&lt;li&gt;KMD/UMD 둘다 API 형태로 제공되므로 이를 wrapping하여 portability layer를 integrator가 작성해야 함&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;nvdla-system-integration&#34;&gt;NVDLA System Integration&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA는 요구사항을 맞출 수 있는 광범위한 파라메터를 제공, 원하는 성능을 위해 MAC unit의 수 / convolution buffer size / on-chip SRAM size를 선택하는 것은 중요한 스텝임&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tuning-question&#34;&gt;Tuning Question&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What math precision is required for the workload expected for any given instantiation?
&lt;ul&gt;
&lt;li&gt;NVDLA의 사용 면적은 대부분이 MAC 유닛과 conv buffer에 의해 결정&lt;/li&gt;
&lt;li&gt;트레이닝 시에는 32bit fp를 사용하지만 8bit로 줄일 수도 있다..(그냥 양자화 하라는 이야기인듯)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What are the number of MAC unit, and the required memory bandwidth?
&lt;ul&gt;
&lt;li&gt;MAC throughput이나 memory bandwidth가 bottleneck이 될 수 있다.&lt;/li&gt;
&lt;li&gt;MAC unit의 수는 결정하기 쉬운데 layer의 input/output size, kernel size가 결정되 있기 때문이다. 요구되는 operation 수를 MAC unit의 수로 나누면 레이어를 처리하는데 필요한 클럭 사이클이 나옴&lt;/li&gt;
&lt;li&gt;Memory BW 경우 input image, output image, weigth를 한번씩 읽어야 하므로 최소 cycle은 상기 합을 읽고 써야하는 샘플의 수로 나눈 값이다. 그러나 convolution buffer가 작아서 iput이나 weight를 한번에 저장할 수 없다면 작업은 나누어 져야 한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Is there a need for on-chip SRAM?
&lt;ul&gt;
&lt;li&gt;external memory bandwidth가 성능/비용 코스트가 많이 든다면 on-chip SRAM이 second level cache로써 도움이 될 수 있다.(SRAM이 시스템 메모리보다 빨라야 함)&lt;/li&gt;
&lt;li&gt;convolution buffer를 키우는 것보다 SRAM이 싸지만 convolution buffer limited 상황에서는 SRAM이 큰 효과를 주는 factor는 아니다.&lt;/li&gt;
&lt;li&gt;즉, 레이어의 대역폭이 제한된 경우, 시스템 메모리의 2배수로 동작하는 입력이미지를 저장하기에 충분한 SRAM이 있다면 성능은 두배가 됨, 그러나 컨볼루션 버퍼의 크기에 의하여 성능이 제한된다면 대역폭 문제가 아니므로 SRAM 추가해도 별 소용 없다.&lt;/li&gt;
&lt;li&gt;결국, Convolution buffer 크기는 Bandwidth 요구량을 줄여주고, SRAM은 가용한 대역폭을 늘려준다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example-area-and-performance-with-nvdla&#34;&gt;Example Area and Performance with NVDLA&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;사이트에 MAC unit 수, Conv buffer 사이즈 등에 따른 NVDLA에서 ResNet-50의 성능 및 power estimation/silicon area이 나와 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sample-platform&#34;&gt;Sample platform&lt;/h3&gt;
&lt;h4 id=&#34;simulation&#34;&gt;Simulation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;GreenSocs QBOX 기반의 시뮬레이션 플랫폼&lt;/li&gt;
&lt;li&gt;QEMU CPU 모델(x86, ARMv8)은 NVDLA SystemC와 결합하여 SW 개발/디버깅을 위한 register-accurate system 제공&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;fpga&#34;&gt;FPGA&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;FPGA를 위한 NVDLA verilog model(합성 가능)&lt;/li&gt;
&lt;li&gt;PPA 최적화를 위한 노력이 들어가있지 않으므로 SoC 성능과 비교하지 말라고 되어 있음&lt;/li&gt;
&lt;li&gt;Amazon EC2 &amp;ldquo;F1&amp;rdquo; 환경을 사용(xilinx base)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model&#34;&gt;model&lt;/h3&gt;
&lt;h4 id=&#34;verilog-model&#34;&gt;verilog model&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;RTL form의 synthesis, simulation model&lt;/li&gt;
&lt;li&gt;4개의 functional interface를 가짐 : slave host interface, interrupt line, two master interface(내/외부 메모리 접근용)&lt;/li&gt;
&lt;li&gt;host와 메모리 인터페이스는 soc 통합을 위한 어댑터가 필요(AXI4 어댑터 제공)&lt;/li&gt;
&lt;li&gt;NVDLA core는 single clock domain이나 bus adaptor는 clock domain crossing을 허용(버스 어댑터에에서 클럭 도메인 크로싱이 된다는 뜻인듯)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;simulation-model-and-verification-suite&#34;&gt;Simulation model and verification suite&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;소프트웨어 개발, 시스템 통합 및 테스트를 위한 TLM2 SystemC 모델(Synopsys VDK나 GreenSocs QBox플랫폼 같은 시뮬레이션 환경에서 사용되도록 작성)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;software&#34;&gt;software&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;초기 NVDLA 오픈소스는 &amp;ldquo;headless&amp;quot;를 위한 소프트웨어만 포함(리눅스 환경),&lt;/li&gt;
&lt;li&gt;Kernel mode driver, user mdoe test utility가 소스형태로 제공됨&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>[AI HW Design] Chap03 Parallel Architecture (2/3)</title>
        <link>https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/</link>
        <pubDate>Fri, 30 Dec 2022 00:00:00 +0000</pubDate>
        
        <guid>https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/</guid>
        <description>&lt;p&gt;본 챕터에서는 몇가지 주요한 Paralle Architecture에 대하여 소개한다.
이 페이지에서는 NVDLA와 Google TPU에 대해서 기술한다.&lt;/p&gt;
&lt;h2 id=&#34;nvidia-deep-learning-accelerator-nvdla&#34;&gt;NVIDIA Deep Learning Accelerator (NVDLA)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA는 FPGA로 구성 가능한 추론을 위한 오픈소스 아키텍쳐 (&lt;a class=&#34;link&#34; href=&#34;http://nvdla.org&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://nvdla.org&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Primitive functional blocks으로 CNN을 지원 (convolution, activation, pooling, normalization)&lt;/li&gt;
&lt;li&gt;각 블럭은 next layer의 active와 configuration을 위한 double buffer를 가짐&lt;/li&gt;
&lt;li&gt;next layer의 operation은 active operation이 완료되어야 시작&lt;/li&gt;
&lt;li&gt;independent mode와 pipeline을 사용하는 fused mode가 있음
&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/NVDLA_core_architecture.png&#34;
	width=&#34;1142&#34;
	height=&#34;950&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/NVDLA_core_architecture_hu2cca1af52d961361ac30bee8eb97a9b7_113988_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap03-parallel-architecture-2/3/NVDLA_core_architecture_hu2cca1af52d961361ac30bee8eb97a9b7_113988_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;NVDLA core architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;120&#34;
		data-flex-basis=&#34;288px&#34;
	
&gt;
&lt;em&gt;Figure. NVDLA core architecture&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convolution-operation&#34;&gt;Convolution Operation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Direct convolution, Image input convolution, winograd convolution, Batch convolution 지원 (상세내역은 책 참조)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;single-data-point-operationsdp&#34;&gt;Single Data Point Operation(SDP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SDP는 linear functions와 Look-up Table nonlinear functions을 통해 activation과 normalizatin을 지원 (상세내역은 책 참조)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;planar-data-operationpdp&#34;&gt;Planar Data Operation(PDP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;PDP는 maximum/minimum/average pooling을 지원&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multiplane-operation&#34;&gt;Multiplane Operation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cross Channel Data Processor(CPD)은 Local Response Normalization(LRN)을 수행&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-memory-and-reshape-operations&#34;&gt;Data Memory and Reshape Operations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;bridge DMA는 외부 메모리와 메모리 인터페이스간 데이터 전송을 담당&lt;/li&gt;
&lt;li&gt;data reshape engine은 data trasnformations, splitting, slicing, merging, contraction, reshape transpose 를 담당&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;system-configuration&#34;&gt;System Configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA는 small/large system model로 구현할 수 있음
&lt;ul&gt;
&lt;li&gt;small system model : IoT 기기와 같이 작은 모델을 위한 모델, 복잡도와 storage를 낮추고 single task를 수행&lt;/li&gt;
&lt;li&gt;large system model : mutiple task를 위한 coprocessor와 메모리 인터페이스 추가&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;external-interface&#34;&gt;External Interface&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA는 외부와 통신을 위한 Configuration Space Bus(CSB), Data backbone(DBB), SRAM interface, Interrupt interface를 가짐 (상세내용은 책 참조)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;software-design&#34;&gt;Software Design&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NVDLA SW는 Trained model을 parser/compiler/optimizer를 통해 loadable로 변환&lt;/li&gt;
&lt;li&gt;User Mode Driver(UMD)에 의해 Loadalbe이 로딩 되고 Job이 Kernel Mode Driver(KMD)로 제출됨, KMD는 스케줄링 수행&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;google-tensor-processing-unittpu&#34;&gt;Google Tensor Processing Unit(TPU)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;구글은 speech recognition 수요 해결을 위해 TPU v1(stand alone)과 v2/v3(cloud)를 개발&lt;/li&gt;
&lt;li&gt;TPU v1은 하기 스펙으로 MLP 0/1, CNN 0/1, RNN 0/1 6가지 neural network application을 수행 가능
&lt;ul&gt;
&lt;li&gt;256 × 256 eight bits MAC unit&lt;/li&gt;
&lt;li&gt;4 Mb on-chip Accumulator Memory (AM)&lt;/li&gt;
&lt;li&gt;24 Mb Unified Buffer (UB) – activation memory&lt;/li&gt;
&lt;li&gt;8 Gb off-chip weight DRAM memory&lt;/li&gt;
&lt;li&gt;Two 2133 MHz DDR3 channels&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TPU는 6가지 neural network application을 수행할 수 있음
&lt;ul&gt;
&lt;li&gt;Multi-layer perceptron(MLP) 0/1, Convolution Neural Network(CNN) 0/1, Recurrent Neural Network(RNN) 0/1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;system-architecture&#34;&gt;System Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;TPU v1은 매트릭스 연산을 Matrix Multiply Unit(MMU)에서 수행&lt;/li&gt;
&lt;li&gt;MMU는 256 × 256 eight bits MAC unit이며 16bit 연산을 수행할 경우 성능은 8bit 대비 절반(Sparse matrix 연산을 지원하지 않음)&lt;/li&gt;
&lt;li&gt;Weight FIFO는 matrix weight를 8Gb DRAM에서 읽어오며 activation, pooling, normalization 후 중간 연산 결과를 24Mb Unified Buffer에 저장&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/tpu-15dly1.max-500x500.PNG&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;TPU System Architecture&#34;
	
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multipy-accumulatemac-systolic-array&#34;&gt;Multipy-Accumulate(MAC) Systolic Array&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Systolic array는 TPU의 핵심이자 High throughput / low latency를 가진 SIMD pipeline.&lt;/li&gt;
&lt;li&gt;책에 별 설명이 없으므로 이에 대한 내용은 더 찾아보는 것이 좋다(다른 많은 곳에 잘 나와 있음)&lt;/li&gt;
&lt;li&gt;단점은 전력 소모가 많다는 것(데이터 센터 등에 적합)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;new-brain-floating-point-format&#34;&gt;New Brain Floating-point Format&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;TPU v1은 input data를 FP32에서 INT8로 quantization하여 연산하며 이에 따라 안정성/정확도 문제 발생&lt;/li&gt;
&lt;li&gt;이를 위해 IEEE FP16 대신 Brain Floating Point format (BFP16) 사용
&lt;ul&gt;
&lt;li&gt;BFP16 : Mantissa를 7bit으로 줄이고 exponent를 FP32와 같은 8bit으로 늘림&lt;/li&gt;
&lt;li&gt;Sign 1bit, Exponent 8bit, Mantissa 7bit&lt;/li&gt;
&lt;li&gt;multiplier area와 power를 줄이고 Scaling loss 없이 FP32와 동일한 정확도를 얻음&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;performance-comparision&#34;&gt;Performance Comparision&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;roof-line model의 관점에서 볼 때 TPU가 가장 높은 peak performance를 달성했다.&lt;/li&gt;
&lt;li&gt;roof-line model은 Y축이 성능을 나타내며(평평한 부분이 최고 성능), X축이 byte 당 operation intensity&lt;/li&gt;
&lt;li&gt;부가 설명을 하자면 한번에 얼마나 많은 연산을 수행하게 할 때 성능이 어디까지 올라가는지 지표, loof-line은 메모리 Bandwith 때문에 걸림&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cloud-tpu-configuration&#34;&gt;Cloud TPU configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;TPU v2/v3는 v1에서 DDR을 HBM으로 바꾸고 v1을 Pod로 묶음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cloud-software-architecture&#34;&gt;Cloud Software Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;구글은 cloud computation을 위해 새로운 SW 아키텍쳐를 개발&lt;/li&gt;
&lt;li&gt;Model을 TensorFlow를 통해 computational graph로 해석&lt;/li&gt;
&lt;li&gt;상세 내용은 책을 참조&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
