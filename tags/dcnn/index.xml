<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>DCNN on MW Devlog</title>
        <link>https://muonkmu.github.io/tags/dcnn/</link>
        <description>Recent content in DCNN on MW Devlog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 12 Feb 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://muonkmu.github.io/tags/dcnn/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>[AI HW Design] Chap05 Convolution Optimization (1/3)</title>
        <link>https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/</link>
        <pubDate>Sun, 12 Feb 2023 00:00:00 +0000</pubDate>
        
        <guid>https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/</guid>
        <description>&lt;p&gt;Convolution은 90% 이상의 Computing resource를 사용하며, data access를 줄이기 위해 feature maps reuse / filter weights reuse / partial sum reuse 같은 전략이 사용된다.
이번 챕터에서는 filter decomposition과 Row Stationary(RS) flow를 설명한다.&lt;/p&gt;
&lt;h2 id=&#34;deep-convolution-neural-network-accelerator-dcnn&#34;&gt;Deep Convolution Neural Network Accelerator (DCNN)&lt;/h2&gt;
&lt;p&gt;DCNN은 클라우드 타겟이 아닌 엣지 디바이스 타겟으로 UCLA에서 개발, 다음과 같은 특징을 지님&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Streaming data flow minimizes data access&lt;/li&gt;
&lt;li&gt;병렬 컴퓨팅을 위해 bandwidth  향상이 아닌 Interleaving architecture&lt;/li&gt;
&lt;li&gt;Large-size filter decomposition supports arbitrary convolution window&lt;/li&gt;
&lt;li&gt;추가적인 pooling functional unit을 통한 Convolution Unit(CU)의 workload 감소&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;system-architecture&#34;&gt;System Architecture&lt;/h3&gt;
&lt;p&gt;DCNN의 구성은 다음과 같다&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Buffer Bank
&lt;ul&gt;
&lt;li&gt;중간 데이터 저장 및 외부 메모리와 데이터 교환 목적&lt;/li&gt;
&lt;li&gt;Layer Input 용, Layer Output 용 2가지 셋으로 나누어짐&lt;/li&gt;
&lt;li&gt;또한 odd/even channel/feature를 위한 Bank A와 Bank B로 나누어짐(Interleaved)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Column Buffer
&lt;ul&gt;
&lt;li&gt;Buffer banck의 데이터를 CU engine의 input data type으로 remap&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Convolution Unint(CU) engine
&lt;ul&gt;
&lt;li&gt;CU engin은 kernel size 3X3까지 지원하는 16개의 Convolution unit으로 구성&lt;/li&gt;
&lt;li&gt;16bit fixed-point 연산&lt;/li&gt;
&lt;li&gt;local Prefetch unit이 DMA로 부터 weight/bias 값을 주기적으로 업데이트 함&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Accumulation (ACCU) buffer
&lt;ul&gt;
&lt;li&gt;convolution 동안 partial sum 연산 또는 Max pooling 연산 수행&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/01_DCNN_HW_Archi.png&#34;
	width=&#34;751&#34;
	height=&#34;463&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/01_DCNN_HW_Archi_hu3c580145d215e702491efc9736215254_29901_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/01_DCNN_HW_Archi_hu3c580145d215e702491efc9736215254_29901_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;DCNN hardware architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;162&#34;
		data-flex-basis=&#34;389px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;Control Command는 외부 메모리에서 128-depth FIFO로 로드 되며 하기 2가지로 분류됨
&lt;ul&gt;
&lt;li&gt;configure command : network layer를 구성하고 pooling/ReLU function 활성화&lt;/li&gt;
&lt;li&gt;excution command : convolution/pooing 초기화 및 필터 decompose 기술&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;filter-decomposition&#34;&gt;Filter Decomposition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;다양한 필터 커널 사이즈 대응을 위해 3X3 CU engine을 이용하여 filter decomposition 기술 이용&lt;/li&gt;
&lt;li&gt;커널 사이즈가 3의 배수가 아니면 zero-padding 이용&lt;/li&gt;
&lt;li&gt;convolution 후 결과는 one output feature map으로 재결합 됨&lt;/li&gt;
&lt;li&gt;상세 사항은 책 참조&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;streaming-architecture&#34;&gt;Streaming Architecture&lt;/h3&gt;
&lt;p&gt;데이터의 이동을 최소화하기위해 Filter Weights Reuse와 Input Channel Reuse 사용&lt;/p&gt;
&lt;h4 id=&#34;filter-weights-reuse&#34;&gt;Filter Weights Reuse&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;3X3 convolution : filter weight는 CU engine에 저장되고 input feature map이 CU engine으로 공급되며 연산이 완료 될 때까지 filter weight는 업데이트 되지 않음&lt;/li&gt;
&lt;li&gt;1X1 convolution : CU unit의 9개 곱셈기 중 7개는 off 되고 2개만 odd/even 채널의 partial sum 계산을 위해 사용&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/02_Data_streaming_archi_data_flow.png&#34;
	width=&#34;934&#34;
	height=&#34;693&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/02_Data_streaming_archi_data_flow_hu44efc47054aa1ffce71cbf0d622070e8_154287_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/02_Data_streaming_archi_data_flow_hu44efc47054aa1ffce71cbf0d622070e8_154287_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Data streaming architecture with the data flow&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;134&#34;
		data-flex-basis=&#34;323px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;Buffer Bank의 output bandwidth를 최대화하기 위해 구조는 하기와 같다(input cannel 및 Column buffer 구조 이야기 같은데)
&lt;ul&gt;
&lt;li&gt;16개의 row 데이터는 odd/even data set으로 나뉨&lt;/li&gt;
&lt;li&gt;2개의 FIFO는 각 데이터 셋에 페어링 됨 (8개의 row 데이터)&lt;/li&gt;
&lt;li&gt;8개의 input row data는 10개의 overlapped data로 매핑&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;input-channel-reuse&#34;&gt;Input Channel Reuse&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;1X1 convolution을 위해 interleaved architecture 사용 (16개 데이터 셋이 odd/even 채널로 2개 데이터셋 구분됨)&lt;/li&gt;
&lt;li&gt;2개의 데이터 셋은 다른 filter weight와 곱해져 32개 output이 나옴&lt;/li&gt;
&lt;li&gt;출력은 같은 odd/even 채널 끼리 더해짐&lt;/li&gt;
&lt;li&gt;위의 내용과 비슷한 것 같은데 filter weight가 이동하고 input 값이 고정이라는 걸 다시 설명한 듯&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pooling&#34;&gt;Pooling&lt;/h3&gt;
&lt;p&gt;pooling function은 average pooling과 max pooing 이 다른 구조로 분리되어 있음&lt;/p&gt;
&lt;h4 id=&#34;average-pooling&#34;&gt;Average Pooling&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Average Pooling function은 Convolution layer에서 Inpu/Output 채널이 같은 사이즈인 CU엔진에 의해 구현
&lt;ul&gt;
&lt;li&gt;kernel의 사이즈가 pooling window와 일치하는&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;대응되는 filter weight는 1/K^2으로 되고 나머지는 0으로 된 후 convolution 수행&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;max-pooling&#34;&gt;Max Pooling&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Max pooling은 ACCU에서 별도 모듈로 구현&lt;/li&gt;
&lt;li&gt;Max pooling 모듈은 scratch pad에 있는 8개의 output 값과 연결되며 이는 다른 stride를 지원하기 위해 MUX와 연결&lt;/li&gt;
&lt;li&gt;MUX의 출력은 MAX Pooling 계산기로 가는데 이는 3개의 입력과 1개의 output feedback 입력을 받아 계산하며 인풋이 없어질때까지 연산 반복&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/03_Max_pooling_archi.png&#34;
	width=&#34;924&#34;
	height=&#34;652&#34;
	srcset=&#34;https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/03_Max_pooling_archi_hud451dc32710866902d390d177efeafc4_54340_480x0_resize_box_3.png 480w, https://muonkmu.github.io/p/ai-hw-design-chap05-convolution-optimization-1/3/03_Max_pooling_archi_hud451dc32710866902d390d177efeafc4_54340_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Max pooling architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;141&#34;
		data-flex-basis=&#34;340px&#34;
	
&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convolution-unitcu-engine&#34;&gt;Convolution Unit(CU) Engine&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;3X3 CU engine은 9개의 PE(input feature와 filter weight를 곱함)와 1개의 ADDER로 구성&lt;/li&gt;
&lt;li&gt;다른 커널 사이즈 지원을 위해 PE는 On/Off 기능을 가짐&lt;/li&gt;
&lt;li&gt;상세 내용은 책 참조 (그림과 3X3일 때, 1X1일때 예시 있음)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;accumulation-accu-buffer&#34;&gt;Accumulation (ACCU) Buffer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ACCU는 scratch pad에 partial sums 과 stores output feature maps을 저장&lt;/li&gt;
&lt;li&gt;ACCU는 partial product accumulation을 위한 Ping-pong buffer, Max pooling을 위한 temporary storage, readout block으로 구성 (책에 그림 참조)&lt;/li&gt;
&lt;li&gt;Convolution이 진행되는 동안 1개 버퍼는 덧셈만 하고 다른 하나의 버퍼는 Max pooling을 진행, convolution이 완료된후 각 버퍼의 연결이 switch 됨&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model-compression&#34;&gt;Model Compression&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Model Compression을 하기 위해 training 과정에서 모델을 pruning하고 filter weights를 codebook으로 quantization 함.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
